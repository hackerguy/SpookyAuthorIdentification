{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://storage.googleapis.com/kaggle-media/competitions/spooky-books/dmitrij-paskevic-44124.jpg\" style=\"width:200px; float: left; padding-right: 10px\"/>\n",
    "<h2 style=\"font-face: verdana; font-size: 32px;\">Spooky Author Identification</h2>\n",
    "<h3 style=\"font-face: verdana; font-size: 16px;\">Derive rich features for Machine Learning with the Watson Cognitive APIs</h3>\n",
    "<br><br>\n",
    "<a href=\"https://www.kaggle.com/c/spooky-author-identification/\">Spooky Author Identification Kaggle Competition</a>\n",
    "\n",
    "<h3 style=\"font-face: verdana; font-size: 16px;\">The objective of this machine learning model is to predict the author of excerpts from horror stories by Edgar Allan Poe, Mary Shelley, and HP Lovecraft.</h3>\n",
    "\n",
    "The dataset contains text from works of fiction written by these spooky authors. The goal is to accurately identify the author of the sentences.\n",
    "\n",
    "Data fields in the dataset:\n",
    "\n",
    "    id - a unique identifier for each sentence\n",
    "    text - some text written by one of the authors\n",
    "    author - the author of the sentence (EAP: Edgar Allan Poe, HPL: HP Lovecraft; MWS: Mary Wollstonecraft Shelley)\n",
    "\n",
    "<h3 style=\"font-face: verdana; font-size: 16px;\">Approach</h3>\n",
    "\n",
    "We will approach this challenge by first using a traditional multiclassification machine learning approach. We will then explore using IBM Watson Natural Language Understanding to derive additional enhanced features on which to learn a machine learning model.\n",
    "\n",
    "<h3 style=\"font-face: verdana; font-size: 16px;\">IBM Watson Natural Language Understanding</h3>\n",
    "\n",
    "IBM Watsonâ„¢ Natural Language Understanding (NLU) can analyze semantic features of text input, including categories, concepts, emotion, entities, keywords, metadata, relations, semantic roles, and sentiment. In this example, we will utilize the emotion and sentiment features of NLU to create enhanced machine learning features.\n",
    "\n",
    "\n",
    "<h4 style=\"font-face: verdana; font-size: 16px;\">Emotion</h4>\n",
    "\n",
    "The emotion feature of NLU allows you to analyze emotion conveyed by specific target phrases or by the document as a whole. You can also enable emotion analysis for entities and keywords that are automatically detected by the service. In this example, we will simply analyze the spooky excerpt as a whole. The emotions we will derive features for are \n",
    "\n",
    "- Anger\n",
    "- Joy\n",
    "- Sadness\n",
    "- Fear\n",
    "- Disgust\n",
    "\n",
    "Emotion scores range from 0 to 1 for sadness, joy, fear, disgust, and anger. A 0 means the text doesn't convey the emotion, and a 1 means the text definitely carries the emotion.\n",
    "\n",
    "<h4 style=\"font-face: verdana; font-size: 16px;\">Sentiment</h4>\n",
    "\n",
    "The sentiment feature of NLU allows you to analyze the sentiment toward specific target phrases and the sentiment of the document as a whole. You can also get sentiment information for detected entities and keywords by enabling the sentiment option for those features. In this example, we will simply analyze the spooky excerpt as a whole.\n",
    "\n",
    "The sentiment score ranges from -1 (negative sentiment) to 1 (positive sentiment).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and unzip the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if os.path.isfile('train.zip'):\n",
    "    os.remove(\"train.zip\")\n",
    "if os.path.isfile('train.csv'):\n",
    "    os.remove(\"train.csv\")\n",
    "import wget\n",
    "url = 'https://github.com/hackerguy/SpookyAuthorIdentification/blob/master/train.zip?raw=true'\n",
    "wget.download(url)\n",
    "import zipfile\n",
    "zip = zipfile.ZipFile('train.zip', 'r')\n",
    "zip.extractall()\n",
    "zip.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the data set as a Spark DataFrame\n",
    "### Infer schema and column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "data = (spark.read\n",
    "  .format('csv')\n",
    "  .option('header', 'true')\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .load('train.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of ascertaining the dimensions of my dungeon; as I might make its circuit, and return to the point whence I set out, without being aware of the fact; so perfectly uniform seemed the wall.</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling might be a mere mistake.</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from which, as he capered down the hill, cutting all manner of fantastic steps, he took snuff incessantly with an air of the greatest possible self satisfaction.</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor Terrace on the sixteen fertile counties spread beneath, speckled by happy cottages and wealthier towns, all looked as in former years, heart cheering and fair.</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Superintendent abandoned his attempts; but a perplexed look occasionally steals over his countenance as he sits thinking at his desk.</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  \\\n",
       "0  id26305   \n",
       "1  id17569   \n",
       "2  id11008   \n",
       "3  id27763   \n",
       "4  id12958   \n",
       "\n",
       "                                                                                                                                                                                                                                      text  \\\n",
       "0  This process, however, afforded me no means of ascertaining the dimensions of my dungeon; as I might make its circuit, and return to the point whence I set out, without being aware of the fact; so perfectly uniform seemed the wall.   \n",
       "1  It never once occurred to me that the fumbling might be a mere mistake.                                                                                                                                                                   \n",
       "2  In his left hand was a gold snuff box, from which, as he capered down the hill, cutting all manner of fantastic steps, he took snuff incessantly with an air of the greatest possible self satisfaction.                                  \n",
       "3  How lovely is spring As we looked from Windsor Terrace on the sixteen fertile counties spread beneath, speckled by happy cottages and wealthier towns, all looked as in former years, heart cheering and fair.                            \n",
       "4  Finding nothing else, not even gold, the Superintendent abandoned his attempts; but a perplexed look occasionally steals over his countenance as he sits thinking at his desk.                                                            \n",
       "\n",
       "  author  \n",
       "0  EAP    \n",
       "1  HPL    \n",
       "2  EAP    \n",
       "3  MWS    \n",
       "4  HPL    "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "data.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the schema of the data including data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- author: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove rows that do not have valid author fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.filter((data['author']=='EAP')| (data['author']=='HPL') | (data['author']=='MWS'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove text fields that contain \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.sql.functions import col\n",
    "#data = data.filter(~ col('text').like('%\"%'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Overview - number of rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 18047 rows in the dataset.\n",
      "There are 3 columns in the dataset.\n"
     ]
    }
   ],
   "source": [
    "print(\"There are {} rows in the dataset.\".format(str(data.count())))\n",
    "print(\"There are {} columns in the dataset.\".format(str(len(data.columns))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Split up the dataframe - this will be imortant later on to limit the API call rate to the NLU Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data00,data01,data02,data03,data04,data05,data06,data07,data08,data09,data10,data11,data12,data13,data14,data15,data16,data17,data18,data19, \\\n",
    "data20,data21,data22,data23,data24,data25,data26,data27,data28,data29,data30,data31,data32,data33,data34,data35,data36,data37,data38,data39, \\\n",
    "data40,data41,data42,data43,data44,data45,data46,data47,data48,data49,data50,data51,data52,data53,data54,data55,data56,data57,data58,data59, \\\n",
    "data60,data61,data62,data63,data64,data65,data66,data67,data68,data69,data70,data71,data72,data73,data74,data75,data76,data77,data78,data79, \\\n",
    "data80, data81,data82,data83,data84,data85,data86,data87,data88,data89,data90,data91,data92,data93,data94,data95,data96,data97,data98,data99 \\\n",
    " = data.randomSplit([1.0]*100, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use a subset of the data for processing efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraction = 1.0\n",
    "\n",
    "data00 = data00.sample(False, fraction, seed=0)\n",
    "data01 = data01.sample(False, fraction, seed=0)\n",
    "data02 = data02.sample(False, fraction, seed=0)\n",
    "data03 = data03.sample(False, fraction, seed=0)\n",
    "data04 = data04.sample(False, fraction, seed=0)\n",
    "data05 = data05.sample(False, fraction, seed=0)\n",
    "data06 = data06.sample(False, fraction, seed=0)\n",
    "data07 = data07.sample(False, fraction, seed=0)\n",
    "data08 = data08.sample(False, fraction, seed=0)\n",
    "data09 = data09.sample(False, fraction, seed=0)\n",
    "data10 = data10.sample(False, fraction, seed=0)\n",
    "data11 = data11.sample(False, fraction, seed=0)\n",
    "data12 = data12.sample(False, fraction, seed=0)\n",
    "data13 = data13.sample(False, fraction, seed=0)\n",
    "data14 = data14.sample(False, fraction, seed=0)\n",
    "data15 = data15.sample(False, fraction, seed=0)\n",
    "data16 = data16.sample(False, fraction, seed=0)\n",
    "data17 = data17.sample(False, fraction, seed=0)\n",
    "data18 = data18.sample(False, fraction, seed=0)\n",
    "data19 = data19.sample(False, fraction, seed=0)\n",
    "data20 = data20.sample(False, fraction, seed=0)\n",
    "data21 = data21.sample(False, fraction, seed=0)\n",
    "data22 = data22.sample(False, fraction, seed=0)\n",
    "data23 = data23.sample(False, fraction, seed=0)\n",
    "data24 = data24.sample(False, fraction, seed=0)\n",
    "data25 = data25.sample(False, fraction, seed=0)\n",
    "data26 = data26.sample(False, fraction, seed=0)\n",
    "data27 = data27.sample(False, fraction, seed=0)\n",
    "data28 = data28.sample(False, fraction, seed=0)\n",
    "data29 = data29.sample(False, fraction, seed=0)\n",
    "data30 = data30.sample(False, fraction, seed=0)\n",
    "data31 = data31.sample(False, fraction, seed=0)\n",
    "data32 = data32.sample(False, fraction, seed=0)\n",
    "data33 = data33.sample(False, fraction, seed=0)\n",
    "data34 = data34.sample(False, fraction, seed=0)\n",
    "data35 = data35.sample(False, fraction, seed=0)\n",
    "data36 = data36.sample(False, fraction, seed=0)\n",
    "data37 = data37.sample(False, fraction, seed=0)\n",
    "data38 = data38.sample(False, fraction, seed=0)\n",
    "data39 = data39.sample(False, fraction, seed=0)\n",
    "data40 = data40.sample(False, fraction, seed=0)\n",
    "data41 = data41.sample(False, fraction, seed=0)\n",
    "data42 = data42.sample(False, fraction, seed=0)\n",
    "data43 = data43.sample(False, fraction, seed=0)\n",
    "data44 = data44.sample(False, fraction, seed=0)\n",
    "data45 = data45.sample(False, fraction, seed=0)\n",
    "data46 = data46.sample(False, fraction, seed=0)\n",
    "data47 = data47.sample(False, fraction, seed=0)\n",
    "data48 = data48.sample(False, fraction, seed=0)\n",
    "data49 = data49.sample(False, fraction, seed=0)\n",
    "data50 = data50.sample(False, fraction, seed=0)\n",
    "data51 = data51.sample(False, fraction, seed=0)\n",
    "data52 = data52.sample(False, fraction, seed=0)\n",
    "data53 = data53.sample(False, fraction, seed=0)\n",
    "data54 = data54.sample(False, fraction, seed=0)\n",
    "data55 = data55.sample(False, fraction, seed=0)\n",
    "data56 = data56.sample(False, fraction, seed=0)\n",
    "data57 = data57.sample(False, fraction, seed=0)\n",
    "data58 = data58.sample(False, fraction, seed=0)\n",
    "data59 = data59.sample(False, fraction, seed=0)\n",
    "data60 = data60.sample(False, fraction, seed=0)\n",
    "data61 = data61.sample(False, fraction, seed=0)\n",
    "data62 = data62.sample(False, fraction, seed=0)\n",
    "data63 = data63.sample(False, fraction, seed=0)\n",
    "data64 = data64.sample(False, fraction, seed=0)\n",
    "data65 = data65.sample(False, fraction, seed=0)\n",
    "data66 = data66.sample(False, fraction, seed=0)\n",
    "data67 = data67.sample(False, fraction, seed=0)\n",
    "data68 = data68.sample(False, fraction, seed=0)\n",
    "data69 = data69.sample(False, fraction, seed=0)\n",
    "data70 = data70.sample(False, fraction, seed=0)\n",
    "data71 = data71.sample(False, fraction, seed=0)\n",
    "data72 = data72.sample(False, fraction, seed=0)\n",
    "data74 = data74.sample(False, fraction, seed=0)\n",
    "data75 = data75.sample(False, fraction, seed=0)\n",
    "data76 = data76.sample(False, fraction, seed=0)\n",
    "data77 = data77.sample(False, fraction, seed=0)\n",
    "data78 = data78.sample(False, fraction, seed=0)\n",
    "data79 = data79.sample(False, fraction, seed=0)\n",
    "data80 = data80.sample(False, fraction, seed=0)\n",
    "data81 = data81.sample(False, fraction, seed=0)\n",
    "data82 = data82.sample(False, fraction, seed=0)\n",
    "data83 = data83.sample(False, fraction, seed=0)\n",
    "data84 = data84.sample(False, fraction, seed=0)\n",
    "data85 = data85.sample(False, fraction, seed=0)\n",
    "data86 = data86.sample(False, fraction, seed=0)\n",
    "data87 = data87.sample(False, fraction, seed=0)\n",
    "data88 = data88.sample(False, fraction, seed=0)\n",
    "data89 = data89.sample(False, fraction, seed=0)\n",
    "data90 = data90.sample(False, fraction, seed=0)\n",
    "data91 = data91.sample(False, fraction, seed=0)\n",
    "data92 = data92.sample(False, fraction, seed=0)\n",
    "data93 = data93.sample(False, fraction, seed=0)\n",
    "data94 = data94.sample(False, fraction, seed=0)\n",
    "data95 = data95.sample(False, fraction, seed=0)\n",
    "data96 = data96.sample(False, fraction, seed=0)\n",
    "data97 = data97.sample(False, fraction, seed=0)\n",
    "data98 = data98.sample(False, fraction, seed=0)\n",
    "data99 = data99.sample(False, fraction, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The combined dataset contains 18006 rows.\n"
     ]
    }
   ],
   "source": [
    "dataUnion= (data00.union(data01).union(data02).union(data03).union(data04).union(data05).union(data06).union(data07).union(data08).union(data09)\n",
    "            .union(data10).union(data11).union(data12).union(data13).union(data14).union(data15).union(data16).union(data17).union(data18).union(data19)\n",
    "            .union(data20).union(data21).union(data22).union(data23).union(data24).union(data25).union(data26).union(data27).union(data28).union(data29)\n",
    "            .union(data30).union(data31).union(data32).union(data33).union(data34).union(data35).union(data36).union(data37).union(data38).union(data39)\n",
    "            .union(data40).union(data41).union(data42).union(data43).union(data44).union(data45).union(data46).union(data47).union(data48).union(data49)\n",
    "            .union(data50).union(data51).union(data52).union(data53).union(data54).union(data55).union(data56).union(data57).union(data58).union(data59)\n",
    "            .union(data60).union(data61).union(data62).union(data63).union(data64).union(data65).union(data66).union(data67).union(data68).union(data79)\n",
    "            .union(data70).union(data71).union(data72).union(data73).union(data74).union(data75).union(data76).union(data77).union(data78).union(data79)\n",
    "            .union(data80).union(data81).union(data82).union(data83).union(data84).union(data85).union(data86).union(data87).union(data88).union(data89)\n",
    "            .union(data90).union(data91).union(data92).union(data93).union(data94).union(data95).union(data96).union(data97).union(data98).union(data99))\n",
    "\n",
    "print(\"The combined dataset contains {} rows.\".format(dataUnion.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>words</th>\n",
       "      <th>#tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I walked rapidly, softly, and close to the ruined houses.</td>\n",
       "      <td>[i, walked, rapidly,, softly,, and, close, to, the, ruined, houses.]</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The youth's febrile mind, apparently, was dwelling on strange things; and the doctor shuddered now and then as he spoke of them.</td>\n",
       "      <td>[the, youth's, febrile, mind,, apparently,, was, dwelling, on, strange, things;, and, the, doctor, shuddered, now, and, then, as, he, spoke, of, them.]</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"But since the murderer has been discovered \"\" \"\"The murderer discovered Good God how can that be? who could attempt to pursue him?\"</td>\n",
       "      <td>[\"but, since, the, murderer, has, been, discovered, \"\", \"\"the, murderer, discovered, good, god, how, can, that, be?, who, could, attempt, to, pursue, him?\"]</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>So thick were the vapours that the way was hard, and though Atal followed on at last, he could scarce see the grey shape of Barzai on the dim slope above in the clouded moonlight.</td>\n",
       "      <td>[so, thick, were, the, vapours, that, the, way, was, hard,, and, though, atal, followed, on, at, last,, he, could, scarce, see, the, grey, shape, of, barzai, on, the, dim, slope, above, in, the, clouded, moonlight.]</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>By authority of the king such districts were placed under ban, and all persons forbidden, under pain of death, to intrude upon their dismal solitude.</td>\n",
       "      <td>[by, authority, of, the, king, such, districts, were, placed, under, ban,, and, all, persons, forbidden,, under, pain, of, death,, to, intrude, upon, their, dismal, solitude.]</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                  text  \\\n",
       "0  I walked rapidly, softly, and close to the ruined houses.                                                                                                                             \n",
       "1  The youth's febrile mind, apparently, was dwelling on strange things; and the doctor shuddered now and then as he spoke of them.                                                      \n",
       "2  \"But since the murderer has been discovered \"\" \"\"The murderer discovered Good God how can that be? who could attempt to pursue him?\"                                                  \n",
       "3  So thick were the vapours that the way was hard, and though Atal followed on at last, he could scarce see the grey shape of Barzai on the dim slope above in the clouded moonlight.   \n",
       "4  By authority of the king such districts were placed under ban, and all persons forbidden, under pain of death, to intrude upon their dismal solitude.                                 \n",
       "\n",
       "                                                                                                                                                                                                                     words  \\\n",
       "0  [i, walked, rapidly,, softly,, and, close, to, the, ruined, houses.]                                                                                                                                                      \n",
       "1  [the, youth's, febrile, mind,, apparently,, was, dwelling, on, strange, things;, and, the, doctor, shuddered, now, and, then, as, he, spoke, of, them.]                                                                   \n",
       "2  [\"but, since, the, murderer, has, been, discovered, \"\", \"\"the, murderer, discovered, good, god, how, can, that, be?, who, could, attempt, to, pursue, him?\"]                                                              \n",
       "3  [so, thick, were, the, vapours, that, the, way, was, hard,, and, though, atal, followed, on, at, last,, he, could, scarce, see, the, grey, shape, of, barzai, on, the, dim, slope, above, in, the, clouded, moonlight.]   \n",
       "4  [by, authority, of, the, king, such, districts, were, placed, under, ban,, and, all, persons, forbidden,, under, pain, of, death,, to, intrude, upon, their, dismal, solitude.]                                           \n",
       "\n",
       "   #tokens  \n",
       "0  10       \n",
       "1  22       \n",
       "2  23       \n",
       "3  35       \n",
       "4  25       "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "\n",
    "countTokens = udf(lambda words: len(words), IntegerType())\n",
    "\n",
    "tokenized = tokenizer.transform(dataUnion)\n",
    "(tokenized.select(\"text\", \"words\")\n",
    "    .withColumn(\"#tokens\", countTokens(col(\"words\"))).toPandas().head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>words</th>\n",
       "      <th>filtered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I walked rapidly, softly, and close to the ruined houses.</td>\n",
       "      <td>[i, walked, rapidly,, softly,, and, close, to, the, ruined, houses.]</td>\n",
       "      <td>[walked, rapidly,, softly,, close, ruined, houses.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The youth's febrile mind, apparently, was dwelling on strange things; and the doctor shuddered now and then as he spoke of them.</td>\n",
       "      <td>[the, youth's, febrile, mind,, apparently,, was, dwelling, on, strange, things;, and, the, doctor, shuddered, now, and, then, as, he, spoke, of, them.]</td>\n",
       "      <td>[youth's, febrile, mind,, apparently,, dwelling, strange, things;, doctor, shuddered, spoke, them.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"But since the murderer has been discovered \"\" \"\"The murderer discovered Good God how can that be? who could attempt to pursue him?\"</td>\n",
       "      <td>[\"but, since, the, murderer, has, been, discovered, \"\", \"\"the, murderer, discovered, good, god, how, can, that, be?, who, could, attempt, to, pursue, him?\"]</td>\n",
       "      <td>[\"but, since, murderer, discovered, \"\", \"\"the, murderer, discovered, good, god, be?, could, attempt, pursue, him?\"]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>So thick were the vapours that the way was hard, and though Atal followed on at last, he could scarce see the grey shape of Barzai on the dim slope above in the clouded moonlight.</td>\n",
       "      <td>[so, thick, were, the, vapours, that, the, way, was, hard,, and, though, atal, followed, on, at, last,, he, could, scarce, see, the, grey, shape, of, barzai, on, the, dim, slope, above, in, the, clouded, moonlight.]</td>\n",
       "      <td>[thick, vapours, way, hard,, though, atal, followed, last,, could, scarce, see, grey, shape, barzai, dim, slope, clouded, moonlight.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>By authority of the king such districts were placed under ban, and all persons forbidden, under pain of death, to intrude upon their dismal solitude.</td>\n",
       "      <td>[by, authority, of, the, king, such, districts, were, placed, under, ban,, and, all, persons, forbidden,, under, pain, of, death,, to, intrude, upon, their, dismal, solitude.]</td>\n",
       "      <td>[authority, king, districts, placed, ban,, persons, forbidden,, pain, death,, intrude, upon, dismal, solitude.]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                  text  \\\n",
       "0  I walked rapidly, softly, and close to the ruined houses.                                                                                                                             \n",
       "1  The youth's febrile mind, apparently, was dwelling on strange things; and the doctor shuddered now and then as he spoke of them.                                                      \n",
       "2  \"But since the murderer has been discovered \"\" \"\"The murderer discovered Good God how can that be? who could attempt to pursue him?\"                                                  \n",
       "3  So thick were the vapours that the way was hard, and though Atal followed on at last, he could scarce see the grey shape of Barzai on the dim slope above in the clouded moonlight.   \n",
       "4  By authority of the king such districts were placed under ban, and all persons forbidden, under pain of death, to intrude upon their dismal solitude.                                 \n",
       "\n",
       "                                                                                                                                                                                                                     words  \\\n",
       "0  [i, walked, rapidly,, softly,, and, close, to, the, ruined, houses.]                                                                                                                                                      \n",
       "1  [the, youth's, febrile, mind,, apparently,, was, dwelling, on, strange, things;, and, the, doctor, shuddered, now, and, then, as, he, spoke, of, them.]                                                                   \n",
       "2  [\"but, since, the, murderer, has, been, discovered, \"\", \"\"the, murderer, discovered, good, god, how, can, that, be?, who, could, attempt, to, pursue, him?\"]                                                              \n",
       "3  [so, thick, were, the, vapours, that, the, way, was, hard,, and, though, atal, followed, on, at, last,, he, could, scarce, see, the, grey, shape, of, barzai, on, the, dim, slope, above, in, the, clouded, moonlight.]   \n",
       "4  [by, authority, of, the, king, such, districts, were, placed, under, ban,, and, all, persons, forbidden,, under, pain, of, death,, to, intrude, upon, their, dismal, solitude.]                                           \n",
       "\n",
       "                                                                                                                                filtered  \n",
       "0  [walked, rapidly,, softly,, close, ruined, houses.]                                                                                    \n",
       "1  [youth's, febrile, mind,, apparently,, dwelling, strange, things;, doctor, shuddered, spoke, them.]                                    \n",
       "2  [\"but, since, murderer, discovered, \"\", \"\"the, murderer, discovered, good, god, be?, could, attempt, pursue, him?\"]                    \n",
       "3  [thick, vapours, way, hard,, though, atal, followed, last,, could, scarce, see, grey, shape, barzai, dim, slope, clouded, moonlight.]  \n",
       "4  [authority, king, districts, placed, ban,, persons, forbidden,, pain, death,, intrude, upon, dismal, solitude.]                        "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").setCaseSensitive(False)\n",
    "removed = remover.transform(tokenized)\n",
    "removed.select(\"text\", \"words\", \"filtered\" ).toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show list of common words removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\n",
      "me\n",
      "my\n",
      "myself\n",
      "we\n",
      "our\n",
      "ours\n",
      "ourselves\n",
      "you\n",
      "your\n",
      "yours\n",
      "yourself\n",
      "yourselves\n",
      "he\n",
      "him\n",
      "his\n",
      "himself\n",
      "she\n",
      "her\n",
      "hers\n",
      "herself\n",
      "it\n",
      "its\n",
      "itself\n",
      "they\n",
      "them\n",
      "their\n",
      "theirs\n",
      "themselves\n",
      "what\n",
      "which\n",
      "who\n",
      "whom\n",
      "this\n",
      "that\n",
      "these\n",
      "those\n",
      "am\n",
      "is\n",
      "are\n",
      "was\n",
      "were\n",
      "be\n",
      "been\n",
      "being\n",
      "have\n",
      "has\n",
      "had\n",
      "having\n",
      "do\n",
      "does\n",
      "did\n",
      "doing\n",
      "a\n",
      "an\n",
      "the\n",
      "and\n",
      "but\n",
      "if\n",
      "or\n",
      "because\n",
      "as\n",
      "until\n",
      "while\n",
      "of\n",
      "at\n",
      "by\n",
      "for\n",
      "with\n",
      "about\n",
      "against\n",
      "between\n",
      "into\n",
      "through\n",
      "during\n",
      "before\n",
      "after\n",
      "above\n",
      "below\n",
      "to\n",
      "from\n",
      "up\n",
      "down\n",
      "in\n",
      "out\n",
      "on\n",
      "off\n",
      "over\n",
      "under\n",
      "again\n",
      "further\n",
      "then\n",
      "once\n",
      "here\n",
      "there\n",
      "when\n",
      "where\n",
      "why\n",
      "how\n",
      "all\n",
      "any\n",
      "both\n",
      "each\n",
      "few\n",
      "more\n",
      "most\n",
      "other\n",
      "some\n",
      "such\n",
      "no\n",
      "nor\n",
      "not\n",
      "only\n",
      "own\n",
      "same\n",
      "so\n",
      "than\n",
      "too\n",
      "very\n",
      "s\n",
      "t\n",
      "can\n",
      "will\n",
      "just\n",
      "don\n",
      "should\n",
      "now\n",
      "d\n",
      "ll\n",
      "m\n",
      "o\n",
      "re\n",
      "ve\n",
      "y\n",
      "ain\n",
      "aren\n",
      "couldn\n",
      "didn\n",
      "doesn\n",
      "hadn\n",
      "hasn\n",
      "haven\n",
      "isn\n",
      "ma\n",
      "mightn\n",
      "mustn\n",
      "needn\n",
      "shan\n",
      "shouldn\n",
      "wasn\n",
      "weren\n",
      "won\n",
      "wouldn\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "[print(x) for x in remover.getStopWords()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hash the words and inverse weight words that occur frequently across all text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>rawFeatures</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I walked rapidly, softly, and close to the ruined houses.</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0)</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.28480660233, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.40340830361, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.34785505396, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.43523626321, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.37494740591, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.92606070011, 0.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The youth's febrile mind, apparently, was dwelling on strange things; and the doctor shuddered now and then as he spoke of them.</td>\n",
       "      <td>(1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)</td>\n",
       "      <td>(2.16563034477, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.06264553022, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.24261075656, 0.0, 0.0, 0.0, 0.0, 0.0, 2.34785505396, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.02156144685, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.37137171676, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.99059922124, 2.17536278169, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.77890305577, 0.0, 0.0, 0.0, 0.0, 0.0, 1.96650166966, 2.27233693682, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"But since the murderer has been discovered \"\" \"\"The murderer discovered Good God how can that be? who could attempt to pursue him?\"</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 3.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.4950110513, 0.0, 0.0, 2.18469716536, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.95820286685, 6.03589213122, 0.0, 2.34785505396, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.77398097856, 1.97527015948, 2.01654261574, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.07274440858, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.22287138877, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.21272702844, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.77890305577, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>So thick were the vapours that the way was hard, and though Atal followed on at last, he could scarce see the grey shape of Barzai on the dim slope above in the clouded moonlight.</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 1.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 1.96650166966, 3.78762387259, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.4950110513, 0.0, 4.12529106043, 2.18469716536, 0.0, 0.0, 0.0, 2.0758383337, 0.0, 0.0, 2.17145843315, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.77398097856, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.32544676214, 0.0, 0.0, 2.07274440858, 0.0, 0.0, 0.0, 0.0, 0.0, 2.35076457012, 0.0, 0.0, 0.0, 0.0, 2.18174004147, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.17097145968, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.04489230361, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.39423673213, 0.0, 2.01612551458, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>By authority of the king such districts were placed under ban, and all persons forbidden, under pain of death, to intrude upon their dismal solitude.</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0)</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.29247367165, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0758383337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.77398097856, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.43523626321, 0.0, 4.65089352427, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.59734496689, 0.0, 2.24261075656, 0.0, 2.10139453289, 0.0, 2.15935467851, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.01612551458, 2.21069863018, 0.0, 0.0, 0.0, 1.95584437519, 0.0, 0.0, 0.0, 0.0, 0.0)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                  text  \\\n",
       "0  I walked rapidly, softly, and close to the ruined houses.                                                                                                                             \n",
       "1  The youth's febrile mind, apparently, was dwelling on strange things; and the doctor shuddered now and then as he spoke of them.                                                      \n",
       "2  \"But since the murderer has been discovered \"\" \"\"The murderer discovered Good God how can that be? who could attempt to pursue him?\"                                                  \n",
       "3  So thick were the vapours that the way was hard, and though Atal followed on at last, he could scarce see the grey shape of Barzai on the dim slope above in the clouded moonlight.   \n",
       "4  By authority of the king such districts were placed under ban, and all persons forbidden, under pain of death, to intrude upon their dismal solitude.                                 \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            rawFeatures  \\\n",
       "0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0)   \n",
       "1  (1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)   \n",
       "2  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 3.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)   \n",
       "3  (0.0, 0.0, 0.0, 0.0, 1.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)   \n",
       "4  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0)   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             features  \n",
       "0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.28480660233, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.40340830361, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.34785505396, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.43523626321, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.37494740591, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.92606070011, 0.0)                                                                                                    \n",
       "1  (2.16563034477, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.06264553022, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.24261075656, 0.0, 0.0, 0.0, 0.0, 0.0, 2.34785505396, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.02156144685, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.37137171676, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.99059922124, 2.17536278169, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.77890305577, 0.0, 0.0, 0.0, 0.0, 0.0, 1.96650166966, 2.27233693682, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)                                                  \n",
       "2  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.4950110513, 0.0, 0.0, 2.18469716536, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.95820286685, 6.03589213122, 0.0, 2.34785505396, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.77398097856, 1.97527015948, 2.01654261574, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.07274440858, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.22287138877, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.21272702844, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.77890305577, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)                                         \n",
       "3  (0.0, 0.0, 0.0, 0.0, 1.96650166966, 3.78762387259, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.4950110513, 0.0, 4.12529106043, 2.18469716536, 0.0, 0.0, 0.0, 2.0758383337, 0.0, 0.0, 2.17145843315, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.77398097856, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.32544676214, 0.0, 0.0, 2.07274440858, 0.0, 0.0, 0.0, 0.0, 0.0, 2.35076457012, 0.0, 0.0, 0.0, 0.0, 2.18174004147, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.17097145968, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.04489230361, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.39423673213, 0.0, 2.01612551458, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)  \n",
       "4  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.29247367165, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0758383337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.77398097856, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.43523626321, 0.0, 4.65089352427, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.59734496689, 0.0, 2.24261075656, 0.0, 2.10139453289, 0.0, 2.15935467851, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.01612551458, 2.21069863018, 0.0, 0.0, 0.0, 1.95584437519, 0.0, 0.0, 0.0, 0.0, 0.0)                                         "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=100)\n",
    "featurizedData = hashingTF.transform(removed)\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "rescaledData.select(\"text\", \"rawFeatures\", \"features\").toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode the label column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "labelIndexer = StringIndexer(inputCol='author', outputCol='label').fit(dataUnion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Logistic Regression Algorithm to predict author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(labelCol = \"label\", maxIter=10, regParam=0.3, threshold=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert indexed labels back to original labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import IndexToString\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", labels=labelIndexer.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the machine learning pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages = [tokenizer, remover, hashingTF, idf, labelIndexer, lr, labelConverter]\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages = stages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the parameter setting of the pipeline stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer:\n",
      "inputCol: input column name. (current: text)\n",
      "outputCol: output column name. (default: Tokenizer_401cbc4723a57585160c__output, current: words)\n",
      "*************************\n",
      "Remover:\n",
      "caseSensitive: whether to do a case sensitive comparison over the stop words (default: False, current: False)\n",
      "inputCol: input column name. (current: words)\n",
      "outputCol: output column name. (default: StopWordsRemover_48c489e32d6f6e9e1e5d__output, current: filtered)\n",
      "stopWords: The words to be filtered out (default: [u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'you', u'your', u'yours', u'yourself', u'yourselves', u'he', u'him', u'his', u'himself', u'she', u'her', u'hers', u'herself', u'it', u'its', u'itself', u'they', u'them', u'their', u'theirs', u'themselves', u'what', u'which', u'who', u'whom', u'this', u'that', u'these', u'those', u'am', u'is', u'are', u'was', u'were', u'be', u'been', u'being', u'have', u'has', u'had', u'having', u'do', u'does', u'did', u'doing', u'a', u'an', u'the', u'and', u'but', u'if', u'or', u'because', u'as', u'until', u'while', u'of', u'at', u'by', u'for', u'with', u'about', u'against', u'between', u'into', u'through', u'during', u'before', u'after', u'above', u'below', u'to', u'from', u'up', u'down', u'in', u'out', u'on', u'off', u'over', u'under', u'again', u'further', u'then', u'once', u'here', u'there', u'when', u'where', u'why', u'how', u'all', u'any', u'both', u'each', u'few', u'more', u'most', u'other', u'some', u'such', u'no', u'nor', u'not', u'only', u'own', u'same', u'so', u'than', u'too', u'very', u's', u't', u'can', u'will', u'just', u'don', u'should', u'now', u'd', u'll', u'm', u'o', u're', u've', u'y', u'ain', u'aren', u'couldn', u'didn', u'doesn', u'hadn', u'hasn', u'haven', u'isn', u'ma', u'mightn', u'mustn', u'needn', u'shan', u'shouldn', u'wasn', u'weren', u'won', u'wouldn'])\n",
      "*************************\n",
      "HashingTF:\n",
      "binary: If True, all non zero counts are set to 1. This is useful for discrete probabilistic models that model binary events rather than integer counts. Default False. (default: False)\n",
      "inputCol: input column name. (current: filtered)\n",
      "numFeatures: number of features. (default: 262144, current: 100)\n",
      "outputCol: output column name. (default: HashingTF_41fc8d7f633cadffaef9__output, current: rawFeatures)\n",
      "*************************\n",
      "IDF:\n",
      "inputCol: input column name. (current: rawFeatures)\n",
      "minDocFreq: minimum number of documents in which a term should appear for filtering (default: 0)\n",
      "outputCol: output column name. (default: IDF_41a3a1c478daeaa2502f__output, current: features)\n",
      "*************************\n",
      "LogisticRegression:\n",
      "aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\n",
      "family: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial (default: auto)\n",
      "featuresCol: features column name. (default: features)\n",
      "fitIntercept: whether to fit an intercept term. (default: True)\n",
      "labelCol: label column name. (default: label, current: label)\n",
      "maxIter: max number of iterations (>= 0). (default: 100, current: 10)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
      "rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\n",
      "regParam: regularization parameter (>= 0). (default: 0.0, current: 0.3)\n",
      "standardization: whether to standardize the training features before fitting the model. (default: True)\n",
      "threshold: Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p]. (default: 0.5, current: 0.7)\n",
      "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n",
      "*************************\n",
      "Pipeline:\n",
      "stages: a list of pipeline stages (current: [Tokenizer_401cbc4723a57585160c, StopWordsRemover_48c489e32d6f6e9e1e5d, HashingTF_41fc8d7f633cadffaef9, IDF_41a3a1c478daeaa2502f, StringIndexer_4fb4a0243956ab943f66, LogisticRegression_4aeeb6550e8126643dd8, IndexToString_477abf57d81c4f54872c])\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenizer:\")\n",
    "print(tokenizer.explainParams())\n",
    "print(\"*************************\")\n",
    "print(\"Remover:\")\n",
    "print(remover.explainParams())\n",
    "print(\"*************************\")\n",
    "print(\"HashingTF:\")\n",
    "print(hashingTF.explainParams())\n",
    "print(\"*************************\")\n",
    "print(\"IDF:\")\n",
    "print(idf.explainParams())\n",
    "print(\"*************************\")\n",
    "print(\"LogisticRegression:\")\n",
    "print(lr.explainParams())\n",
    "print(\"*************************\")\n",
    "print(\"Pipeline:\")\n",
    "print(pipeline.explainParams())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the dataset into training and test data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of records in the traininig data set is 12546.\n",
      "The number of rows labeled EAP in the training data set is 4906.\n",
      "The number of rows labeled HPL in the training data set is 3800.\n",
      "The number of rows labeled MWS in the training data set is 3840.\n",
      "\n",
      "The number of records in the test data set is 5460.\n",
      "The number of rows labeled EAP in the test data set is 2127.\n",
      "The number of rows labeled HPL in the test data set is 1628.\n",
      "The number of rows labeled MWS in the test data set is 1705.\n"
     ]
    }
   ],
   "source": [
    "train, test = dataUnion.randomSplit([70.0,30.0], seed=1)\n",
    "print('The number of records in the traininig data set is {}.'.format(train.count()))\n",
    "print('The number of rows labeled EAP in the training data set is {}.'.format(train.filter(train['author'] == 'EAP').count()))\n",
    "print('The number of rows labeled HPL in the training data set is {}.'.format(train.filter(train['author'] == 'HPL').count()))\n",
    "print('The number of rows labeled MWS in the training data set is {}.'.format(train.filter(train['author'] == 'MWS').count()))\n",
    "print(\"\")\n",
    "print('The number of records in the test data set is {}.'.format(test.count()))\n",
    "print('The number of rows labeled EAP in the test data set is {}.'.format(test.filter(test['author'] == 'EAP').count()))\n",
    "print('The number of rows labeled HPL in the test data set is {}.'.format(test.filter(test['author'] == 'HPL').count()))\n",
    "print('The number of rows labeled MWS in the test data set is {}.'.format(test.filter(test['author'] == 'MWS').count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model using the training data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions using the test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>label</th>\n",
       "      <th>prediction</th>\n",
       "      <th>predictedLabel</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EAP</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[0.452491734009, 0.226369926301, 0.32113833969]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EAP</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[0.434162771416, 0.31316910382, 0.252668124764]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MWS</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>HPL</td>\n",
       "      <td>[0.308629162933, 0.269640610805, 0.421730226262]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MWS</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[0.420694534605, 0.32656573462, 0.252739730775]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MWS</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[0.371616214344, 0.320021501658, 0.308362283998]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  author  label  prediction predictedLabel  \\\n",
       "0  EAP    0      0           EAP             \n",
       "1  EAP    0      0           EAP             \n",
       "2  MWS    1      2           HPL             \n",
       "3  MWS    1      0           EAP             \n",
       "4  MWS    1      0           EAP             \n",
       "\n",
       "                                        probability  \n",
       "0  [0.452491734009, 0.226369926301, 0.32113833969]   \n",
       "1  [0.434162771416, 0.31316910382, 0.252668124764]   \n",
       "2  [0.308629162933, 0.269640610805, 0.421730226262]  \n",
       "3  [0.420694534605, 0.32656573462, 0.252739730775]   \n",
       "4  [0.371616214344, 0.320021501658, 0.308362283998]  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.select(\"author\", \"label\", \"prediction\", 'predictedLabel', \"probability\").toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model performance by calculating the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 45.55%.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol = \"label\", predictionCol=\"prediction\").setMetricName(\"accuracy\")\n",
    "print('Accuracy = {:0.2f}%.'.format(evaluator.evaluate(predictions)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate the prediction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted EAP correctly 1703 times.\n",
      "Failed to predict EAP 424 times.\n",
      "Predicted EAP incorrectly 2137 times.\n"
     ]
    }
   ],
   "source": [
    "EAPandEAP = predictions.filter(predictions['author']=='EAP').filter(predictions['predictedLabel']=='EAP').count()\n",
    "EAPnotEAP = predictions.filter(predictions['author']=='EAP').filter(predictions['predictedLabel']!='EAP').count()\n",
    "notEAPbutEAP = predictions.filter(predictions['author']!='EAP').filter(predictions['predictedLabel']=='EAP').count()\n",
    "print(\"Predicted EAP correctly {} times.\".format(EAPandEAP))\n",
    "print(\"Failed to predict EAP {} times.\".format(EAPnotEAP))\n",
    "print(\"Predicted EAP incorrectly {} times.\".format(notEAPbutEAP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted HPL correctly 466 times.\n",
      "Failed to predict HPL 1162 times.\n",
      "Predicted HPL incorrectly 483 times.\n"
     ]
    }
   ],
   "source": [
    "HPLandHPL = predictions.filter(predictions['author']=='HPL').filter(predictions['predictedLabel']=='HPL').count()\n",
    "HPLnotHPL = predictions.filter(predictions['author']=='HPL').filter(predictions['predictedLabel']!='HPL').count()\n",
    "notHPLbutHPL = predictions.filter(predictions['author']!='HPL').filter(predictions['predictedLabel']=='HPL').count()\n",
    "print(\"Predicted HPL correctly {} times.\".format(HPLandHPL))\n",
    "print(\"Failed to predict HPL {} times.\".format(HPLnotHPL))\n",
    "print(\"Predicted HPL incorrectly {} times.\".format(notHPLbutHPL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted MWS correctly 318 times.\n",
      "Failed to predict MWS 1387 times.\n",
      "Predicted MWS incorrectly 353 times.\n"
     ]
    }
   ],
   "source": [
    "MWSandMWS = predictions.filter(predictions['author']=='MWS').filter(predictions['predictedLabel']=='MWS').count()\n",
    "MWSnotMWS = predictions.filter(predictions['author']=='MWS').filter(predictions['predictedLabel']!='MWS').count()\n",
    "notMWSbutMWS = predictions.filter(predictions['author']!='MWS').filter(predictions['predictedLabel']=='MWS').count()\n",
    "print(\"Predicted MWS correctly {} times.\".format(MWSandMWS))\n",
    "print(\"Failed to predict MWS {} times.\".format(MWSnotMWS))\n",
    "print(\"Predicted MWS incorrectly {} times.\".format(notMWSbutMWS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Natural Language Watson Understanding to create rich features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup configuration for the Watson Natural Language Understanding (NLU) service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import watson_developer_cloud\n",
    "from watson_developer_cloud import NaturalLanguageUnderstandingV1\n",
    "import watson_developer_cloud.natural_language_understanding.features.v1 as Features\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLU_USERNAME = '14b446f0-f05e-4c1b-9fa2-5a116e24761b'\n",
    "NLU_PASSWORD = 'ufIeGCHBfyvt'\n",
    "natural_language_understanding = NaturalLanguageUnderstandingV1(\n",
    "  username=NLU_USERNAME,\n",
    "  password=NLU_PASSWORD,\n",
    "  version=\"2017-02-27\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show example of employing NLU API on single row of the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"I walked rapidly, softly, and close to the ruined houses.\"\n",
      "\n",
      "Anger = 0.417696\n",
      "Joy = 0.021155\n",
      "Sadness = 0.482235\n",
      "Fear = 0.298235\n",
      "Disgust = 0.134835\n",
      "Sentiment = 0.0\n",
      "\n",
      "{\n",
      "  \"usage\": {\n",
      "    \"text_characters\": 59, \n",
      "    \"features\": 2, \n",
      "    \"text_units\": 1\n",
      "  }, \n",
      "  \"emotion\": {\n",
      "    \"document\": {\n",
      "      \"emotion\": {\n",
      "        \"anger\": 0.417696, \n",
      "        \"joy\": 0.021155, \n",
      "        \"sadness\": 0.482235, \n",
      "        \"fear\": 0.298235, \n",
      "        \"disgust\": 0.134835\n",
      "      }\n",
      "    }\n",
      "  }, \n",
      "  \"language\": \"en\", \n",
      "  \"sentiment\": {\n",
      "    \"document\": {\n",
      "      \"score\": 0.0, \n",
      "      \"label\": \"neutral\"\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "dataNLUtest = dataUnion.select(dataUnion[\"text\"]).toJSON().collect()[0][8:-1]\n",
    "print(dataNLUtest)\n",
    "import json\n",
    "features=[\n",
    "    Features.Emotion(),\n",
    "    Features.Sentiment()\n",
    "  ]\n",
    "nlu = natural_language_understanding.analyze(text=dataNLUtest, features=features, language='en', clean='true')\n",
    "anger = nlu['emotion']['document']['emotion']['anger']\n",
    "joy = nlu['emotion']['document']['emotion']['joy']\n",
    "sadness = nlu['emotion']['document']['emotion']['sadness']\n",
    "fear = nlu['emotion']['document']['emotion']['fear']\n",
    "disgust = nlu['emotion']['document']['emotion']['disgust']\n",
    "sentiment = nlu['sentiment']['document']['score']\n",
    "\n",
    "print(\"\")\n",
    "print(\"Anger = {}\".format(anger))\n",
    "print(\"Joy = {}\".format(joy))\n",
    "print(\"Sadness = {}\".format(sadness))\n",
    "print(\"Fear = {}\".format(fear))\n",
    "print(\"Disgust = {}\".format(disgust))\n",
    "print(\"Sentiment = {}\".format(sentiment))\n",
    "print(\"\")\n",
    "print(json.dumps(nlu, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define UDF to create NLU derived features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "#import json\n",
    "udfNLU = (udf(lambda text: json.dumps(NaturalLanguageUnderstandingV1(\n",
    "    username=NLU_USERNAME, password=NLU_PASSWORD, version=\"2017-02-27\")\n",
    "    .analyze(text=text, features=features, language='en', clean='true'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoke UDF to create new column with NLU output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataSmall = data.limit(10)\n",
    "#dataSmall = dataSmall.withColumn('nlu', udfNLU(dataSmall['text']))\n",
    "#dataSmall.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data00NLU = data00.withColumn('nlu', udfNLU(data00['text']))\n",
    "data01NLU = data01.withColumn('nlu', udfNLU(data01['text']))\n",
    "data02NLU = data02.withColumn('nlu', udfNLU(data02['text']))\n",
    "data03NLU = data03.withColumn('nlu', udfNLU(data03['text']))\n",
    "data04NLU = data04.withColumn('nlu', udfNLU(data04['text']))\n",
    "data05NLU = data05.withColumn('nlu', udfNLU(data05['text']))\n",
    "data06NLU = data06.withColumn('nlu', udfNLU(data06['text']))\n",
    "data07NLU = data07.withColumn('nlu', udfNLU(data07['text']))\n",
    "data08NLU = data08.withColumn('nlu', udfNLU(data08['text']))\n",
    "data09NLU = data09.withColumn('nlu', udfNLU(data09['text']))\n",
    "data10NLU = data10.withColumn('nlu', udfNLU(data10['text']))\n",
    "data11NLU = data11.withColumn('nlu', udfNLU(data11['text']))\n",
    "data12NLU = data12.withColumn('nlu', udfNLU(data12['text']))\n",
    "data13NLU = data13.withColumn('nlu', udfNLU(data13['text']))\n",
    "data14NLU = data14.withColumn('nlu', udfNLU(data14['text']))\n",
    "data15NLU = data15.withColumn('nlu', udfNLU(data15['text']))\n",
    "data16NLU = data16.withColumn('nlu', udfNLU(data16['text']))\n",
    "data17NLU = data17.withColumn('nlu', udfNLU(data17['text']))\n",
    "data18NLU = data18.withColumn('nlu', udfNLU(data18['text']))\n",
    "data19NLU = data19.withColumn('nlu', udfNLU(data19['text']))\n",
    "data20NLU = data20.withColumn('nlu', udfNLU(data20['text']))\n",
    "data21NLU = data21.withColumn('nlu', udfNLU(data21['text']))\n",
    "data22NLU = data22.withColumn('nlu', udfNLU(data22['text']))\n",
    "data23NLU = data23.withColumn('nlu', udfNLU(data23['text']))\n",
    "data24NLU = data24.withColumn('nlu', udfNLU(data24['text']))\n",
    "data25NLU = data25.withColumn('nlu', udfNLU(data25['text']))\n",
    "data26NLU = data26.withColumn('nlu', udfNLU(data26['text']))\n",
    "data27NLU = data27.withColumn('nlu', udfNLU(data27['text']))\n",
    "data28NLU = data28.withColumn('nlu', udfNLU(data28['text']))\n",
    "data29NLU = data29.withColumn('nlu', udfNLU(data29['text']))\n",
    "data30NLU = data30.withColumn('nlu', udfNLU(data30['text']))\n",
    "data31NLU = data31.withColumn('nlu', udfNLU(data31['text']))\n",
    "data32NLU = data32.withColumn('nlu', udfNLU(data32['text']))\n",
    "data33NLU = data33.withColumn('nlu', udfNLU(data33['text']))\n",
    "data34NLU = data34.withColumn('nlu', udfNLU(data34['text']))\n",
    "data35NLU = data35.withColumn('nlu', udfNLU(data35['text']))\n",
    "data36NLU = data36.withColumn('nlu', udfNLU(data36['text']))\n",
    "data37NLU = data37.withColumn('nlu', udfNLU(data37['text']))\n",
    "data38NLU = data38.withColumn('nlu', udfNLU(data38['text']))\n",
    "data39NLU = data39.withColumn('nlu', udfNLU(data39['text']))\n",
    "data40NLU = data40.withColumn('nlu', udfNLU(data40['text']))\n",
    "data41NLU = data41.withColumn('nlu', udfNLU(data41['text']))\n",
    "data42NLU = data42.withColumn('nlu', udfNLU(data42['text']))\n",
    "data43NLU = data43.withColumn('nlu', udfNLU(data43['text']))\n",
    "data44NLU = data44.withColumn('nlu', udfNLU(data44['text']))\n",
    "data45NLU = data45.withColumn('nlu', udfNLU(data45['text']))\n",
    "data46NLU = data46.withColumn('nlu', udfNLU(data46['text']))\n",
    "data47NLU = data47.withColumn('nlu', udfNLU(data47['text']))\n",
    "data48NLU = data48.withColumn('nlu', udfNLU(data48['text']))\n",
    "data49NLU = data49.withColumn('nlu', udfNLU(data49['text']))\n",
    "data50NLU = data50.withColumn('nlu', udfNLU(data50['text']))\n",
    "data51NLU = data51.withColumn('nlu', udfNLU(data51['text']))\n",
    "data52NLU = data52.withColumn('nlu', udfNLU(data52['text']))\n",
    "data53NLU = data53.withColumn('nlu', udfNLU(data53['text']))\n",
    "data54NLU = data54.withColumn('nlu', udfNLU(data54['text']))\n",
    "data55NLU = data55.withColumn('nlu', udfNLU(data55['text']))\n",
    "data56NLU = data56.withColumn('nlu', udfNLU(data56['text']))\n",
    "data57NLU = data57.withColumn('nlu', udfNLU(data57['text']))\n",
    "data58NLU = data58.withColumn('nlu', udfNLU(data58['text']))\n",
    "data59NLU = data59.withColumn('nlu', udfNLU(data59['text']))\n",
    "data60NLU = data60.withColumn('nlu', udfNLU(data60['text']))\n",
    "data61NLU = data61.withColumn('nlu', udfNLU(data61['text']))\n",
    "data62NLU = data62.withColumn('nlu', udfNLU(data62['text']))\n",
    "data63NLU = data63.withColumn('nlu', udfNLU(data63['text']))\n",
    "data64NLU = data64.withColumn('nlu', udfNLU(data64['text']))\n",
    "data65NLU = data65.withColumn('nlu', udfNLU(data65['text']))\n",
    "data66NLU = data66.withColumn('nlu', udfNLU(data66['text']))\n",
    "data67NLU = data67.withColumn('nlu', udfNLU(data67['text']))\n",
    "data68NLU = data68.withColumn('nlu', udfNLU(data68['text']))\n",
    "data69NLU = data69.withColumn('nlu', udfNLU(data69['text']))\n",
    "data70NLU = data70.withColumn('nlu', udfNLU(data70['text']))\n",
    "data71NLU = data71.withColumn('nlu', udfNLU(data71['text']))\n",
    "data72NLU = data72.withColumn('nlu', udfNLU(data72['text']))\n",
    "data73NLU = data73.withColumn('nlu', udfNLU(data73['text']))\n",
    "data74NLU = data74.withColumn('nlu', udfNLU(data74['text']))\n",
    "data75NLU = data75.withColumn('nlu', udfNLU(data75['text']))\n",
    "data76NLU = data76.withColumn('nlu', udfNLU(data76['text']))\n",
    "data77NLU = data77.withColumn('nlu', udfNLU(data77['text']))\n",
    "data78NLU = data78.withColumn('nlu', udfNLU(data78['text']))\n",
    "data79NLU = data79.withColumn('nlu', udfNLU(data79['text']))\n",
    "data80NLU = data80.withColumn('nlu', udfNLU(data80['text']))\n",
    "data81NLU = data81.withColumn('nlu', udfNLU(data81['text']))\n",
    "data82NLU = data82.withColumn('nlu', udfNLU(data82['text']))\n",
    "data83NLU = data83.withColumn('nlu', udfNLU(data83['text']))\n",
    "data84NLU = data84.withColumn('nlu', udfNLU(data84['text']))\n",
    "data85NLU = data85.withColumn('nlu', udfNLU(data85['text']))\n",
    "data86NLU = data86.withColumn('nlu', udfNLU(data86['text']))\n",
    "data87NLU = data87.withColumn('nlu', udfNLU(data87['text']))\n",
    "data88NLU = data88.withColumn('nlu', udfNLU(data88['text']))\n",
    "data89NLU = data89.withColumn('nlu', udfNLU(data89['text']))\n",
    "data90NLU = data90.withColumn('nlu', udfNLU(data90['text']))\n",
    "data91NLU = data91.withColumn('nlu', udfNLU(data91['text']))\n",
    "data92NLU = data92.withColumn('nlu', udfNLU(data92['text']))\n",
    "data93NLU = data93.withColumn('nlu', udfNLU(data93['text']))\n",
    "data94NLU = data94.withColumn('nlu', udfNLU(data94['text']))\n",
    "data95NLU = data95.withColumn('nlu', udfNLU(data95['text']))\n",
    "data96NLU = data96.withColumn('nlu', udfNLU(data96['text']))\n",
    "data97NLU = data97.withColumn('nlu', udfNLU(data97['text']))\n",
    "data98NLU = data98.withColumn('nlu', udfNLU(data98['text']))\n",
    "data99NLU = data99.withColumn('nlu', udfNLU(data99['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The combined dataset contains 18047 rows.\n"
     ]
    }
   ],
   "source": [
    "dataNLU= (data00NLU.union(data01NLU).union(data02NLU).union(data03NLU).union(data04NLU).union(data05NLU).union(data06NLU).union(data07NLU).union(data08NLU).union(data09NLU)\n",
    "            .union(data10NLU).union(data11NLU).union(data12NLU).union(data13NLU).union(data14NLU).union(data15NLU).union(data16NLU).union(data17NLU).union(data18NLU).union(data19NLU)\n",
    "            .union(data20NLU).union(data21NLU).union(data22NLU).union(data23NLU).union(data24NLU).union(data25NLU).union(data26NLU).union(data27NLU).union(data28NLU).union(data29NLU)\n",
    "            .union(data30NLU).union(data31NLU).union(data32NLU).union(data33NLU).union(data34NLU).union(data35NLU).union(data36NLU).union(data37NLU).union(data38NLU).union(data39NLU)\n",
    "            .union(data40NLU).union(data41NLU).union(data42NLU).union(data43NLU).union(data44NLU).union(data45NLU).union(data46NLU).union(data47NLU).union(data48NLU).union(data49NLU)\n",
    "            .union(data50NLU).union(data51NLU).union(data52NLU).union(data53NLU).union(data54NLU).union(data55NLU).union(data56NLU).union(data57NLU).union(data58NLU).union(data59NLU)\n",
    "            .union(data60NLU).union(data61NLU).union(data62NLU).union(data63NLU).union(data64NLU).union(data65NLU).union(data66NLU).union(data67NLU).union(data68NLU).union(data69NLU)\n",
    "            .union(data70NLU).union(data71NLU).union(data72NLU).union(data73NLU).union(data74NLU).union(data75NLU).union(data76NLU).union(data77NLU).union(data78NLU).union(data79NLU)\n",
    "            .union(data80NLU).union(data81NLU).union(data82NLU).union(data83NLU).union(data84NLU).union(data85NLU).union(data86NLU).union(data87NLU).union(data88NLU).union(data89NLU)\n",
    "            .union(data90NLU).union(data91NLU).union(data92NLU).union(data93NLU).union(data94NLU).union(data95NLU).union(data96NLU).union(data97NLU).union(data98NLU).union(data99NLU)\n",
    "            .cache())\n",
    "\n",
    "print(\"The combined dataset contains {} rows.\".format(dataNLU.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define UDFs to identify bad rows retuned by NLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "udfAngerTest = udf(lambda nlu: json.loads(nlu))\n",
    "udfJoyTest = udf(lambda nlu: json.loads(nlu))\n",
    "udfSadnessTest = udf(lambda nlu: json.loads(nlu))\n",
    "udfFearTest = udf(lambda nlu: json.loads(nlu))\n",
    "udfDisgustTest = udf(lambda nlu: json.loads(nlu))\n",
    "udfSentimentTest = udf(lambda nlu: json.loads(nlu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataNLUtest = (dataNLU.withColumn('AngerTest', udfAngerTest(dataNLU['nlu']))\n",
    "        .withColumn('JoyTest', udfJoyTest(dataNLU['nlu']))\n",
    "        .withColumn('SadnessTest', udfSadnessTest(dataNLU['nlu']))\n",
    "        .withColumn('FearTest', udfFearTest(dataNLU['nlu']))\n",
    "        .withColumn('DisgustTest', udfDisgustTest(dataNLU['nlu']))\n",
    "        .withColumn('SentimentTest', udfSentimentTest(dataNLU['nlu'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of bad rows found = 0\n",
      "Number of bad rows found = 0\n",
      "Number of bad rows found = 0\n",
      "Number of bad rows found = 0\n",
      "Number of bad rows found = 0\n",
      "Number of bad rows found = 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of bad rows found = {}\".format(dataNLUtest.filter(~ col('AngerTest').like('%anger%')).count()))\n",
    "print(\"Number of bad rows found = {}\".format(dataNLUtest.filter(~ col('JoyTest').like('%joy%')).count()))\n",
    "print(\"Number of bad rows found = {}\".format(dataNLUtest.filter(~ col('SadnessTest').like('%sadness%')).count()))\n",
    "print(\"Number of bad rows found = {}\".format(dataNLUtest.filter(~ col('FearTest').like('%fear%')).count()))\n",
    "print(\"Number of bad rows found = {}\".format(dataNLUtest.filter(~ col('DisgustTest').like('%disgust%')).count()))\n",
    "print(\"Number of bad rows found = {}\".format(dataNLUtest.filter(~ col('SentimentTest').like('%sentiment%')).count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove bad rows returned from NLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataNLU = (dataNLUtest.filter(col('AngerTest').like('%anger%'))\n",
    "            .filter(col('JoyTest').like('%joy%'))\n",
    "            .filter(col('SadnessTest').like('%sadness%'))\n",
    "            .filter(col('FearTest').like('%fear%'))\n",
    "            .filter(col('DisgustTest').like('%disgust%'))\n",
    "            .filter(col('SentimentTest').like('%sentiment%')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of bad rows found = 0\n",
      "Number of bad rows found = 0\n",
      "Number of bad rows found = 0\n",
      "Number of bad rows found = 0\n",
      "Number of bad rows found = 0\n",
      "Number of bad rows found = 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of bad rows found = {}\".format(dataNLU.filter(~ col('AngerTest').like('%anger%')).count()))\n",
    "print(\"Number of bad rows found = {}\".format(dataNLU.filter(~ col('JoyTest').like('%joy%')).count()))\n",
    "print(\"Number of bad rows found = {}\".format(dataNLU.filter(~ col('SadnessTest').like('%sadness%')).count()))\n",
    "print(\"Number of bad rows found = {}\".format(dataNLU.filter(~ col('FearTest').like('%fear%')).count()))\n",
    "print(\"Number of bad rows found = {}\".format(dataNLU.filter(~ col('DisgustTest').like('%disgust%')).count()))\n",
    "print(\"Number of bad rows found = {}\".format(dataNLU.filter(~ col('SentimentTest').like('%sentiment%')).count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop NLU test columnts\n",
    "dataNLU = dataNLU.drop('AngerTest','JoyTest', 'SadnessTest', 'FearTest', 'DisgustTest', 'SentimentTest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define UDFs to extract NLU derived features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "udfAnger = udf(lambda nlu: json.loads(nlu)[\"emotion\"][\"document\"][\"emotion\"][\"anger\"], DoubleType())\n",
    "udfJoy = udf(lambda nlu: json.loads(nlu)[\"emotion\"][\"document\"][\"emotion\"][\"joy\"], DoubleType())\n",
    "udfSadness = udf(lambda nlu: json.loads(nlu)[\"emotion\"][\"document\"][\"emotion\"][\"sadness\"], DoubleType())\n",
    "udfFear = udf(lambda nlu: json.loads(nlu)[\"emotion\"][\"document\"][\"emotion\"][\"fear\"], DoubleType())\n",
    "udfDisgust = udf(lambda nlu: json.loads(nlu)[\"emotion\"][\"document\"][\"emotion\"][\"disgust\"], DoubleType())\n",
    "udfSentiment = udf(lambda nlu: json.loads(nlu)['sentiment']['document']['score'], DoubleType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoke UDFs to create new columns for the enhanced emotion and sentiment features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataNLU2 = (dataSmall.withColumn('Anger', udfAnger(dataSmall['nlu']))\n",
    "#        .withColumn('Joy', udfJoy(dataSmall['nlu']))\n",
    "#        .withColumn('Sadness', udfSadness(dataSmall['nlu']))\n",
    "#        .withColumn('Fear', udfFear(dataSmall['nlu']))\n",
    "#        .withColumn('Disgust', udfDisgust(dataSmall['nlu']))\n",
    "#        .withColumn('Sentiment', udfSentiment(dataSmall['nlu'])))\n",
    "#dataNLU2.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataNLU = (dataNLU.withColumn('Anger', udfAnger(dataNLU['nlu']))\n",
    "        .withColumn('Joy', udfJoy(dataNLU['nlu']))\n",
    "        .withColumn('Sadness', udfSadness(dataNLU['nlu']))\n",
    "        .withColumn('Fear', udfFear(dataNLU['nlu']))\n",
    "        .withColumn('Disgust', udfDisgust(dataNLU['nlu']))\n",
    "        .withColumn('Sentiment', udfSentiment(dataNLU['nlu'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Anger</th>\n",
       "      <th>Joy</th>\n",
       "      <th>Sadness</th>\n",
       "      <th>Fear</th>\n",
       "      <th>Disgust</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I walked rapidly, softly, and close to the ruined houses.</td>\n",
       "      <td>0.417696</td>\n",
       "      <td>0.021155</td>\n",
       "      <td>0.482235</td>\n",
       "      <td>0.298235</td>\n",
       "      <td>0.134835</td>\n",
       "      <td>-0.651749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The youth's febrile mind, apparently, was dwelling on strange things; and the doctor shuddered now and then as he spoke of them.</td>\n",
       "      <td>0.055444</td>\n",
       "      <td>0.116561</td>\n",
       "      <td>0.365396</td>\n",
       "      <td>0.351243</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>-0.765087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"But since the murderer has been discovered \"\" \"\"The murderer discovered Good God how can that be? who could attempt to pursue him?\"</td>\n",
       "      <td>0.218197</td>\n",
       "      <td>0.137784</td>\n",
       "      <td>0.393301</td>\n",
       "      <td>0.066075</td>\n",
       "      <td>0.429190</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>So thick were the vapours that the way was hard, and though Atal followed on at last, he could scarce see the grey shape of Barzai on the dim slope above in the clouded moonlight.</td>\n",
       "      <td>0.032738</td>\n",
       "      <td>0.150773</td>\n",
       "      <td>0.694929</td>\n",
       "      <td>0.203870</td>\n",
       "      <td>0.029030</td>\n",
       "      <td>-0.506536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>By authority of the king such districts were placed under ban, and all persons forbidden, under pain of death, to intrude upon their dismal solitude.</td>\n",
       "      <td>0.247888</td>\n",
       "      <td>0.026101</td>\n",
       "      <td>0.771124</td>\n",
       "      <td>0.095373</td>\n",
       "      <td>0.122879</td>\n",
       "      <td>-0.858506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Thus, while Perdita was entertaining her guests, and anxiously awaiting the arrival of her lord, his ring was brought her; and she was told that a poor woman had a note to deliver to her from its wearer.</td>\n",
       "      <td>0.098105</td>\n",
       "      <td>0.104288</td>\n",
       "      <td>0.470236</td>\n",
       "      <td>0.089047</td>\n",
       "      <td>0.107151</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>As the evening wore away he became more and more absorbed in reverie, from which no sallies of mine could arouse him.</td>\n",
       "      <td>0.110423</td>\n",
       "      <td>0.141169</td>\n",
       "      <td>0.527687</td>\n",
       "      <td>0.172764</td>\n",
       "      <td>0.117253</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>He stopped in his tracks then, flailing his arms wildly in the air, began to stagger backward.</td>\n",
       "      <td>0.234947</td>\n",
       "      <td>0.170794</td>\n",
       "      <td>0.115705</td>\n",
       "      <td>0.310494</td>\n",
       "      <td>0.091302</td>\n",
       "      <td>-0.868316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>In feeling my way I had found many angles, and thus deduced an idea of great irregularity; so potent is the effect of total darkness upon one arousing from lethargy or sleep The angles were simply those of a few slight depressions, or niches, at odd intervals.</td>\n",
       "      <td>0.021526</td>\n",
       "      <td>0.102365</td>\n",
       "      <td>0.736927</td>\n",
       "      <td>0.158123</td>\n",
       "      <td>0.025682</td>\n",
       "      <td>-0.891721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>He seemed insensible to the presence of any one else, but if, as a trial to awaken his sensibility, my aunt brought me into the room he would instantly rush out with every symptom of fury and distraction.</td>\n",
       "      <td>0.437653</td>\n",
       "      <td>0.155442</td>\n",
       "      <td>0.238697</td>\n",
       "      <td>0.203852</td>\n",
       "      <td>0.013623</td>\n",
       "      <td>-0.915947</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                   text  \\\n",
       "0  I walked rapidly, softly, and close to the ruined houses.                                                                                                                                                                                                              \n",
       "1  The youth's febrile mind, apparently, was dwelling on strange things; and the doctor shuddered now and then as he spoke of them.                                                                                                                                       \n",
       "2  \"But since the murderer has been discovered \"\" \"\"The murderer discovered Good God how can that be? who could attempt to pursue him?\"                                                                                                                                   \n",
       "3  So thick were the vapours that the way was hard, and though Atal followed on at last, he could scarce see the grey shape of Barzai on the dim slope above in the clouded moonlight.                                                                                    \n",
       "4  By authority of the king such districts were placed under ban, and all persons forbidden, under pain of death, to intrude upon their dismal solitude.                                                                                                                  \n",
       "5  Thus, while Perdita was entertaining her guests, and anxiously awaiting the arrival of her lord, his ring was brought her; and she was told that a poor woman had a note to deliver to her from its wearer.                                                            \n",
       "6  As the evening wore away he became more and more absorbed in reverie, from which no sallies of mine could arouse him.                                                                                                                                                  \n",
       "7  He stopped in his tracks then, flailing his arms wildly in the air, began to stagger backward.                                                                                                                                                                         \n",
       "8  In feeling my way I had found many angles, and thus deduced an idea of great irregularity; so potent is the effect of total darkness upon one arousing from lethargy or sleep The angles were simply those of a few slight depressions, or niches, at odd intervals.   \n",
       "9  He seemed insensible to the presence of any one else, but if, as a trial to awaken his sensibility, my aunt brought me into the room he would instantly rush out with every symptom of fury and distraction.                                                           \n",
       "\n",
       "      Anger       Joy   Sadness      Fear   Disgust  Sentiment  \n",
       "0  0.417696  0.021155  0.482235  0.298235  0.134835 -0.651749   \n",
       "1  0.055444  0.116561  0.365396  0.351243  0.088965 -0.765087   \n",
       "2  0.218197  0.137784  0.393301  0.066075  0.429190  0.000000   \n",
       "3  0.032738  0.150773  0.694929  0.203870  0.029030 -0.506536   \n",
       "4  0.247888  0.026101  0.771124  0.095373  0.122879 -0.858506   \n",
       "5  0.098105  0.104288  0.470236  0.089047  0.107151  0.000000   \n",
       "6  0.110423  0.141169  0.527687  0.172764  0.117253  0.000000   \n",
       "7  0.234947  0.170794  0.115705  0.310494  0.091302 -0.868316   \n",
       "8  0.021526  0.102365  0.736927  0.158123  0.025682 -0.891721   \n",
       "9  0.437653  0.155442  0.238697  0.203852  0.013623 -0.915947   "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataNLU.select(dataNLU['text'], dataNLU['Anger'], dataNLU['Joy'], dataNLU['Sadness'], dataNLU['Fear'], dataNLU['Disgust'], dataNLU['Sentiment']).toPandas().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrain model with NLU features added"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the dataset into training and test data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainNLU, testNLU = dataNLU.randomSplit([70.0,30.0], seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bucketize the NLU features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Bucketizer\n",
    "AngerBucketSplits = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "AngerBucket = Bucketizer(splits=AngerBucketSplits, inputCol=\"Anger\", outputCol=\"AngerBucket\")\n",
    "JoyBucketSplits = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "JoyBucket = Bucketizer(splits=JoyBucketSplits, inputCol=\"Joy\", outputCol=\"JoyBucket\")\n",
    "SadnessBucketSplits = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "SadnessBucket = Bucketizer(splits=SadnessBucketSplits, inputCol=\"Sadness\", outputCol=\"SadnessBucket\")\n",
    "FearBucketSplits = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "FearBucket = Bucketizer(splits=FearBucketSplits, inputCol=\"Fear\", outputCol=\"FearBucket\")\n",
    "DisgustBucketSplits = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "DisgustBucket = Bucketizer(splits=DisgustBucketSplits, inputCol=\"Disgust\", outputCol=\"DisgustBucket\")\n",
    "SentimentBucketSplits = [-1.0, -0.9, -0.8, -0.7, -0.6, -0.5, -0.4, -0.3, -0.2, -0.1, 0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "SentimentBucket = Bucketizer(splits=SentimentBucketSplits, inputCol=\"Sentiment\", outputCol=\"SentimentBucket\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = (VectorAssembler(inputCols=[\"features\", \"AngerBucket\", \"JoyBucket\", \"SadnessBucket\", \"FearBucket\", \"DisgustBucket\",\"SentimentBucket\"], \n",
    "             outputCol=\"featuresNLU\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a revised machine learning pipeline utilizing the new bucketed NLU feaures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrNLU = LogisticRegression(labelCol = \"label\", featuresCol= \"featuresNLU\", maxIter=10, regParam=0.3, threshold=0.5)\n",
    "stagesNLU = ([tokenizer, remover, hashingTF, idf, AngerBucket, JoyBucket, SadnessBucket, FearBucket, DisgustBucket, SentimentBucket,\n",
    "            assembler, labelIndexer, lrNLU, labelConverter])\n",
    "pipelineNLU = Pipeline(stages = stagesNLU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the new model using the training data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelNLU = pipelineNLU.fit(trainNLU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make updated predictions (with NLU features) using the test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionsNLU = modelNLU.transform(testNLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>label</th>\n",
       "      <th>prediction</th>\n",
       "      <th>predictedLabel</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EAP</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[0.406589386585, 0.259198863094, 0.334211750321]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EAP</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[0.445357856625, 0.310846362146, 0.24379578123]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MWS</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[0.341663611648, 0.316793807219, 0.341542581133]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MWS</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[0.43338070479, 0.341784103287, 0.224835191923]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MWS</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[0.404221211604, 0.286485576834, 0.309293211562]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  author  label  prediction predictedLabel  \\\n",
       "0  EAP    0      0           EAP             \n",
       "1  EAP    0      0           EAP             \n",
       "2  MWS    1      0           EAP             \n",
       "3  MWS    1      0           EAP             \n",
       "4  MWS    1      0           EAP             \n",
       "\n",
       "                                        probability  \n",
       "0  [0.406589386585, 0.259198863094, 0.334211750321]  \n",
       "1  [0.445357856625, 0.310846362146, 0.24379578123]   \n",
       "2  [0.341663611648, 0.316793807219, 0.341542581133]  \n",
       "3  [0.43338070479, 0.341784103287, 0.224835191923]   \n",
       "4  [0.404221211604, 0.286485576834, 0.309293211562]  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictionsNLU.select(\"author\", \"label\", \"prediction\", 'predictedLabel', \"probability\").toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the updated model performance by calculating the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with NLU = 47.94%.\n"
     ]
    }
   ],
   "source": [
    "evaluatorNLU = MulticlassClassificationEvaluator(labelCol = \"label\", predictionCol=\"prediction\").setMetricName(\"accuracy\")\n",
    "print('Accuracy with NLU = {:0.2f}%.'.format(evaluatorNLU.evaluate(predictionsNLU)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate Improved Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted EAP correctly 1597 times vs. 1703 previously.\n",
      "Failed to predict EAP 532 times vs. 424 previously.\n",
      "Predicted EAP incorrectly 1805 times vs. 2137 previously.\n"
     ]
    }
   ],
   "source": [
    "EAPandEAPnlu = predictionsNLU.filter(predictionsNLU['author']=='EAP').filter(predictionsNLU['predictedLabel']=='EAP').count()\n",
    "EAPnotEAPnlu = predictionsNLU.filter(predictionsNLU['author']=='EAP').filter(predictionsNLU['predictedLabel']!='EAP').count()\n",
    "notEAPbutEAPnlu = predictionsNLU.filter(predictionsNLU['author']!='EAP').filter(predictionsNLU['predictedLabel']=='EAP').count()\n",
    "print(\"Predicted EAP correctly {} times vs. {} previously.\".format(EAPandEAPnlu, EAPandEAP))\n",
    "print(\"Failed to predict EAP {} times vs. {} previously.\".format(EAPnotEAPnlu, EAPnotEAP))\n",
    "print(\"Predicted EAP incorrectly {} times vs. {} previously.\".format(notEAPbutEAPnlu, notEAPbutEAP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted HPL correctly 582 times vs. 466 previously.\n",
      "Failed to predict HPL 1050 times vs. 1162 previously.\n",
      "Predicted HPL incorrectly 581 times vs. 483 previously.\n"
     ]
    }
   ],
   "source": [
    "HPLandHPLnlu = predictionsNLU.filter(predictionsNLU['author']=='HPL').filter(predictionsNLU['predictedLabel']=='HPL').count()\n",
    "HPLnotHPLnlu = predictionsNLU.filter(predictionsNLU['author']=='HPL').filter(predictionsNLU['predictedLabel']!='HPL').count()\n",
    "notHPLbutHPLnlu = predictionsNLU.filter(predictionsNLU['author']!='HPL').filter(predictionsNLU['predictedLabel']=='HPL').count()\n",
    "print(\"Predicted HPL correctly {} times vs. {} previously.\".format(HPLandHPLnlu, HPLandHPL))\n",
    "print(\"Failed to predict HPL {} times vs. {} previously.\".format(HPLnotHPLnlu, HPLnotHPL))\n",
    "print(\"Predicted HPL incorrectly {} times vs. {} previously.\".format(notHPLbutHPLnlu, notHPLbutHPL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted MWS correctly 444 times vs. 318 previously.\n",
      "Failed to predict MWS 1266 times vs. 1387 previously.\n",
      "Predicted MWS incorrectly 462 times vs. 353 previously.\n"
     ]
    }
   ],
   "source": [
    "MWSandMWSnlu = predictionsNLU.filter(predictionsNLU['author']=='MWS').filter(predictionsNLU['predictedLabel']=='MWS').count()\n",
    "MWSnotMWSnlu = predictionsNLU.filter(predictionsNLU['author']=='MWS').filter(predictionsNLU['predictedLabel']!='MWS').count()\n",
    "notMWSbutMWSnlu = predictionsNLU.filter(predictionsNLU['author']!='MWS').filter(predictionsNLU['predictedLabel']=='MWS').count()\n",
    "print(\"Predicted MWS correctly {} times vs. {} previously.\".format(MWSandMWSnlu, MWSandMWS))\n",
    "print(\"Failed to predict MWS {} times vs. {} previously.\".format(MWSnotMWSnlu, MWSnotMWS))\n",
    "print(\"Predicted MWS incorrectly {} times vs. {} previously.\".format(notMWSbutMWSnlu, notMWSbutMWS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![IBM Logo](http://www-03.ibm.com/press/img/Large_IBM_Logo_TN.jpg)\n",
    "\n",
    "Rich Tarro  \n",
    "Solutions Architect, IBM Corporation  \n",
    "rtarro@us.ibm.com\n",
    "\n",
    "December 8, 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 with Spark 2.1",
   "language": "python",
   "name": "python2-spark21"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
