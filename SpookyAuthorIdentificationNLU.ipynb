{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://storage.googleapis.com/kaggle-media/competitions/spooky-books/dmitrij-paskevic-44124.jpg\" style=\"width:200px; float: left; padding-right: 10px\"/>\n",
    "<h2 style=\"font-face: verdana; font-size: 32px;\">Spooky Author Identification</h2>\n",
    "<h3 style=\"font-face: verdana; font-size: 16px;\">Derive rich features for Machine Learning with the Watson Cognitive APIs</h3>\n",
    "<br><br>\n",
    "<a href=\"https://www.kaggle.com/c/spooky-author-identification/\">Spooky Author Identification Kaggle Competition</a>\n",
    "\n",
    "<h3 style=\"font-face: verdana; font-size: 16px;\">The objective of this machine learning model is to predict the author of excerpts from horror stories by Edgar Allan Poe, Mary Shelley, and HP Lovecraft.</h3>\n",
    "\n",
    "The dataset contains text from works of fiction written by these spooky authors. The goal is to accurately identify the author of the sentences.\n",
    "\n",
    "Data fields in the dataset:\n",
    "\n",
    "    id - a unique identifier for each sentence\n",
    "    text - some text written by one of the authors\n",
    "    author - the author of the sentence (EAP: Edgar Allan Poe, HPL: HP Lovecraft; MWS: Mary Wollstonecraft Shelley)\n",
    "\n",
    "<h3 style=\"font-face: verdana; font-size: 16px;\">Approach</h3>\n",
    "\n",
    "We will approach this challenge by first using a traditional multiclassification machine learning approach. We will then explore using IBM Watson Natural Language Understanding to derive additional enhanced features on which to learn a machine learning model.\n",
    "\n",
    "<h3 style=\"font-face: verdana; font-size: 16px;\">IBM Watson Natural Language Understanding</h3>\n",
    "\n",
    "IBM Watsonâ„¢ Natural Language Understanding (NLU) can analyze semantic features of text input, including categories, concepts, emotion, entities, keywords, metadata, relations, semantic roles, and sentiment. In this example, we will utilize the emotion and sentiment features of NLU to create enhanced machine learning features.\n",
    "\n",
    "\n",
    "<h4 style=\"font-face: verdana; font-size: 16px;\">Emotion</h4>\n",
    "\n",
    "The emotion feature of NLU allows you to analyze emotion conveyed by specific target phrases or by the document as a whole. You can also enable emotion analysis for entities and keywords that are automatically detected by the service. In this example, we will simply analyze the spooky excerpt as a whole. The emotions we will derive features for are \n",
    "\n",
    "- Anger\n",
    "- Joy\n",
    "- Sadness\n",
    "- Fear\n",
    "- Disgust\n",
    "\n",
    "Emotion scores range from 0 to 1 for sadness, joy, fear, disgust, and anger. A 0 means the text doesn't convey the emotion, and a 1 means the text definitely carries the emotion.\n",
    "\n",
    "<h4 style=\"font-face: verdana; font-size: 16px;\">Sentiment</h4>\n",
    "\n",
    "The sentiment feature of NLU allows you to analyze the sentiment toward specific target phrases and the sentiment of the document as a whole. You can also get sentiment information for detected entities and keywords by enabling the sentiment option for those features. In this example, we will simply analyze the spooky excerpt as a whole.\n",
    "\n",
    "The sentiment score ranges from -1 (negative sentiment) to 1 (positive sentiment).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Flow\n",
    "\n",
    "\n",
    "**Part 1: [Traditional Machine Learning](#Part-1:-Traditional-Machine-Learning)**\n",
    "  1. [Read in the Data](#1.-Read-in-the-Data)<br>\n",
    "  2. [Clean the Data](#2.-Clean-the-Data)<br>\n",
    "  3. [Feature Engineering](#3.-Feature-Engineering)<br>\n",
    "  4. [Build ML Pipeline and Learn a Model](#4.-Build-ML-Pipeline-and-Learn-a-Model)<br>\n",
    "  5. [Evaluate the Model](#5.-Evaluate-the-Model)\n",
    "    \n",
    "**Part 2: [Machine Learning using Watson Cognitive APIs](#Part-2:-Machine-Learning-using-Watson-Cognitive-APIs)**\n",
    "  1. [Set up for Use of the Natural Language Understanding Service](#1.-Set-up-for-Use-of-the-Natural-Language-Understanding-Service)<br>\n",
    "  2. [Create Enhanced Features using the NLU Service](#2.-Create-Enhanced-Features-using-the-NLU-Service)<br>\n",
    "  3. [Retrain Model with NLU Features Added](#3.-Retrain-Model-with-NLU-Features-Added)\n",
    "  4. [Evaluate the Model Learned with NLU Features](#4.-Evaluate-the-Model-Learned-with-NLU-Features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Traditional Machine Learning\n",
    "\n",
    "This section will employ a traditional multiclassification machine learning approach to learning a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read in the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and unzip the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if os.path.isfile('train.zip'):\n",
    "    os.remove(\"train.zip\")\n",
    "if os.path.isfile('train.csv'):\n",
    "    os.remove(\"train.csv\")\n",
    "import wget\n",
    "url = 'https://github.com/hackerguy/SpookyAuthorIdentification/blob/master/train.zip?raw=true'\n",
    "wget.download(url)\n",
    "import zipfile\n",
    "zip = zipfile.ZipFile('train.zip', 'r')\n",
    "zip.extractall()\n",
    "zip.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the data set as a Spark DataFrame\n",
    "#### Infer schema and column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "data = (spark.read\n",
    "  .format('csv')\n",
    "  .option('header', 'true')\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .load('train.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of ascertaining the dimensions of my dungeon; as I might make its circuit, and return to the point whence I set out, without being aware of the fact; so perfectly uniform seemed the wall.</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling might be a mere mistake.</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from which, as he capered down the hill, cutting all manner of fantastic steps, he took snuff incessantly with an air of the greatest possible self satisfaction.</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor Terrace on the sixteen fertile counties spread beneath, speckled by happy cottages and wealthier towns, all looked as in former years, heart cheering and fair.</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Superintendent abandoned his attempts; but a perplexed look occasionally steals over his countenance as he sits thinking at his desk.</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  \\\n",
       "0  id26305   \n",
       "1  id17569   \n",
       "2  id11008   \n",
       "3  id27763   \n",
       "4  id12958   \n",
       "\n",
       "                                                                                                                                                                                                                                      text  \\\n",
       "0  This process, however, afforded me no means of ascertaining the dimensions of my dungeon; as I might make its circuit, and return to the point whence I set out, without being aware of the fact; so perfectly uniform seemed the wall.   \n",
       "1  It never once occurred to me that the fumbling might be a mere mistake.                                                                                                                                                                   \n",
       "2  In his left hand was a gold snuff box, from which, as he capered down the hill, cutting all manner of fantastic steps, he took snuff incessantly with an air of the greatest possible self satisfaction.                                  \n",
       "3  How lovely is spring As we looked from Windsor Terrace on the sixteen fertile counties spread beneath, speckled by happy cottages and wealthier towns, all looked as in former years, heart cheering and fair.                            \n",
       "4  Finding nothing else, not even gold, the Superintendent abandoned his attempts; but a perplexed look occasionally steals over his countenance as he sits thinking at his desk.                                                            \n",
       "\n",
       "  author  \n",
       "0  EAP    \n",
       "1  HPL    \n",
       "2  EAP    \n",
       "3  MWS    \n",
       "4  HPL    "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "data.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the schema of the data including data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- author: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Overview - number of rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 19579 rows in the dataset.\n",
      "There are 3 columns in the dataset.\n"
     ]
    }
   ],
   "source": [
    "print(\"There are {} rows in the dataset.\".format(str(data.count())))\n",
    "print(\"There are {} columns in the dataset.\".format(str(len(data.columns))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optionally use a subset of the data for processing efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraction = 1.0\n",
    "data = data.sample(False, fraction, seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clean the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove rows that do not have valid author fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.filter((data['author']=='EAP')| (data['author']=='HPL') | (data['author']=='MWS'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove punctuation from the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>textNoPunctuation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of ascertaining the dimensions of my dungeon; as I might make its circuit, and return to the point whence I set out, without being aware of the fact; so perfectly uniform seemed the wall.</td>\n",
       "      <td>This process however afforded me no means of ascertaining the dimensions of my dungeon as I might make its circuit and return to the point whence I set out without being aware of the fact so perfectly uniform seemed the wall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling might be a mere mistake.</td>\n",
       "      <td>It never once occurred to me that the fumbling might be a mere mistake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from which, as he capered down the hill, cutting all manner of fantastic steps, he took snuff incessantly with an air of the greatest possible self satisfaction.</td>\n",
       "      <td>In his left hand was a gold snuff box from which as he capered down the hill cutting all manner of fantastic steps he took snuff incessantly with an air of the greatest possible self satisfaction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor Terrace on the sixteen fertile counties spread beneath, speckled by happy cottages and wealthier towns, all looked as in former years, heart cheering and fair.</td>\n",
       "      <td>How lovely is spring As we looked from Windsor Terrace on the sixteen fertile counties spread beneath speckled by happy cottages and wealthier towns all looked as in former years heart cheering and fair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Superintendent abandoned his attempts; but a perplexed look occasionally steals over his countenance as he sits thinking at his desk.</td>\n",
       "      <td>Finding nothing else not even gold the Superintendent abandoned his attempts but a perplexed look occasionally steals over his countenance as he sits thinking at his desk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  \\\n",
       "0  id26305   \n",
       "1  id17569   \n",
       "2  id11008   \n",
       "3  id27763   \n",
       "4  id12958   \n",
       "\n",
       "                                                                                                                                                                                                                                      text  \\\n",
       "0  This process, however, afforded me no means of ascertaining the dimensions of my dungeon; as I might make its circuit, and return to the point whence I set out, without being aware of the fact; so perfectly uniform seemed the wall.   \n",
       "1  It never once occurred to me that the fumbling might be a mere mistake.                                                                                                                                                                   \n",
       "2  In his left hand was a gold snuff box, from which, as he capered down the hill, cutting all manner of fantastic steps, he took snuff incessantly with an air of the greatest possible self satisfaction.                                  \n",
       "3  How lovely is spring As we looked from Windsor Terrace on the sixteen fertile counties spread beneath, speckled by happy cottages and wealthier towns, all looked as in former years, heart cheering and fair.                            \n",
       "4  Finding nothing else, not even gold, the Superintendent abandoned his attempts; but a perplexed look occasionally steals over his countenance as he sits thinking at his desk.                                                            \n",
       "\n",
       "                                                                                                                                                                                                                  textNoPunctuation  \n",
       "0  This process however afforded me no means of ascertaining the dimensions of my dungeon as I might make its circuit and return to the point whence I set out without being aware of the fact so perfectly uniform seemed the wall  \n",
       "1  It never once occurred to me that the fumbling might be a mere mistake                                                                                                                                                            \n",
       "2  In his left hand was a gold snuff box from which as he capered down the hill cutting all manner of fantastic steps he took snuff incessantly with an air of the greatest possible self satisfaction                               \n",
       "3  How lovely is spring As we looked from Windsor Terrace on the sixteen fertile counties spread beneath speckled by happy cottages and wealthier towns all looked as in former years heart cheering and fair                        \n",
       "4  Finding nothing else not even gold the Superintendent abandoned his attempts but a perplexed look occasionally steals over his countenance as he sits thinking at his desk                                                        "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import SQLTransformer\n",
    "removePunctuationTrans = SQLTransformer(\n",
    "    statement=\"\"\"SELECT *, TRANSLATE(text,',.;?\\''\"+','') AS textNoPunctuation FROM __THIS__\"\"\")\n",
    "data = removePunctuationTrans.transform(data)\n",
    "data.select('id', 'text', 'textNoPunctuation').toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>EAP</td>\n",
       "      <td>This process however afforded me no means of ascertaining the dimensions of my dungeon as I might make its circuit and return to the point whence I set out without being aware of the fact so perfectly uniform seemed the wall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>HPL</td>\n",
       "      <td>It never once occurred to me that the fumbling might be a mere mistake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>EAP</td>\n",
       "      <td>In his left hand was a gold snuff box from which as he capered down the hill cutting all manner of fantastic steps he took snuff incessantly with an air of the greatest possible self satisfaction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>MWS</td>\n",
       "      <td>How lovely is spring As we looked from Windsor Terrace on the sixteen fertile counties spread beneath speckled by happy cottages and wealthier towns all looked as in former years heart cheering and fair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>HPL</td>\n",
       "      <td>Finding nothing else not even gold the Superintendent abandoned his attempts but a perplexed look occasionally steals over his countenance as he sits thinking at his desk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id author  \\\n",
       "0  id26305  EAP     \n",
       "1  id17569  HPL     \n",
       "2  id11008  EAP     \n",
       "3  id27763  MWS     \n",
       "4  id12958  HPL     \n",
       "\n",
       "                                                                                                                                                                                                                               text  \n",
       "0  This process however afforded me no means of ascertaining the dimensions of my dungeon as I might make its circuit and return to the point whence I set out without being aware of the fact so perfectly uniform seemed the wall  \n",
       "1  It never once occurred to me that the fumbling might be a mere mistake                                                                                                                                                            \n",
       "2  In his left hand was a gold snuff box from which as he capered down the hill cutting all manner of fantastic steps he took snuff incessantly with an air of the greatest possible self satisfaction                               \n",
       "3  How lovely is spring As we looked from Windsor Terrace on the sixteen fertile counties spread beneath speckled by happy cottages and wealthier towns all looked as in former years heart cheering and fair                        \n",
       "4  Finding nothing else not even gold the Superintendent abandoned his attempts but a perplexed look occasionally steals over his countenance as he sits thinking at his desk                                                        "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.drop('text').withColumnRenamed('textNoPunctuation', 'text')\n",
    "data.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are now 18047 rows in the dataset.\n"
     ]
    }
   ],
   "source": [
    "print(\"There are now {} rows in the dataset.\".format(str(data.count())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>words</th>\n",
       "      <th>#tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This process however afforded me no means of ascertaining the dimensions of my dungeon as I might make its circuit and return to the point whence I set out without being aware of the fact so perfectly uniform seemed the wall</td>\n",
       "      <td>[this, process, however, afforded, me, no, means, of, ascertaining, the, dimensions, of, my, dungeon, as, i, might, make, its, circuit, and, return, to, the, point, whence, i, set, out, without, being, aware, of, the, fact, so, perfectly, uniform, seemed, the, wall]</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It never once occurred to me that the fumbling might be a mere mistake</td>\n",
       "      <td>[it, never, once, occurred, to, me, that, the, fumbling, might, be, a, mere, mistake]</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In his left hand was a gold snuff box from which as he capered down the hill cutting all manner of fantastic steps he took snuff incessantly with an air of the greatest possible self satisfaction</td>\n",
       "      <td>[in, his, left, hand, was, a, gold, snuff, box, from, which, as, he, capered, down, the, hill, cutting, all, manner, of, fantastic, steps, he, took, snuff, incessantly, with, an, air, of, the, greatest, possible, self, satisfaction]</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How lovely is spring As we looked from Windsor Terrace on the sixteen fertile counties spread beneath speckled by happy cottages and wealthier towns all looked as in former years heart cheering and fair</td>\n",
       "      <td>[how, lovely, is, spring, as, we, looked, from, windsor, terrace, on, the, sixteen, fertile, counties, spread, beneath, speckled, by, happy, cottages, and, wealthier, towns, all, looked, as, in, former, years, heart, cheering, and, fair]</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Finding nothing else not even gold the Superintendent abandoned his attempts but a perplexed look occasionally steals over his countenance as he sits thinking at his desk</td>\n",
       "      <td>[finding, nothing, else, not, even, gold, the, superintendent, abandoned, his, attempts, but, a, perplexed, look, occasionally, steals, over, his, countenance, as, he, sits, thinking, at, his, desk]</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                               text  \\\n",
       "0  This process however afforded me no means of ascertaining the dimensions of my dungeon as I might make its circuit and return to the point whence I set out without being aware of the fact so perfectly uniform seemed the wall   \n",
       "1  It never once occurred to me that the fumbling might be a mere mistake                                                                                                                                                             \n",
       "2  In his left hand was a gold snuff box from which as he capered down the hill cutting all manner of fantastic steps he took snuff incessantly with an air of the greatest possible self satisfaction                                \n",
       "3  How lovely is spring As we looked from Windsor Terrace on the sixteen fertile counties spread beneath speckled by happy cottages and wealthier towns all looked as in former years heart cheering and fair                         \n",
       "4  Finding nothing else not even gold the Superintendent abandoned his attempts but a perplexed look occasionally steals over his countenance as he sits thinking at his desk                                                         \n",
       "\n",
       "                                                                                                                                                                                                                                                                        words  \\\n",
       "0  [this, process, however, afforded, me, no, means, of, ascertaining, the, dimensions, of, my, dungeon, as, i, might, make, its, circuit, and, return, to, the, point, whence, i, set, out, without, being, aware, of, the, fact, so, perfectly, uniform, seemed, the, wall]   \n",
       "1  [it, never, once, occurred, to, me, that, the, fumbling, might, be, a, mere, mistake]                                                                                                                                                                                        \n",
       "2  [in, his, left, hand, was, a, gold, snuff, box, from, which, as, he, capered, down, the, hill, cutting, all, manner, of, fantastic, steps, he, took, snuff, incessantly, with, an, air, of, the, greatest, possible, self, satisfaction]                                     \n",
       "3  [how, lovely, is, spring, as, we, looked, from, windsor, terrace, on, the, sixteen, fertile, counties, spread, beneath, speckled, by, happy, cottages, and, wealthier, towns, all, looked, as, in, former, years, heart, cheering, and, fair]                                \n",
       "4  [finding, nothing, else, not, even, gold, the, superintendent, abandoned, his, attempts, but, a, perplexed, look, occasionally, steals, over, his, countenance, as, he, sits, thinking, at, his, desk]                                                                       \n",
       "\n",
       "   #tokens  \n",
       "0  41       \n",
       "1  14       \n",
       "2  36       \n",
       "3  34       \n",
       "4  27       "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "\n",
    "countTokens = udf(lambda words: len(words), IntegerType())\n",
    "\n",
    "tokenized = tokenizer.transform(data)\n",
    "(tokenized.select(\"text\", \"words\")\n",
    "    .withColumn(\"#tokens\", countTokens(col(\"words\"))).toPandas().head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>words</th>\n",
       "      <th>filtered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This process however afforded me no means of ascertaining the dimensions of my dungeon as I might make its circuit and return to the point whence I set out without being aware of the fact so perfectly uniform seemed the wall</td>\n",
       "      <td>[this, process, however, afforded, me, no, means, of, ascertaining, the, dimensions, of, my, dungeon, as, i, might, make, its, circuit, and, return, to, the, point, whence, i, set, out, without, being, aware, of, the, fact, so, perfectly, uniform, seemed, the, wall]</td>\n",
       "      <td>[process, however, afforded, means, ascertaining, dimensions, dungeon, might, make, circuit, return, point, whence, set, without, aware, fact, perfectly, uniform, seemed, wall]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It never once occurred to me that the fumbling might be a mere mistake</td>\n",
       "      <td>[it, never, once, occurred, to, me, that, the, fumbling, might, be, a, mere, mistake]</td>\n",
       "      <td>[never, occurred, fumbling, might, mere, mistake]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In his left hand was a gold snuff box from which as he capered down the hill cutting all manner of fantastic steps he took snuff incessantly with an air of the greatest possible self satisfaction</td>\n",
       "      <td>[in, his, left, hand, was, a, gold, snuff, box, from, which, as, he, capered, down, the, hill, cutting, all, manner, of, fantastic, steps, he, took, snuff, incessantly, with, an, air, of, the, greatest, possible, self, satisfaction]</td>\n",
       "      <td>[left, hand, gold, snuff, box, capered, hill, cutting, manner, fantastic, steps, took, snuff, incessantly, air, greatest, possible, self, satisfaction]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How lovely is spring As we looked from Windsor Terrace on the sixteen fertile counties spread beneath speckled by happy cottages and wealthier towns all looked as in former years heart cheering and fair</td>\n",
       "      <td>[how, lovely, is, spring, as, we, looked, from, windsor, terrace, on, the, sixteen, fertile, counties, spread, beneath, speckled, by, happy, cottages, and, wealthier, towns, all, looked, as, in, former, years, heart, cheering, and, fair]</td>\n",
       "      <td>[lovely, spring, looked, windsor, terrace, sixteen, fertile, counties, spread, beneath, speckled, happy, cottages, wealthier, towns, looked, former, years, heart, cheering, fair]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Finding nothing else not even gold the Superintendent abandoned his attempts but a perplexed look occasionally steals over his countenance as he sits thinking at his desk</td>\n",
       "      <td>[finding, nothing, else, not, even, gold, the, superintendent, abandoned, his, attempts, but, a, perplexed, look, occasionally, steals, over, his, countenance, as, he, sits, thinking, at, his, desk]</td>\n",
       "      <td>[finding, nothing, else, even, gold, superintendent, abandoned, attempts, perplexed, look, occasionally, steals, countenance, sits, thinking, desk]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                               text  \\\n",
       "0  This process however afforded me no means of ascertaining the dimensions of my dungeon as I might make its circuit and return to the point whence I set out without being aware of the fact so perfectly uniform seemed the wall   \n",
       "1  It never once occurred to me that the fumbling might be a mere mistake                                                                                                                                                             \n",
       "2  In his left hand was a gold snuff box from which as he capered down the hill cutting all manner of fantastic steps he took snuff incessantly with an air of the greatest possible self satisfaction                                \n",
       "3  How lovely is spring As we looked from Windsor Terrace on the sixteen fertile counties spread beneath speckled by happy cottages and wealthier towns all looked as in former years heart cheering and fair                         \n",
       "4  Finding nothing else not even gold the Superintendent abandoned his attempts but a perplexed look occasionally steals over his countenance as he sits thinking at his desk                                                         \n",
       "\n",
       "                                                                                                                                                                                                                                                                        words  \\\n",
       "0  [this, process, however, afforded, me, no, means, of, ascertaining, the, dimensions, of, my, dungeon, as, i, might, make, its, circuit, and, return, to, the, point, whence, i, set, out, without, being, aware, of, the, fact, so, perfectly, uniform, seemed, the, wall]   \n",
       "1  [it, never, once, occurred, to, me, that, the, fumbling, might, be, a, mere, mistake]                                                                                                                                                                                        \n",
       "2  [in, his, left, hand, was, a, gold, snuff, box, from, which, as, he, capered, down, the, hill, cutting, all, manner, of, fantastic, steps, he, took, snuff, incessantly, with, an, air, of, the, greatest, possible, self, satisfaction]                                     \n",
       "3  [how, lovely, is, spring, as, we, looked, from, windsor, terrace, on, the, sixteen, fertile, counties, spread, beneath, speckled, by, happy, cottages, and, wealthier, towns, all, looked, as, in, former, years, heart, cheering, and, fair]                                \n",
       "4  [finding, nothing, else, not, even, gold, the, superintendent, abandoned, his, attempts, but, a, perplexed, look, occasionally, steals, over, his, countenance, as, he, sits, thinking, at, his, desk]                                                                       \n",
       "\n",
       "                                                                                                                                                                             filtered  \n",
       "0  [process, however, afforded, means, ascertaining, dimensions, dungeon, might, make, circuit, return, point, whence, set, without, aware, fact, perfectly, uniform, seemed, wall]    \n",
       "1  [never, occurred, fumbling, might, mere, mistake]                                                                                                                                   \n",
       "2  [left, hand, gold, snuff, box, capered, hill, cutting, manner, fantastic, steps, took, snuff, incessantly, air, greatest, possible, self, satisfaction]                             \n",
       "3  [lovely, spring, looked, windsor, terrace, sixteen, fertile, counties, spread, beneath, speckled, happy, cottages, wealthier, towns, looked, former, years, heart, cheering, fair]  \n",
       "4  [finding, nothing, else, even, gold, superintendent, abandoned, attempts, perplexed, look, occasionally, steals, countenance, sits, thinking, desk]                                 "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").setCaseSensitive(False)\n",
    "removed = remover.transform(tokenized)\n",
    "removed.select(\"text\", \"words\", \"filtered\" ).toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show list of common words removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\n",
      "me\n",
      "my\n",
      "myself\n",
      "we\n",
      "our\n",
      "ours\n",
      "ourselves\n",
      "you\n",
      "your\n",
      "yours\n",
      "yourself\n",
      "yourselves\n",
      "he\n",
      "him\n",
      "his\n",
      "himself\n",
      "she\n",
      "her\n",
      "hers\n",
      "herself\n",
      "it\n",
      "its\n",
      "itself\n",
      "they\n",
      "them\n",
      "their\n",
      "theirs\n",
      "themselves\n",
      "what\n",
      "which\n",
      "who\n",
      "whom\n",
      "this\n",
      "that\n",
      "these\n",
      "those\n",
      "am\n",
      "is\n",
      "are\n",
      "was\n",
      "were\n",
      "be\n",
      "been\n",
      "being\n",
      "have\n",
      "has\n",
      "had\n",
      "having\n",
      "do\n",
      "does\n",
      "did\n",
      "doing\n",
      "a\n",
      "an\n",
      "the\n",
      "and\n",
      "but\n",
      "if\n",
      "or\n",
      "because\n",
      "as\n",
      "until\n",
      "while\n",
      "of\n",
      "at\n",
      "by\n",
      "for\n",
      "with\n",
      "about\n",
      "against\n",
      "between\n",
      "into\n",
      "through\n",
      "during\n",
      "before\n",
      "after\n",
      "above\n",
      "below\n",
      "to\n",
      "from\n",
      "up\n",
      "down\n",
      "in\n",
      "out\n",
      "on\n",
      "off\n",
      "over\n",
      "under\n",
      "again\n",
      "further\n",
      "then\n",
      "once\n",
      "here\n",
      "there\n",
      "when\n",
      "where\n",
      "why\n",
      "how\n",
      "all\n",
      "any\n",
      "both\n",
      "each\n",
      "few\n",
      "more\n",
      "most\n",
      "other\n",
      "some\n",
      "such\n",
      "no\n",
      "nor\n",
      "not\n",
      "only\n",
      "own\n",
      "same\n",
      "so\n",
      "than\n",
      "too\n",
      "very\n",
      "s\n",
      "t\n",
      "can\n",
      "will\n",
      "just\n",
      "don\n",
      "should\n",
      "now\n",
      "d\n",
      "ll\n",
      "m\n",
      "o\n",
      "re\n",
      "ve\n",
      "y\n",
      "ain\n",
      "aren\n",
      "couldn\n",
      "didn\n",
      "doesn\n",
      "hadn\n",
      "hasn\n",
      "haven\n",
      "isn\n",
      "ma\n",
      "mightn\n",
      "mustn\n",
      "needn\n",
      "shan\n",
      "shouldn\n",
      "wasn\n",
      "weren\n",
      "won\n",
      "wouldn\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "[print(x) for x in remover.getStopWords()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hash the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>filtered</th>\n",
       "      <th>rawFeatures</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This process however afforded me no means of ascertaining the dimensions of my dungeon as I might make its circuit and return to the point whence I set out without being aware of the fact so perfectly uniform seemed the wall</td>\n",
       "      <td>[process, however, afforded, means, ascertaining, dimensions, dungeon, might, make, circuit, return, point, whence, set, without, aware, fact, perfectly, uniform, seemed, wall]</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 3.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 2.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It never once occurred to me that the fumbling might be a mere mistake</td>\n",
       "      <td>[never, occurred, fumbling, might, mere, mistake]</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In his left hand was a gold snuff box from which as he capered down the hill cutting all manner of fantastic steps he took snuff incessantly with an air of the greatest possible self satisfaction</td>\n",
       "      <td>[left, hand, gold, snuff, box, capered, hill, cutting, manner, fantastic, steps, took, snuff, incessantly, air, greatest, possible, self, satisfaction]</td>\n",
       "      <td>(1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How lovely is spring As we looked from Windsor Terrace on the sixteen fertile counties spread beneath speckled by happy cottages and wealthier towns all looked as in former years heart cheering and fair</td>\n",
       "      <td>[lovely, spring, looked, windsor, terrace, sixteen, fertile, counties, spread, beneath, speckled, happy, cottages, wealthier, towns, looked, former, years, heart, cheering, fair]</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 2.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Finding nothing else not even gold the Superintendent abandoned his attempts but a perplexed look occasionally steals over his countenance as he sits thinking at his desk</td>\n",
       "      <td>[finding, nothing, else, even, gold, superintendent, abandoned, attempts, perplexed, look, occasionally, steals, countenance, sits, thinking, desk]</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 2.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                               text  \\\n",
       "0  This process however afforded me no means of ascertaining the dimensions of my dungeon as I might make its circuit and return to the point whence I set out without being aware of the fact so perfectly uniform seemed the wall   \n",
       "1  It never once occurred to me that the fumbling might be a mere mistake                                                                                                                                                             \n",
       "2  In his left hand was a gold snuff box from which as he capered down the hill cutting all manner of fantastic steps he took snuff incessantly with an air of the greatest possible self satisfaction                                \n",
       "3  How lovely is spring As we looked from Windsor Terrace on the sixteen fertile counties spread beneath speckled by happy cottages and wealthier towns all looked as in former years heart cheering and fair                         \n",
       "4  Finding nothing else not even gold the Superintendent abandoned his attempts but a perplexed look occasionally steals over his countenance as he sits thinking at his desk                                                         \n",
       "\n",
       "                                                                                                                                                                             filtered  \\\n",
       "0  [process, however, afforded, means, ascertaining, dimensions, dungeon, might, make, circuit, return, point, whence, set, without, aware, fact, perfectly, uniform, seemed, wall]     \n",
       "1  [never, occurred, fumbling, might, mere, mistake]                                                                                                                                    \n",
       "2  [left, hand, gold, snuff, box, capered, hill, cutting, manner, fantastic, steps, took, snuff, incessantly, air, greatest, possible, self, satisfaction]                              \n",
       "3  [lovely, spring, looked, windsor, terrace, sixteen, fertile, counties, spread, beneath, speckled, happy, cottages, wealthier, towns, looked, former, years, heart, cheering, fair]   \n",
       "4  [finding, nothing, else, even, gold, superintendent, abandoned, attempts, perplexed, look, occasionally, steals, countenance, sits, thinking, desk]                                  \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            rawFeatures  \n",
       "0  (0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 3.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 2.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0)  \n",
       "1  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)  \n",
       "2  (1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0)  \n",
       "3  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 2.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)  \n",
       "4  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 2.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=100)\n",
    "featurizedData = hashingTF.transform(removed)\n",
    "featurizedData.select(\"text\", \"filtered\", \"rawFeatures\").toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse weight words that occur frequently across all text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>rawFeatures</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This process however afforded me no means of ascertaining the dimensions of my dungeon as I might make its circuit and return to the point whence I set out without being aware of the fact so perfectly uniform seemed the wall</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 3.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 2.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0)</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 1.96008370255, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.51250710909, 0.0, 2.1009477469, 0.0, 0.0, 6.9190798178, 0.0, 0.0, 0.0, 2.41060872607, 0.0, 0.0, 1.96996715916, 0.0, 0.0, 0.0, 0.0, 1.78579325995, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.25804660893, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.24907793895, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.10230736642, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.01009412312, 0.0, 0.0, 0.0, 0.0, 0.0, 2.09103328984, 0.0, 0.0, 2.39711986429, 2.01839981871, 3.89981497898, 1.83385665589, 0.0, 0.0, 0.0, 0.0, 0.0, 2.08432935412, 0.0, 0.0, 1.94835106894, 0.0, 0.0, 0.0, 0.0, 0.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It never once occurred to me that the fumbling might be a mere mistake</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.40752705953, 0.0, 1.51250710909, 0.0, 0.0, 0.0, 0.0, 0.0, 2.4318197521, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.09148182091, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0246746772, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.83385665589, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In his left hand was a gold snuff box from which as he capered down the hill cutting all manner of fantastic steps he took snuff incessantly with an air of the greatest possible self satisfaction</td>\n",
       "      <td>(1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0)</td>\n",
       "      <td>(2.13597436901, 0.0, 0.0, 2.0870055377, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.28762660906, 0.0, 2.43245046799, 0.0, 0.0, 0.0, 0.0, 0.0, 2.05969106426, 0.0, 0.0, 4.22192297112, 0.0, 0.0, 0.0, 0.0, 2.39651103626, 0.0, 0.0, 0.0, 0.0, 0.0, 2.12478022227, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.17404369236, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.18106594052, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.9162136437, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.25540040469, 2.08432935412, 0.0, 2.21196027599, 1.94835106894, 0.0, 2.24436218486, 0.0, 1.9436962894, 0.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How lovely is spring As we looked from Windsor Terrace on the sixteen fertile counties spread beneath speckled by happy cottages and wealthier towns all looked as in former years heart cheering and fair</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 2.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.40384155168, 0.0, 0.0, 2.05969106426, 2.25645804624, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.79302207252, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.36299503263, 4.51609321786, 1.96560639903, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.15491432911, 1.95851137518, 2.58702184618, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.03225685337, 0.0, 2.18106594052, 0.0, 0.0, 2.07061535905, 0.0, 2.19092795338, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.34065778503, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.39711986429, 0.0, 3.89981497898, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Finding nothing else not even gold the Superintendent abandoned his attempts but a perplexed look occasionally steals over his countenance as he sits thinking at his desk</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 2.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.23137836185, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.22880170492, 0.0, 0.0, 1.78579325995, 0.0, 0.0, 4.02101554719, 2.12478022227, 0.0, 0.0, 0.0, 0.0, 2.36299503263, 0.0, 0.0, 0.0, 5.10599514506, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.17032889251, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.39711986429, 0.0, 1.94990748949, 0.0, 2.43308158192, 0.0, 2.15730724722, 2.06143095161, 0.0, 0.0, 0.0, 2.21196027599, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                               text  \\\n",
       "0  This process however afforded me no means of ascertaining the dimensions of my dungeon as I might make its circuit and return to the point whence I set out without being aware of the fact so perfectly uniform seemed the wall   \n",
       "1  It never once occurred to me that the fumbling might be a mere mistake                                                                                                                                                             \n",
       "2  In his left hand was a gold snuff box from which as he capered down the hill cutting all manner of fantastic steps he took snuff incessantly with an air of the greatest possible self satisfaction                                \n",
       "3  How lovely is spring As we looked from Windsor Terrace on the sixteen fertile counties spread beneath speckled by happy cottages and wealthier towns all looked as in former years heart cheering and fair                         \n",
       "4  Finding nothing else not even gold the Superintendent abandoned his attempts but a perplexed look occasionally steals over his countenance as he sits thinking at his desk                                                         \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            rawFeatures  \\\n",
       "0  (0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 3.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 2.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0)   \n",
       "1  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)   \n",
       "2  (1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0)   \n",
       "3  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 2.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)   \n",
       "4  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 2.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 features  \n",
       "0  (0.0, 0.0, 0.0, 0.0, 1.96008370255, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.51250710909, 0.0, 2.1009477469, 0.0, 0.0, 6.9190798178, 0.0, 0.0, 0.0, 2.41060872607, 0.0, 0.0, 1.96996715916, 0.0, 0.0, 0.0, 0.0, 1.78579325995, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.25804660893, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.24907793895, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.10230736642, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.01009412312, 0.0, 0.0, 0.0, 0.0, 0.0, 2.09103328984, 0.0, 0.0, 2.39711986429, 2.01839981871, 3.89981497898, 1.83385665589, 0.0, 0.0, 0.0, 0.0, 0.0, 2.08432935412, 0.0, 0.0, 1.94835106894, 0.0, 0.0, 0.0, 0.0, 0.0)  \n",
       "1  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.40752705953, 0.0, 1.51250710909, 0.0, 0.0, 0.0, 0.0, 0.0, 2.4318197521, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.09148182091, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0246746772, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.83385665589, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)                                                                                                                          \n",
       "2  (2.13597436901, 0.0, 0.0, 2.0870055377, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.28762660906, 0.0, 2.43245046799, 0.0, 0.0, 0.0, 0.0, 0.0, 2.05969106426, 0.0, 0.0, 4.22192297112, 0.0, 0.0, 0.0, 0.0, 2.39651103626, 0.0, 0.0, 0.0, 0.0, 0.0, 2.12478022227, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.17404369236, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.18106594052, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.9162136437, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.25540040469, 2.08432935412, 0.0, 2.21196027599, 1.94835106894, 0.0, 2.24436218486, 0.0, 1.9436962894, 0.0)             \n",
       "3  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.40384155168, 0.0, 0.0, 2.05969106426, 2.25645804624, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.79302207252, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.36299503263, 4.51609321786, 1.96560639903, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.15491432911, 1.95851137518, 2.58702184618, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.03225685337, 0.0, 2.18106594052, 0.0, 0.0, 2.07061535905, 0.0, 2.19092795338, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.34065778503, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.39711986429, 0.0, 3.89981497898, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)          \n",
       "4  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.23137836185, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.22880170492, 0.0, 0.0, 1.78579325995, 0.0, 0.0, 4.02101554719, 2.12478022227, 0.0, 0.0, 0.0, 0.0, 2.36299503263, 0.0, 0.0, 0.0, 5.10599514506, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.17032889251, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.39711986429, 0.0, 1.94990748949, 0.0, 2.43308158192, 0.0, 2.15730724722, 2.06143095161, 0.0, 0.0, 0.0, 2.21196027599, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)                                        "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import IDF\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "rescaledData.select(\"text\", \"rawFeatures\", \"features\").toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build ML Pipeline and Learn a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode the label column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "labelIndexer = StringIndexer(inputCol='author', outputCol='label').fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Logistic Regression Algorithm to predict author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(labelCol = \"label\", maxIter=10, regParam=0.3, threshold=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert indexed labels back to original labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import IndexToString\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", labels=labelIndexer.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the machine learning pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages = [removePunctuationTrans, tokenizer, remover, hashingTF, idf, labelIndexer, lr, labelConverter]\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages = stages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the parameter setting of the pipeline stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove Punctuation SQL Transformer:\n",
      "statement: SQL statement (current: SELECT *, TRANSLATE(text,',.;?''\"+','') AS textNoPunctuation FROM __THIS__)\n",
      "*************************\n",
      "inputCol: input column name. (current: text)\n",
      "outputCol: output column name. (default: Tokenizer_4bfba813c56a101d3f2a__output, current: words)\n",
      "*************************\n",
      "Tokenizer:\n",
      "inputCol: input column name. (current: text)\n",
      "outputCol: output column name. (default: Tokenizer_4bfba813c56a101d3f2a__output, current: words)\n",
      "*************************\n",
      "Remover:\n",
      "caseSensitive: whether to do a case sensitive comparison over the stop words (default: False, current: False)\n",
      "inputCol: input column name. (current: words)\n",
      "outputCol: output column name. (default: StopWordsRemover_4ced937aa312c16bd618__output, current: filtered)\n",
      "stopWords: The words to be filtered out (default: [u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'you', u'your', u'yours', u'yourself', u'yourselves', u'he', u'him', u'his', u'himself', u'she', u'her', u'hers', u'herself', u'it', u'its', u'itself', u'they', u'them', u'their', u'theirs', u'themselves', u'what', u'which', u'who', u'whom', u'this', u'that', u'these', u'those', u'am', u'is', u'are', u'was', u'were', u'be', u'been', u'being', u'have', u'has', u'had', u'having', u'do', u'does', u'did', u'doing', u'a', u'an', u'the', u'and', u'but', u'if', u'or', u'because', u'as', u'until', u'while', u'of', u'at', u'by', u'for', u'with', u'about', u'against', u'between', u'into', u'through', u'during', u'before', u'after', u'above', u'below', u'to', u'from', u'up', u'down', u'in', u'out', u'on', u'off', u'over', u'under', u'again', u'further', u'then', u'once', u'here', u'there', u'when', u'where', u'why', u'how', u'all', u'any', u'both', u'each', u'few', u'more', u'most', u'other', u'some', u'such', u'no', u'nor', u'not', u'only', u'own', u'same', u'so', u'than', u'too', u'very', u's', u't', u'can', u'will', u'just', u'don', u'should', u'now', u'd', u'll', u'm', u'o', u're', u've', u'y', u'ain', u'aren', u'couldn', u'didn', u'doesn', u'hadn', u'hasn', u'haven', u'isn', u'ma', u'mightn', u'mustn', u'needn', u'shan', u'shouldn', u'wasn', u'weren', u'won', u'wouldn'])\n",
      "*************************\n",
      "HashingTF:\n",
      "binary: If True, all non zero counts are set to 1. This is useful for discrete probabilistic models that model binary events rather than integer counts. Default False. (default: False)\n",
      "inputCol: input column name. (current: filtered)\n",
      "numFeatures: number of features. (default: 262144, current: 100)\n",
      "outputCol: output column name. (default: HashingTF_429e8c14fade11e95b7d__output, current: rawFeatures)\n",
      "*************************\n",
      "IDF:\n",
      "inputCol: input column name. (current: rawFeatures)\n",
      "minDocFreq: minimum number of documents in which a term should appear for filtering (default: 0)\n",
      "outputCol: output column name. (default: IDF_4e48b3a8d64bd211d06e__output, current: features)\n",
      "*************************\n",
      "LogisticRegression:\n",
      "aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\n",
      "family: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial (default: auto)\n",
      "featuresCol: features column name. (default: features)\n",
      "fitIntercept: whether to fit an intercept term. (default: True)\n",
      "labelCol: label column name. (default: label, current: label)\n",
      "maxIter: max number of iterations (>= 0). (default: 100, current: 10)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
      "rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\n",
      "regParam: regularization parameter (>= 0). (default: 0.0, current: 0.3)\n",
      "standardization: whether to standardize the training features before fitting the model. (default: True)\n",
      "threshold: Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p]. (default: 0.5, current: 0.7)\n",
      "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n",
      "*************************\n",
      "Pipeline:\n",
      "stages: a list of pipeline stages (current: [SQLTransformer_4e3b9c2505c2595d77d7, Tokenizer_4bfba813c56a101d3f2a, StopWordsRemover_4ced937aa312c16bd618, HashingTF_429e8c14fade11e95b7d, IDF_4e48b3a8d64bd211d06e, StringIndexer_4f27b8f66c7b42320ea7, LogisticRegression_4b2a95e88fb52e78b7d1, IndexToString_449ba2c8f83cd39c9798])\n"
     ]
    }
   ],
   "source": [
    "print(\"Remove Punctuation SQL Transformer:\")\n",
    "print(removePunctuationTrans.explainParams())\n",
    "print(\"*************************\")\n",
    "print(tokenizer.explainParams())\n",
    "print(\"*************************\")\n",
    "print(\"Tokenizer:\")\n",
    "print(tokenizer.explainParams())\n",
    "print(\"*************************\")\n",
    "print(\"Remover:\")\n",
    "print(remover.explainParams())\n",
    "print(\"*************************\")\n",
    "print(\"HashingTF:\")\n",
    "print(hashingTF.explainParams())\n",
    "print(\"*************************\")\n",
    "print(\"IDF:\")\n",
    "print(idf.explainParams())\n",
    "print(\"*************************\")\n",
    "print(\"LogisticRegression:\")\n",
    "print(lr.explainParams())\n",
    "print(\"*************************\")\n",
    "print(\"Pipeline:\")\n",
    "print(pipeline.explainParams())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset into training and test data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of records in the training data set is 12609.\n",
      "The number of rows labeled EAP in the training data set is 4883.\n",
      "The number of rows labeled HPL in the training data set is 3813.\n",
      "The number of rows labeled MWS in the training data set is 3913.\n",
      "\n",
      "The number of records in the test data set is 5438.\n",
      "The number of rows labeled EAP in the test data set is 2161.\n",
      "The number of rows labeled HPL in the test data set is 1638.\n",
      "The number of rows labeled MWS in the test data set is 1639.\n"
     ]
    }
   ],
   "source": [
    "train, test = data.randomSplit([70.0,30.0], seed=1)\n",
    "print('The number of records in the training data set is {}.'.format(train.count()))\n",
    "print('The number of rows labeled EAP in the training data set is {}.'.format(train.filter(train['author'] == 'EAP').count()))\n",
    "print('The number of rows labeled HPL in the training data set is {}.'.format(train.filter(train['author'] == 'HPL').count()))\n",
    "print('The number of rows labeled MWS in the training data set is {}.'.format(train.filter(train['author'] == 'MWS').count()))\n",
    "print(\"\")\n",
    "print('The number of records in the test data set is {}.'.format(test.count()))\n",
    "print('The number of rows labeled EAP in the test data set is {}.'.format(test.filter(test['author'] == 'EAP').count()))\n",
    "print('The number of rows labeled HPL in the test data set is {}.'.format(test.filter(test['author'] == 'HPL').count()))\n",
    "print('The number of rows labeled MWS in the test data set is {}.'.format(test.filter(test['author'] == 'MWS').count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model using the training data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions using the test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>label</th>\n",
       "      <th>prediction</th>\n",
       "      <th>predictedLabel</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MWS</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[0.422635106973, 0.402083885492, 0.175281007535]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MWS</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[0.422908912669, 0.336405583622, 0.240685503709]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HPL</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[0.353100537669, 0.300331156335, 0.346568305996]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EAP</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[0.4494165135, 0.34636784381, 0.20421564269]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MWS</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>HPL</td>\n",
       "      <td>[0.309808024323, 0.331300813582, 0.358891162094]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  author  label  prediction predictedLabel  \\\n",
       "0  MWS    1      0           EAP             \n",
       "1  MWS    1      0           EAP             \n",
       "2  HPL    2      0           EAP             \n",
       "3  EAP    0      0           EAP             \n",
       "4  MWS    1      2           HPL             \n",
       "\n",
       "                                        probability  \n",
       "0  [0.422635106973, 0.402083885492, 0.175281007535]  \n",
       "1  [0.422908912669, 0.336405583622, 0.240685503709]  \n",
       "2  [0.353100537669, 0.300331156335, 0.346568305996]  \n",
       "3  [0.4494165135, 0.34636784381, 0.20421564269]      \n",
       "4  [0.309808024323, 0.331300813582, 0.358891162094]  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.select(\"author\", \"label\", \"prediction\", 'predictedLabel', \"probability\").toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model performance by calculating the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 46.93%.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol = \"label\", predictionCol=\"prediction\").setMetricName(\"accuracy\")\n",
    "print('Accuracy = {:0.2f}%.'.format(evaluator.evaluate(predictions)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate the prediction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted EAP correctly 1698 times.\n",
      "Failed to predict EAP 463 times.\n",
      "Predicted EAP incorrectly 1980 times.\n"
     ]
    }
   ],
   "source": [
    "EAPandEAP = predictions.filter(predictions['author']=='EAP').filter(predictions['predictedLabel']=='EAP').count()\n",
    "EAPnotEAP = predictions.filter(predictions['author']=='EAP').filter(predictions['predictedLabel']!='EAP').count()\n",
    "notEAPbutEAP = predictions.filter(predictions['author']!='EAP').filter(predictions['predictedLabel']=='EAP').count()\n",
    "print(\"Predicted EAP correctly {} times.\".format(EAPandEAP))\n",
    "print(\"Failed to predict EAP {} times.\".format(EAPnotEAP))\n",
    "print(\"Predicted EAP incorrectly {} times.\".format(notEAPbutEAP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted HPL correctly 496 times.\n",
      "Failed to predict HPL 1142 times.\n",
      "Predicted HPL incorrectly 507 times.\n"
     ]
    }
   ],
   "source": [
    "HPLandHPL = predictions.filter(predictions['author']=='HPL').filter(predictions['predictedLabel']=='HPL').count()\n",
    "HPLnotHPL = predictions.filter(predictions['author']=='HPL').filter(predictions['predictedLabel']!='HPL').count()\n",
    "notHPLbutHPL = predictions.filter(predictions['author']!='HPL').filter(predictions['predictedLabel']=='HPL').count()\n",
    "print(\"Predicted HPL correctly {} times.\".format(HPLandHPL))\n",
    "print(\"Failed to predict HPL {} times.\".format(HPLnotHPL))\n",
    "print(\"Predicted HPL incorrectly {} times.\".format(notHPLbutHPL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted MWS correctly 358 times.\n",
      "Failed to predict MWS 1281 times.\n",
      "Predicted MWS incorrectly 399 times.\n"
     ]
    }
   ],
   "source": [
    "MWSandMWS = predictions.filter(predictions['author']=='MWS').filter(predictions['predictedLabel']=='MWS').count()\n",
    "MWSnotMWS = predictions.filter(predictions['author']=='MWS').filter(predictions['predictedLabel']!='MWS').count()\n",
    "notMWSbutMWS = predictions.filter(predictions['author']!='MWS').filter(predictions['predictedLabel']=='MWS').count()\n",
    "print(\"Predicted MWS correctly {} times.\".format(MWSandMWS))\n",
    "print(\"Failed to predict MWS {} times.\".format(MWSnotMWS))\n",
    "print(\"Predicted MWS incorrectly {} times.\".format(notMWSbutMWS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Machine Learning using Watson Cognitive APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will employ Watson Natrual Language Understanding to create rich features and learn a new model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set up for Use of the Natural Language Understanding Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup configuration for the Watson Natural Language Understanding (NLU) service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import watson_developer_cloud\n",
    "from watson_developer_cloud import NaturalLanguageUnderstandingV1\n",
    "import watson_developer_cloud.natural_language_understanding.features.v1 as Features\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLU_USERNAME = '398bc575-8eaa-4485-bfad-5c8a599e7bc6'\n",
    "NLU_PASSWORD = 'dlipHXI0kPXl'\n",
    "natural_language_understanding = NaturalLanguageUnderstandingV1(\n",
    "  username=NLU_USERNAME,\n",
    "  password=NLU_PASSWORD,\n",
    "  version=\"2017-02-27\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Split up the dataframe - this will be important in order to limit the API call rate to the NLU Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data00,data01,data02,data03,data04,data05,data06,data07,data08,data09,data10,data11,data12,data13,data14,data15,data16,data17,data18,data19, \\\n",
    "data20,data21,data22,data23,data24,data25,data26,data27,data28,data29,data30,data31,data32,data33,data34,data35,data36,data37,data38,data39, \\\n",
    "data40,data41,data42,data43,data44,data45,data46,data47,data48,data49,data50,data51,data52,data53,data54,data55,data56,data57,data58,data59, \\\n",
    "data60,data61,data62,data63,data64,data65,data66,data67,data68,data69,data70,data71,data72,data73,data74,data75,data76,data77,data78,data79, \\\n",
    "data80,data81,data82,data83,data84,data85,data86,data87,data88,data89,data90,data91,data92,data93,data94,data95,data96,data97,data98,data99 \\\n",
    " = data.randomSplit([1.0]*100, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show example of employing NLU API on single row of the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"I walked rapidly softly and close to the ruined houses\"\n",
      "\n",
      "Anger = 0.417696\n",
      "Joy = 0.021155\n",
      "Sadness = 0.482235\n",
      "Fear = 0.298235\n",
      "Disgust = 0.134835\n",
      "Sentiment = -0.314108\n",
      "\n",
      "{\n",
      "  \"usage\": {\n",
      "    \"text_characters\": 56, \n",
      "    \"features\": 2, \n",
      "    \"text_units\": 1\n",
      "  }, \n",
      "  \"emotion\": {\n",
      "    \"document\": {\n",
      "      \"emotion\": {\n",
      "        \"anger\": 0.417696, \n",
      "        \"joy\": 0.021155, \n",
      "        \"sadness\": 0.482235, \n",
      "        \"fear\": 0.298235, \n",
      "        \"disgust\": 0.134835\n",
      "      }\n",
      "    }\n",
      "  }, \n",
      "  \"language\": \"en\", \n",
      "  \"sentiment\": {\n",
      "    \"document\": {\n",
      "      \"score\": -0.314108, \n",
      "      \"label\": \"negative\"\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "dataNLUtest = data00.select(data00[\"text\"]).toJSON().collect()[0][8:-1]\n",
    "print(dataNLUtest)\n",
    "import json\n",
    "features=[\n",
    "    Features.Emotion(),\n",
    "    Features.Sentiment()\n",
    "  ]\n",
    "nlu = natural_language_understanding.analyze(text=dataNLUtest, features=features, language='en', clean='true')\n",
    "anger = nlu['emotion']['document']['emotion']['anger']\n",
    "joy = nlu['emotion']['document']['emotion']['joy']\n",
    "sadness = nlu['emotion']['document']['emotion']['sadness']\n",
    "fear = nlu['emotion']['document']['emotion']['fear']\n",
    "disgust = nlu['emotion']['document']['emotion']['disgust']\n",
    "sentiment = nlu['sentiment']['document']['score']\n",
    "\n",
    "print(\"\")\n",
    "print(\"Anger = {}\".format(anger))\n",
    "print(\"Joy = {}\".format(joy))\n",
    "print(\"Sadness = {}\".format(sadness))\n",
    "print(\"Fear = {}\".format(fear))\n",
    "print(\"Disgust = {}\".format(disgust))\n",
    "print(\"Sentiment = {}\".format(sentiment))\n",
    "print(\"\")\n",
    "print(json.dumps(nlu, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Enhanced Features using the NLU Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define UDF to create NLU derived features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "#import json\n",
    "udfNLU = (udf(lambda text: json.dumps(NaturalLanguageUnderstandingV1(\n",
    "    username=NLU_USERNAME, password=NLU_PASSWORD, version=\"2017-02-27\")\n",
    "    .analyze(text=text, features=features, language='en', clean='true'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke UDF to create new column with NLU output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data00NLU = data00.withColumn('nlu', udfNLU(data00['text']))\n",
    "data01NLU = data01.withColumn('nlu', udfNLU(data01['text']))\n",
    "data02NLU = data02.withColumn('nlu', udfNLU(data02['text']))\n",
    "data03NLU = data03.withColumn('nlu', udfNLU(data03['text']))\n",
    "data04NLU = data04.withColumn('nlu', udfNLU(data04['text']))\n",
    "data05NLU = data05.withColumn('nlu', udfNLU(data05['text']))\n",
    "data06NLU = data06.withColumn('nlu', udfNLU(data06['text']))\n",
    "data07NLU = data07.withColumn('nlu', udfNLU(data07['text']))\n",
    "data08NLU = data08.withColumn('nlu', udfNLU(data08['text']))\n",
    "data09NLU = data09.withColumn('nlu', udfNLU(data09['text']))\n",
    "data10NLU = data10.withColumn('nlu', udfNLU(data10['text']))\n",
    "data11NLU = data11.withColumn('nlu', udfNLU(data11['text']))\n",
    "data12NLU = data12.withColumn('nlu', udfNLU(data12['text']))\n",
    "data13NLU = data13.withColumn('nlu', udfNLU(data13['text']))\n",
    "data14NLU = data14.withColumn('nlu', udfNLU(data14['text']))\n",
    "data15NLU = data15.withColumn('nlu', udfNLU(data15['text']))\n",
    "data16NLU = data16.withColumn('nlu', udfNLU(data16['text']))\n",
    "data17NLU = data17.withColumn('nlu', udfNLU(data17['text']))\n",
    "data18NLU = data18.withColumn('nlu', udfNLU(data18['text']))\n",
    "data19NLU = data19.withColumn('nlu', udfNLU(data19['text']))\n",
    "data20NLU = data20.withColumn('nlu', udfNLU(data20['text']))\n",
    "data21NLU = data21.withColumn('nlu', udfNLU(data21['text']))\n",
    "data22NLU = data22.withColumn('nlu', udfNLU(data22['text']))\n",
    "data23NLU = data23.withColumn('nlu', udfNLU(data23['text']))\n",
    "data24NLU = data24.withColumn('nlu', udfNLU(data24['text']))\n",
    "data25NLU = data25.withColumn('nlu', udfNLU(data25['text']))\n",
    "data26NLU = data26.withColumn('nlu', udfNLU(data26['text']))\n",
    "data27NLU = data27.withColumn('nlu', udfNLU(data27['text']))\n",
    "data28NLU = data28.withColumn('nlu', udfNLU(data28['text']))\n",
    "data29NLU = data29.withColumn('nlu', udfNLU(data29['text']))\n",
    "data30NLU = data30.withColumn('nlu', udfNLU(data30['text']))\n",
    "data31NLU = data31.withColumn('nlu', udfNLU(data31['text']))\n",
    "data32NLU = data32.withColumn('nlu', udfNLU(data32['text']))\n",
    "data33NLU = data33.withColumn('nlu', udfNLU(data33['text']))\n",
    "data34NLU = data34.withColumn('nlu', udfNLU(data34['text']))\n",
    "data35NLU = data35.withColumn('nlu', udfNLU(data35['text']))\n",
    "data36NLU = data36.withColumn('nlu', udfNLU(data36['text']))\n",
    "data37NLU = data37.withColumn('nlu', udfNLU(data37['text']))\n",
    "data38NLU = data38.withColumn('nlu', udfNLU(data38['text']))\n",
    "data39NLU = data39.withColumn('nlu', udfNLU(data39['text']))\n",
    "data40NLU = data40.withColumn('nlu', udfNLU(data40['text']))\n",
    "data41NLU = data41.withColumn('nlu', udfNLU(data41['text']))\n",
    "data42NLU = data42.withColumn('nlu', udfNLU(data42['text']))\n",
    "data43NLU = data43.withColumn('nlu', udfNLU(data43['text']))\n",
    "data44NLU = data44.withColumn('nlu', udfNLU(data44['text']))\n",
    "data45NLU = data45.withColumn('nlu', udfNLU(data45['text']))\n",
    "data46NLU = data46.withColumn('nlu', udfNLU(data46['text']))\n",
    "data47NLU = data47.withColumn('nlu', udfNLU(data47['text']))\n",
    "data48NLU = data48.withColumn('nlu', udfNLU(data48['text']))\n",
    "data49NLU = data49.withColumn('nlu', udfNLU(data49['text']))\n",
    "data50NLU = data50.withColumn('nlu', udfNLU(data50['text']))\n",
    "data51NLU = data51.withColumn('nlu', udfNLU(data51['text']))\n",
    "data52NLU = data52.withColumn('nlu', udfNLU(data52['text']))\n",
    "data53NLU = data53.withColumn('nlu', udfNLU(data53['text']))\n",
    "data54NLU = data54.withColumn('nlu', udfNLU(data54['text']))\n",
    "data55NLU = data55.withColumn('nlu', udfNLU(data55['text']))\n",
    "data56NLU = data56.withColumn('nlu', udfNLU(data56['text']))\n",
    "data57NLU = data57.withColumn('nlu', udfNLU(data57['text']))\n",
    "data58NLU = data58.withColumn('nlu', udfNLU(data58['text']))\n",
    "data59NLU = data59.withColumn('nlu', udfNLU(data59['text']))\n",
    "data60NLU = data60.withColumn('nlu', udfNLU(data60['text']))\n",
    "data61NLU = data61.withColumn('nlu', udfNLU(data61['text']))\n",
    "data62NLU = data62.withColumn('nlu', udfNLU(data62['text']))\n",
    "data63NLU = data63.withColumn('nlu', udfNLU(data63['text']))\n",
    "data64NLU = data64.withColumn('nlu', udfNLU(data64['text']))\n",
    "data65NLU = data65.withColumn('nlu', udfNLU(data65['text']))\n",
    "data66NLU = data66.withColumn('nlu', udfNLU(data66['text']))\n",
    "data67NLU = data67.withColumn('nlu', udfNLU(data67['text']))\n",
    "data68NLU = data68.withColumn('nlu', udfNLU(data68['text']))\n",
    "data69NLU = data69.withColumn('nlu', udfNLU(data69['text']))\n",
    "data70NLU = data70.withColumn('nlu', udfNLU(data70['text']))\n",
    "data71NLU = data71.withColumn('nlu', udfNLU(data71['text']))\n",
    "data72NLU = data72.withColumn('nlu', udfNLU(data72['text']))\n",
    "data73NLU = data73.withColumn('nlu', udfNLU(data73['text']))\n",
    "data74NLU = data74.withColumn('nlu', udfNLU(data74['text']))\n",
    "data75NLU = data75.withColumn('nlu', udfNLU(data75['text']))\n",
    "data76NLU = data76.withColumn('nlu', udfNLU(data76['text']))\n",
    "data77NLU = data77.withColumn('nlu', udfNLU(data77['text']))\n",
    "data78NLU = data78.withColumn('nlu', udfNLU(data78['text']))\n",
    "data79NLU = data79.withColumn('nlu', udfNLU(data79['text']))\n",
    "data80NLU = data80.withColumn('nlu', udfNLU(data80['text']))\n",
    "data81NLU = data81.withColumn('nlu', udfNLU(data81['text']))\n",
    "data82NLU = data82.withColumn('nlu', udfNLU(data82['text']))\n",
    "data83NLU = data83.withColumn('nlu', udfNLU(data83['text']))\n",
    "data84NLU = data84.withColumn('nlu', udfNLU(data84['text']))\n",
    "data85NLU = data85.withColumn('nlu', udfNLU(data85['text']))\n",
    "data86NLU = data86.withColumn('nlu', udfNLU(data86['text']))\n",
    "data87NLU = data87.withColumn('nlu', udfNLU(data87['text']))\n",
    "data88NLU = data88.withColumn('nlu', udfNLU(data88['text']))\n",
    "data89NLU = data89.withColumn('nlu', udfNLU(data89['text']))\n",
    "data90NLU = data90.withColumn('nlu', udfNLU(data90['text']))\n",
    "data91NLU = data91.withColumn('nlu', udfNLU(data91['text']))\n",
    "data92NLU = data92.withColumn('nlu', udfNLU(data92['text']))\n",
    "data93NLU = data93.withColumn('nlu', udfNLU(data93['text']))\n",
    "data94NLU = data94.withColumn('nlu', udfNLU(data94['text']))\n",
    "data95NLU = data95.withColumn('nlu', udfNLU(data95['text']))\n",
    "data96NLU = data96.withColumn('nlu', udfNLU(data96['text']))\n",
    "data97NLU = data97.withColumn('nlu', udfNLU(data97['text']))\n",
    "data98NLU = data98.withColumn('nlu', udfNLU(data98['text']))\n",
    "data99NLU = data99.withColumn('nlu', udfNLU(data99['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The combined dataset contains 18047 rows.\n"
     ]
    }
   ],
   "source": [
    "dataNLU= (data00NLU.union(data01NLU).union(data02NLU).union(data03NLU).union(data04NLU).union(data05NLU).union(data06NLU).union(data07NLU).union(data08NLU).union(data09NLU)\n",
    "            .union(data10NLU).union(data11NLU).union(data12NLU).union(data13NLU).union(data14NLU).union(data15NLU).union(data16NLU).union(data17NLU).union(data18NLU).union(data19NLU)\n",
    "            .union(data20NLU).union(data21NLU).union(data22NLU).union(data23NLU).union(data24NLU).union(data25NLU).union(data26NLU).union(data27NLU).union(data28NLU).union(data29NLU)\n",
    "            .union(data30NLU).union(data31NLU).union(data32NLU).union(data33NLU).union(data34NLU).union(data35NLU).union(data36NLU).union(data37NLU).union(data38NLU).union(data39NLU)\n",
    "            .union(data40NLU).union(data41NLU).union(data42NLU).union(data43NLU).union(data44NLU).union(data45NLU).union(data46NLU).union(data47NLU).union(data48NLU).union(data49NLU)\n",
    "            .union(data50NLU).union(data51NLU).union(data52NLU).union(data53NLU).union(data54NLU).union(data55NLU).union(data56NLU).union(data57NLU).union(data58NLU).union(data59NLU)\n",
    "            .union(data60NLU).union(data61NLU).union(data62NLU).union(data63NLU).union(data64NLU).union(data65NLU).union(data66NLU).union(data67NLU).union(data68NLU).union(data69NLU)\n",
    "            .union(data70NLU).union(data71NLU).union(data72NLU).union(data73NLU).union(data74NLU).union(data75NLU).union(data76NLU).union(data77NLU).union(data78NLU).union(data79NLU)\n",
    "            .union(data80NLU).union(data81NLU).union(data82NLU).union(data83NLU).union(data84NLU).union(data85NLU).union(data86NLU).union(data87NLU).union(data88NLU).union(data89NLU)\n",
    "            .union(data90NLU).union(data91NLU).union(data92NLU).union(data93NLU).union(data94NLU).union(data95NLU).union(data96NLU).union(data97NLU).union(data98NLU).union(data99NLU)\n",
    "            .cache())\n",
    "\n",
    "print(\"The combined dataset contains {} rows.\".format(dataNLU.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define UDFs to identify bad rows retuned by NLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "udfAngerTest = udf(lambda nlu: json.loads(nlu))\n",
    "udfJoyTest = udf(lambda nlu: json.loads(nlu))\n",
    "udfSadnessTest = udf(lambda nlu: json.loads(nlu))\n",
    "udfFearTest = udf(lambda nlu: json.loads(nlu))\n",
    "udfDisgustTest = udf(lambda nlu: json.loads(nlu))\n",
    "udfSentimentTest = udf(lambda nlu: json.loads(nlu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataNLUtest = (dataNLU.withColumn('AngerTest', udfAngerTest(dataNLU['nlu']))\n",
    "        .withColumn('JoyTest', udfJoyTest(dataNLU['nlu']))\n",
    "        .withColumn('SadnessTest', udfSadnessTest(dataNLU['nlu']))\n",
    "        .withColumn('FearTest', udfFearTest(dataNLU['nlu']))\n",
    "        .withColumn('DisgustTest', udfDisgustTest(dataNLU['nlu']))\n",
    "        .withColumn('SentimentTest', udfSentimentTest(dataNLU['nlu'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of bad rows found = 0\n",
      "Number of bad rows found = 0\n",
      "Number of bad rows found = 0\n",
      "Number of bad rows found = 0\n",
      "Number of bad rows found = 0\n",
      "Number of bad rows found = 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of bad rows found = {}\".format(dataNLUtest.filter(~ col('AngerTest').like('%anger%')).count()))\n",
    "print(\"Number of bad rows found = {}\".format(dataNLUtest.filter(~ col('JoyTest').like('%joy%')).count()))\n",
    "print(\"Number of bad rows found = {}\".format(dataNLUtest.filter(~ col('SadnessTest').like('%sadness%')).count()))\n",
    "print(\"Number of bad rows found = {}\".format(dataNLUtest.filter(~ col('FearTest').like('%fear%')).count()))\n",
    "print(\"Number of bad rows found = {}\".format(dataNLUtest.filter(~ col('DisgustTest').like('%disgust%')).count()))\n",
    "print(\"Number of bad rows found = {}\".format(dataNLUtest.filter(~ col('SentimentTest').like('%sentiment%')).count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove bad rows returned from NLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataNLU = (dataNLUtest.filter(col('AngerTest').like('%anger%'))\n",
    "            .filter(col('JoyTest').like('%joy%'))\n",
    "            .filter(col('SadnessTest').like('%sadness%'))\n",
    "            .filter(col('FearTest').like('%fear%'))\n",
    "            .filter(col('DisgustTest').like('%disgust%'))\n",
    "            .filter(col('SentimentTest').like('%sentiment%')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of bad rows removed = 0\n",
      "Number of bad rows removed = 0\n",
      "Number of bad rows removed = 0\n",
      "Number of bad rows removed = 0\n",
      "Number of bad rows removed = 0\n",
      "Number of bad rows removed = 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of bad rows removed = {}\".format(dataNLU.filter(~ col('AngerTest').like('%anger%')).count()))\n",
    "print(\"Number of bad rows removed = {}\".format(dataNLU.filter(~ col('JoyTest').like('%joy%')).count()))\n",
    "print(\"Number of bad rows removed = {}\".format(dataNLU.filter(~ col('SadnessTest').like('%sadness%')).count()))\n",
    "print(\"Number of bad rows removed = {}\".format(dataNLU.filter(~ col('FearTest').like('%fear%')).count()))\n",
    "print(\"Number of bad rows removed = {}\".format(dataNLU.filter(~ col('DisgustTest').like('%disgust%')).count()))\n",
    "print(\"Number of bad rows removed = {}\".format(dataNLU.filter(~ col('SentimentTest').like('%sentiment%')).count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop NLU test columns\n",
    "dataNLU = dataNLU.drop('AngerTest','JoyTest', 'SadnessTest', 'FearTest', 'DisgustTest', 'SentimentTest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define UDFs to extract NLU derived features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "udfAnger = udf(lambda nlu: json.loads(nlu)[\"emotion\"][\"document\"][\"emotion\"][\"anger\"], DoubleType())\n",
    "udfJoy = udf(lambda nlu: json.loads(nlu)[\"emotion\"][\"document\"][\"emotion\"][\"joy\"], DoubleType())\n",
    "udfSadness = udf(lambda nlu: json.loads(nlu)[\"emotion\"][\"document\"][\"emotion\"][\"sadness\"], DoubleType())\n",
    "udfFear = udf(lambda nlu: json.loads(nlu)[\"emotion\"][\"document\"][\"emotion\"][\"fear\"], DoubleType())\n",
    "udfDisgust = udf(lambda nlu: json.loads(nlu)[\"emotion\"][\"document\"][\"emotion\"][\"disgust\"], DoubleType())\n",
    "udfSentiment = udf(lambda nlu: json.loads(nlu)['sentiment']['document']['score'], DoubleType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke UDFs to create new columns for the enhanced emotion and sentiment features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataNLU = (dataNLU.withColumn('Anger', udfAnger(dataNLU['nlu']))\n",
    "        .withColumn('Joy', udfJoy(dataNLU['nlu']))\n",
    "        .withColumn('Sadness', udfSadness(dataNLU['nlu']))\n",
    "        .withColumn('Fear', udfFear(dataNLU['nlu']))\n",
    "        .withColumn('Disgust', udfDisgust(dataNLU['nlu']))\n",
    "        .withColumn('Sentiment', udfSentiment(dataNLU['nlu'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Anger</th>\n",
       "      <th>Joy</th>\n",
       "      <th>Sadness</th>\n",
       "      <th>Fear</th>\n",
       "      <th>Disgust</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I walked rapidly softly and close to the ruined houses</td>\n",
       "      <td>0.417696</td>\n",
       "      <td>0.021155</td>\n",
       "      <td>0.482235</td>\n",
       "      <td>0.298235</td>\n",
       "      <td>0.134835</td>\n",
       "      <td>-0.571100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The youth's febrile mind apparently was dwelling on strange things and the doctor shuddered now and then as he spoke of them</td>\n",
       "      <td>0.055444</td>\n",
       "      <td>0.116561</td>\n",
       "      <td>0.365396</td>\n",
       "      <td>0.351243</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>-0.746486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>But since the murderer has been discovered  The murderer discovered Good God how can that be who could attempt to pursue him</td>\n",
       "      <td>0.146588</td>\n",
       "      <td>0.268470</td>\n",
       "      <td>0.384652</td>\n",
       "      <td>0.045321</td>\n",
       "      <td>0.336253</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>So thick were the vapours that the way was hard and though Atal followed on at last he could scarce see the grey shape of Barzai on the dim slope above in the clouded moonlight</td>\n",
       "      <td>0.032738</td>\n",
       "      <td>0.150773</td>\n",
       "      <td>0.694929</td>\n",
       "      <td>0.203870</td>\n",
       "      <td>0.029030</td>\n",
       "      <td>-0.585316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>By authority of the king such districts were placed under ban and all persons forbidden under pain of death to intrude upon their dismal solitude</td>\n",
       "      <td>0.247888</td>\n",
       "      <td>0.026101</td>\n",
       "      <td>0.771124</td>\n",
       "      <td>0.095373</td>\n",
       "      <td>0.122879</td>\n",
       "      <td>-0.836231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Thus while Perdita was entertaining her guests and anxiously awaiting the arrival of her lord his ring was brought her and she was told that a poor woman had a note to deliver to her from its wearer</td>\n",
       "      <td>0.098105</td>\n",
       "      <td>0.104288</td>\n",
       "      <td>0.470236</td>\n",
       "      <td>0.089047</td>\n",
       "      <td>0.107151</td>\n",
       "      <td>-0.436361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>As the evening wore away he became more and more absorbed in reverie from which no sallies of mine could arouse him</td>\n",
       "      <td>0.110423</td>\n",
       "      <td>0.141169</td>\n",
       "      <td>0.527687</td>\n",
       "      <td>0.172764</td>\n",
       "      <td>0.117253</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>He stopped in his tracks then flailing his arms wildly in the air began to stagger backward</td>\n",
       "      <td>0.234947</td>\n",
       "      <td>0.170794</td>\n",
       "      <td>0.115705</td>\n",
       "      <td>0.310494</td>\n",
       "      <td>0.091302</td>\n",
       "      <td>-0.868435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>In feeling my way I had found many angles and thus deduced an idea of great irregularity so potent is the effect of total darkness upon one arousing from lethargy or sleep The angles were simply those of a few slight depressions or niches at odd intervals</td>\n",
       "      <td>0.021526</td>\n",
       "      <td>0.102365</td>\n",
       "      <td>0.736927</td>\n",
       "      <td>0.158123</td>\n",
       "      <td>0.025682</td>\n",
       "      <td>-0.866337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>He seemed insensible to the presence of any one else but if as a trial to awaken his sensibility my aunt brought me into the room he would instantly rush out with every symptom of fury and distraction</td>\n",
       "      <td>0.437653</td>\n",
       "      <td>0.155442</td>\n",
       "      <td>0.238697</td>\n",
       "      <td>0.203852</td>\n",
       "      <td>0.013623</td>\n",
       "      <td>-0.886573</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                              text  \\\n",
       "0  I walked rapidly softly and close to the ruined houses                                                                                                                                                                                                            \n",
       "1  The youth's febrile mind apparently was dwelling on strange things and the doctor shuddered now and then as he spoke of them                                                                                                                                      \n",
       "2  But since the murderer has been discovered  The murderer discovered Good God how can that be who could attempt to pursue him                                                                                                                                      \n",
       "3  So thick were the vapours that the way was hard and though Atal followed on at last he could scarce see the grey shape of Barzai on the dim slope above in the clouded moonlight                                                                                  \n",
       "4  By authority of the king such districts were placed under ban and all persons forbidden under pain of death to intrude upon their dismal solitude                                                                                                                 \n",
       "5  Thus while Perdita was entertaining her guests and anxiously awaiting the arrival of her lord his ring was brought her and she was told that a poor woman had a note to deliver to her from its wearer                                                            \n",
       "6  As the evening wore away he became more and more absorbed in reverie from which no sallies of mine could arouse him                                                                                                                                               \n",
       "7  He stopped in his tracks then flailing his arms wildly in the air began to stagger backward                                                                                                                                                                       \n",
       "8  In feeling my way I had found many angles and thus deduced an idea of great irregularity so potent is the effect of total darkness upon one arousing from lethargy or sleep The angles were simply those of a few slight depressions or niches at odd intervals   \n",
       "9  He seemed insensible to the presence of any one else but if as a trial to awaken his sensibility my aunt brought me into the room he would instantly rush out with every symptom of fury and distraction                                                          \n",
       "\n",
       "      Anger       Joy   Sadness      Fear   Disgust  Sentiment  \n",
       "0  0.417696  0.021155  0.482235  0.298235  0.134835 -0.571100   \n",
       "1  0.055444  0.116561  0.365396  0.351243  0.088965 -0.746486   \n",
       "2  0.146588  0.268470  0.384652  0.045321  0.336253  0.000000   \n",
       "3  0.032738  0.150773  0.694929  0.203870  0.029030 -0.585316   \n",
       "4  0.247888  0.026101  0.771124  0.095373  0.122879 -0.836231   \n",
       "5  0.098105  0.104288  0.470236  0.089047  0.107151 -0.436361   \n",
       "6  0.110423  0.141169  0.527687  0.172764  0.117253  0.000000   \n",
       "7  0.234947  0.170794  0.115705  0.310494  0.091302 -0.868435   \n",
       "8  0.021526  0.102365  0.736927  0.158123  0.025682 -0.866337   \n",
       "9  0.437653  0.155442  0.238697  0.203852  0.013623 -0.886573   "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataNLU.select(dataNLU['text'], dataNLU['Anger'], dataNLU['Joy'], dataNLU['Sadness'], dataNLU['Fear'], dataNLU['Disgust'], dataNLU['Sentiment']).toPandas().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Retrain Model with NLU Features Added"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset into training and test data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainNLU, testNLU = dataNLU.randomSplit([70.0,30.0], seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bucketize the NLU features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Bucketizer\n",
    "AngerBucketSplits = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "AngerBucket = Bucketizer(splits=AngerBucketSplits, inputCol=\"Anger\", outputCol=\"AngerBucket\")\n",
    "JoyBucketSplits = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "JoyBucket = Bucketizer(splits=JoyBucketSplits, inputCol=\"Joy\", outputCol=\"JoyBucket\")\n",
    "SadnessBucketSplits = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "SadnessBucket = Bucketizer(splits=SadnessBucketSplits, inputCol=\"Sadness\", outputCol=\"SadnessBucket\")\n",
    "FearBucketSplits = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "FearBucket = Bucketizer(splits=FearBucketSplits, inputCol=\"Fear\", outputCol=\"FearBucket\")\n",
    "DisgustBucketSplits = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "DisgustBucket = Bucketizer(splits=DisgustBucketSplits, inputCol=\"Disgust\", outputCol=\"DisgustBucket\")\n",
    "SentimentBucketSplits = [-1.0, -0.9, -0.8, -0.7, -0.6, -0.5, -0.4, -0.3, -0.2, -0.1, 0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "SentimentBucket = Bucketizer(splits=SentimentBucketSplits, inputCol=\"Sentiment\", outputCol=\"SentimentBucket\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = (VectorAssembler(inputCols=[\"features\", \"AngerBucket\", \"JoyBucket\", \"SadnessBucket\", \"FearBucket\", \"DisgustBucket\",\"SentimentBucket\"], \n",
    "             outputCol=\"featuresNLU\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a revised machine learning pipeline utilizing the new bucketed NLU feaures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrNLU = LogisticRegression(labelCol = \"label\", featuresCol= \"featuresNLU\", maxIter=10, regParam=0.3, threshold=0.5)\n",
    "stagesNLU = ([removePunctuationTrans, tokenizer, remover, hashingTF, idf, AngerBucket, JoyBucket, SadnessBucket, FearBucket, DisgustBucket, SentimentBucket,\n",
    "            assembler, labelIndexer, lrNLU, labelConverter])\n",
    "pipelineNLU = Pipeline(stages = stagesNLU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the new model using the training data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelNLU = pipelineNLU.fit(trainNLU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate the Model Learned with NLU Features\n",
    "### Make updated predictions using the test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionsNLU = modelNLU.transform(testNLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>label</th>\n",
       "      <th>prediction</th>\n",
       "      <th>predictedLabel</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EAP</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[0.360096429762, 0.286115754736, 0.353787815502]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EAP</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[0.454725977862, 0.296646421719, 0.248627600419]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MWS</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>MWS</td>\n",
       "      <td>[0.326479103653, 0.365455079062, 0.308065817285]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MWS</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[0.472015346289, 0.278435628774, 0.249549024938]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MWS</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[0.406513188439, 0.263848027527, 0.329638784034]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  author  label  prediction predictedLabel  \\\n",
       "0  EAP    0      0           EAP             \n",
       "1  EAP    0      0           EAP             \n",
       "2  MWS    1      1           MWS             \n",
       "3  MWS    1      0           EAP             \n",
       "4  MWS    1      0           EAP             \n",
       "\n",
       "                                        probability  \n",
       "0  [0.360096429762, 0.286115754736, 0.353787815502]  \n",
       "1  [0.454725977862, 0.296646421719, 0.248627600419]  \n",
       "2  [0.326479103653, 0.365455079062, 0.308065817285]  \n",
       "3  [0.472015346289, 0.278435628774, 0.249549024938]  \n",
       "4  [0.406513188439, 0.263848027527, 0.329638784034]  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictionsNLU.select(\"author\", \"label\", \"prediction\", 'predictedLabel', \"probability\").toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the updated model performance by calculating the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with NLU = 49.44%.\n"
     ]
    }
   ],
   "source": [
    "evaluatorNLU = MulticlassClassificationEvaluator(labelCol = \"label\", predictionCol=\"prediction\").setMetricName(\"accuracy\")\n",
    "print('Accuracy with NLU = {:0.2f}%.'.format(evaluatorNLU.evaluate(predictionsNLU)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate Improved Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted EAP correctly 1611 times vs. 1698 previously.\n",
      "Failed to predict EAP 518 times vs. 463 previously.\n",
      "Predicted EAP incorrectly 1734 times vs. 1980 previously.\n"
     ]
    }
   ],
   "source": [
    "EAPandEAPnlu = predictionsNLU.filter(predictionsNLU['author']=='EAP').filter(predictionsNLU['predictedLabel']=='EAP').count()\n",
    "EAPnotEAPnlu = predictionsNLU.filter(predictionsNLU['author']=='EAP').filter(predictionsNLU['predictedLabel']!='EAP').count()\n",
    "notEAPbutEAPnlu = predictionsNLU.filter(predictionsNLU['author']!='EAP').filter(predictionsNLU['predictedLabel']=='EAP').count()\n",
    "print(\"Predicted EAP correctly {} times vs. {} previously.\".format(EAPandEAPnlu, EAPandEAP))\n",
    "print(\"Failed to predict EAP {} times vs. {} previously.\".format(EAPnotEAPnlu, EAPnotEAP))\n",
    "print(\"Predicted EAP incorrectly {} times vs. {} previously.\".format(notEAPbutEAPnlu, notEAPbutEAP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted HPL correctly 620 times vs. 496 previously.\n",
      "Failed to predict HPL 1012 times vs. 1142 previously.\n",
      "Predicted HPL incorrectly 590 times vs. 507 previously.\n"
     ]
    }
   ],
   "source": [
    "HPLandHPLnlu = predictionsNLU.filter(predictionsNLU['author']=='HPL').filter(predictionsNLU['predictedLabel']=='HPL').count()\n",
    "HPLnotHPLnlu = predictionsNLU.filter(predictionsNLU['author']=='HPL').filter(predictionsNLU['predictedLabel']!='HPL').count()\n",
    "notHPLbutHPLnlu = predictionsNLU.filter(predictionsNLU['author']!='HPL').filter(predictionsNLU['predictedLabel']=='HPL').count()\n",
    "print(\"Predicted HPL correctly {} times vs. {} previously.\".format(HPLandHPLnlu, HPLandHPL))\n",
    "print(\"Failed to predict HPL {} times vs. {} previously.\".format(HPLnotHPLnlu, HPLnotHPL))\n",
    "print(\"Predicted HPL incorrectly {} times vs. {} previously.\".format(notHPLbutHPLnlu, notHPLbutHPL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted MWS correctly 474 times vs. 358 previously.\n",
      "Failed to predict MWS 1236 times vs. 1281 previously.\n",
      "Predicted MWS incorrectly 442 times vs. 399 previously.\n"
     ]
    }
   ],
   "source": [
    "MWSandMWSnlu = predictionsNLU.filter(predictionsNLU['author']=='MWS').filter(predictionsNLU['predictedLabel']=='MWS').count()\n",
    "MWSnotMWSnlu = predictionsNLU.filter(predictionsNLU['author']=='MWS').filter(predictionsNLU['predictedLabel']!='MWS').count()\n",
    "notMWSbutMWSnlu = predictionsNLU.filter(predictionsNLU['author']!='MWS').filter(predictionsNLU['predictedLabel']=='MWS').count()\n",
    "print(\"Predicted MWS correctly {} times vs. {} previously.\".format(MWSandMWSnlu, MWSandMWS))\n",
    "print(\"Failed to predict MWS {} times vs. {} previously.\".format(MWSnotMWSnlu, MWSnotMWS))\n",
    "print(\"Predicted MWS incorrectly {} times vs. {} previously.\".format(notMWSbutMWSnlu, notMWSbutMWS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![IBM Logo](http://www-03.ibm.com/press/img/Large_IBM_Logo_TN.jpg)\n",
    "\n",
    "Rich Tarro  \n",
    "Solutions Architect, IBM Corporation  \n",
    "rtarro@us.ibm.com\n",
    "\n",
    "December 20, 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 with Spark 2.1",
   "language": "python",
   "name": "python2-spark21"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
