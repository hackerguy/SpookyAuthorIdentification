{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://storage.googleapis.com/kaggle-media/competitions/spooky-books/dmitrij-paskevic-44124.jpg\" style=\"width:200px; float: left; padding-right: 10px\"/>\n",
    "<h2 style=\"font-face: verdana; font-size: 32px;\">Spooky Author Identification</h2>\n",
    "<h3 style=\"font-face: verdana; font-size: 16px;\">Derive rich features for Machine Learning with the Watson Cognitive APIs</h3>\n",
    "<br><br>\n",
    "<a href=\"https://www.kaggle.com/c/spooky-author-identification/\">Spooky Author Identification Kaggle Competition</a>\n",
    "\n",
    "<h3 style=\"font-face: verdana; font-size: 16px;\">The objective of this machine learning model is to predict the author of excerpts from horror stories by Edgar Allan Poe, Mary Shelley, and HP Lovecraft.</h3>\n",
    "\n",
    "The dataset contains text from works of fiction written by these spooky authors. The goal is to accurately identify the author of the sentences.\n",
    "\n",
    "Data fields in the dataset:\n",
    "\n",
    "    id - a unique identifier for each sentence\n",
    "    text - some text written by one of the authors\n",
    "    author - the author of the sentence (EAP: Edgar Allan Poe, HPL: HP Lovecraft; MWS: Mary Wollstonecraft Shelley)\n",
    "\n",
    "<h3 style=\"font-face: verdana; font-size: 16px;\">Approach</h3>\n",
    "\n",
    "We will approach this challenge by first using a traditional multiclassification machine learning approach. We will then explore using IBM Watson Natural Language Understanding to derive additional enhanced features on which to learn a machine learning model.\n",
    "\n",
    "<h3 style=\"font-face: verdana; font-size: 16px;\">IBM Watson Natural Language Understanding</h3>\n",
    "\n",
    "IBM Watsonâ„¢ Natural Language Understanding (NLU) can analyze semantic features of text input, including categories, concepts, emotion, entities, keywords, metadata, relations, semantic roles, and sentiment. In this example, we will utilize the emotion and sentiment features of NLU to create enhanced machine learning features.\n",
    "\n",
    "\n",
    "<h4 style=\"font-face: verdana; font-size: 16px;\">Emotion</h4>\n",
    "\n",
    "The emotion feature of NLU allows you to analyze emotion conveyed by specific target phrases or by the document as a whole. You can also enable emotion analysis for entities and keywords that are automatically detected by the service. In this example, we will simply analyze the spooky excerpt as a whole. The emotions we will derive features for are \n",
    "\n",
    "- Anger\n",
    "- Joy\n",
    "- Sadness\n",
    "- Fear\n",
    "- Disgust\n",
    "\n",
    "Emotion scores range from 0 to 1 for sadness, joy, fear, disgust, and anger. A 0 means the text doesn't convey the emotion, and a 1 means the text definitely carries the emotion.\n",
    "\n",
    "<h4 style=\"font-face: verdana; font-size: 16px;\">Sentiment</h4>\n",
    "\n",
    "The sentiment feature of NLU allows you to analyze the sentiment toward specific target phrases and the sentiment of the document as a whole. You can also get sentiment information for detected entities and keywords by enabling the sentiment option for those features. In this example, we will simply analyze the spooky excerpt as a whole.\n",
    "\n",
    "The sentiment score ranges from -1 (negative sentiment) to 1 (positive sentiment).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and unzip the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if os.path.isfile('train.zip'):\n",
    "    os.remove(\"train.zip\")\n",
    "if os.path.isfile('train.csv'):\n",
    "    os.remove(\"train.csv\")\n",
    "import wget\n",
    "url = 'https://github.com/hackerguy/SpookyAuthorIdentification/blob/master/train.zip?raw=true'\n",
    "wget.download(url)\n",
    "import zipfile\n",
    "zip = zipfile.ZipFile('train.zip', 'r')\n",
    "zip.extractall()\n",
    "zip.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the data set as a Spark DataFrame\n",
    "### Infer schema and column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "data = (spark.read\n",
    "  .format('csv')\n",
    "  .option('header', 'true')\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .load('train.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of ascertaining the dimensions of my dungeon; as I might make its circuit, and return to the point whence I set out, without being aware of the fact; so perfectly uniform seemed the wall.</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling might be a mere mistake.</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from which, as he capered down the hill, cutting all manner of fantastic steps, he took snuff incessantly with an air of the greatest possible self satisfaction.</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor Terrace on the sixteen fertile counties spread beneath, speckled by happy cottages and wealthier towns, all looked as in former years, heart cheering and fair.</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Superintendent abandoned his attempts; but a perplexed look occasionally steals over his countenance as he sits thinking at his desk.</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  \\\n",
       "0  id26305   \n",
       "1  id17569   \n",
       "2  id11008   \n",
       "3  id27763   \n",
       "4  id12958   \n",
       "\n",
       "                                                                                                                                                                                                                                      text  \\\n",
       "0  This process, however, afforded me no means of ascertaining the dimensions of my dungeon; as I might make its circuit, and return to the point whence I set out, without being aware of the fact; so perfectly uniform seemed the wall.   \n",
       "1  It never once occurred to me that the fumbling might be a mere mistake.                                                                                                                                                                   \n",
       "2  In his left hand was a gold snuff box, from which, as he capered down the hill, cutting all manner of fantastic steps, he took snuff incessantly with an air of the greatest possible self satisfaction.                                  \n",
       "3  How lovely is spring As we looked from Windsor Terrace on the sixteen fertile counties spread beneath, speckled by happy cottages and wealthier towns, all looked as in former years, heart cheering and fair.                            \n",
       "4  Finding nothing else, not even gold, the Superintendent abandoned his attempts; but a perplexed look occasionally steals over his countenance as he sits thinking at his desk.                                                            \n",
       "\n",
       "  author  \n",
       "0  EAP    \n",
       "1  HPL    \n",
       "2  EAP    \n",
       "3  MWS    \n",
       "4  HPL    "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "data.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the schema of the data including data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- author: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Overview - number of rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 19579 rows in the dataset.\n",
      "There are 3 columns in the dataset.\n"
     ]
    }
   ],
   "source": [
    "print(\"There are {} rows in the dataset.\".format(str(data.count())))\n",
    "print(\"There are {} columns in the dataset.\".format(str(len(data.columns))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optionally use a subset of the data for processing efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraction = 1.0\n",
    "data = data.sample(False, fraction, seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Split up the dataframe - this will be important later on to limit the API call rate to the NLU Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data00,data01,data02,data03,data04,data05,data06,data07,data08,data09,data10,data11,data12,data13,data14,data15,data16,data17,data18,data19, \\\n",
    "data20,data21,data22,data23,data24,data25,data26,data27,data28,data29,data30,data31,data32,data33,data34,data35,data36,data37,data38,data39, \\\n",
    "data40,data41,data42,data43,data44,data45,data46,data47,data48,data49,data50,data51,data52,data53,data54,data55,data56,data57,data58,data59, \\\n",
    "data60,data61,data62,data63,data64,data65,data66,data67,data68,data69,data70,data71,data72,data73,data74,data75,data76,data77,data78,data79, \\\n",
    "data80,data81,data82,data83,data84,data85,data86,data87,data88,data89,data90,data91,data92,data93,data94,data95,data96,data97,data98,data99 \\\n",
    " = data.randomSplit([1.0]*100, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The combined dataset contains 19543 rows.\n"
     ]
    }
   ],
   "source": [
    "dataUnion= (data00.union(data01).union(data02).union(data03).union(data04).union(data05).union(data06).union(data07).union(data08).union(data09)\n",
    "            .union(data10).union(data11).union(data12).union(data13).union(data14).union(data15).union(data16).union(data17).union(data18).union(data19)\n",
    "            .union(data20).union(data21).union(data22).union(data23).union(data24).union(data25).union(data26).union(data27).union(data28).union(data29)\n",
    "            .union(data30).union(data31).union(data32).union(data33).union(data34).union(data35).union(data36).union(data37).union(data38).union(data39)\n",
    "            .union(data40).union(data41).union(data42).union(data43).union(data44).union(data45).union(data46).union(data47).union(data48).union(data49)\n",
    "            .union(data50).union(data51).union(data52).union(data53).union(data54).union(data55).union(data56).union(data57).union(data58).union(data59)\n",
    "            .union(data60).union(data61).union(data62).union(data63).union(data64).union(data65).union(data66).union(data67).union(data68).union(data79)\n",
    "            .union(data70).union(data71).union(data72).union(data73).union(data74).union(data75).union(data76).union(data77).union(data78).union(data79)\n",
    "            .union(data80).union(data81).union(data82).union(data83).union(data84).union(data85).union(data86).union(data87).union(data88).union(data89)\n",
    "            .union(data90).union(data91).union(data92).union(data93).union(data94).union(data95).union(data96).union(data97).union(data98).union(data99))\n",
    "\n",
    "print(\"The combined dataset contains {} rows.\".format(dataUnion.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove punctuation from the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>textNoPunct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id00159</td>\n",
       "      <td>Where if anywhere had he been on those nights of daemoniac alienage?</td>\n",
       "      <td>HPL</td>\n",
       "      <td>Where if anywhere had he been on those nights of daemoniac alienage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id00553</td>\n",
       "      <td>Yet the effect is incongruous to the timid alone.</td>\n",
       "      <td>EAP</td>\n",
       "      <td>Yet the effect is incongruous to the timid alone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00666</td>\n",
       "      <td>\"\"\"You may easily believe</td>\n",
       "      <td>\"\" said he</td>\n",
       "      <td>You may easily believe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id00788</td>\n",
       "      <td>I am glad Woodville is not with me for perhaps he would grieve, and I desire to see smiles alone during the last scene of my life; when I last wrote to him I told him of my ill health but not of its mortal tendency, lest he should conceive it to be his duty to come to me for I fear lest the tears of friendship should destroy the blessed calm of my mind.</td>\n",
       "      <td>MWS</td>\n",
       "      <td>I am glad Woodville is not with me for perhaps he would grieve and I desire to see smiles alone during the last scene of my life when I last wrote to him I told him of my ill health but not of its mortal tendency lest he should conceive it to be his duty to come to me for I fear lest the tears of friendship should destroy the blessed calm of my mind</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id00830</td>\n",
       "      <td>You know, the curled up paper tacked to that frightful canvas in the cellar; the thing I thought was a photograph of some scene he meant to use as a background for that monster.</td>\n",
       "      <td>HPL</td>\n",
       "      <td>You know the curled up paper tacked to that frightful canvas in the cellar the thing I thought was a photograph of some scene he meant to use as a background for that monster</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  \\\n",
       "0  id00159   \n",
       "1  id00553   \n",
       "2  id00666   \n",
       "3  id00788   \n",
       "4  id00830   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                  text  \\\n",
       "0  Where if anywhere had he been on those nights of daemoniac alienage?                                                                                                                                                                                                                                                                                                  \n",
       "1  Yet the effect is incongruous to the timid alone.                                                                                                                                                                                                                                                                                                                     \n",
       "2  \"\"\"You may easily believe                                                                                                                                                                                                                                                                                                                                             \n",
       "3  I am glad Woodville is not with me for perhaps he would grieve, and I desire to see smiles alone during the last scene of my life; when I last wrote to him I told him of my ill health but not of its mortal tendency, lest he should conceive it to be his duty to come to me for I fear lest the tears of friendship should destroy the blessed calm of my mind.   \n",
       "4  You know, the curled up paper tacked to that frightful canvas in the cellar; the thing I thought was a photograph of some scene he meant to use as a background for that monster.                                                                                                                                                                                     \n",
       "\n",
       "       author  \\\n",
       "0  HPL          \n",
       "1  EAP          \n",
       "2  \"\" said he   \n",
       "3  MWS          \n",
       "4  HPL          \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                       textNoPunct  \n",
       "0  Where if anywhere had he been on those nights of daemoniac alienage                                                                                                                                                                                                                                                                                              \n",
       "1  Yet the effect is incongruous to the timid alone                                                                                                                                                                                                                                                                                                                 \n",
       "2  You may easily believe                                                                                                                                                                                                                                                                                                                                           \n",
       "3  I am glad Woodville is not with me for perhaps he would grieve and I desire to see smiles alone during the last scene of my life when I last wrote to him I told him of my ill health but not of its mortal tendency lest he should conceive it to be his duty to come to me for I fear lest the tears of friendship should destroy the blessed calm of my mind  \n",
       "4  You know the curled up paper tacked to that frightful canvas in the cellar the thing I thought was a photograph of some scene he meant to use as a background for that monster                                                                                                                                                                                   "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import SQLTransformer\n",
    "removePunctuationTrans = SQLTransformer(\n",
    "    statement=\"\"\"SELECT *, TRANSLATE(text,',.;?\\''\"+','') AS textNoPunct FROM __THIS__\"\"\")\n",
    "dataUnion = removePunctuationTrans.transform(dataUnion)\n",
    "dataUnion.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id00159</td>\n",
       "      <td>HPL</td>\n",
       "      <td>Where if anywhere had he been on those nights of daemoniac alienage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id00553</td>\n",
       "      <td>EAP</td>\n",
       "      <td>Yet the effect is incongruous to the timid alone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00666</td>\n",
       "      <td>\"\" said he</td>\n",
       "      <td>You may easily believe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id00788</td>\n",
       "      <td>MWS</td>\n",
       "      <td>I am glad Woodville is not with me for perhaps he would grieve and I desire to see smiles alone during the last scene of my life when I last wrote to him I told him of my ill health but not of its mortal tendency lest he should conceive it to be his duty to come to me for I fear lest the tears of friendship should destroy the blessed calm of my mind</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id00830</td>\n",
       "      <td>HPL</td>\n",
       "      <td>You know the curled up paper tacked to that frightful canvas in the cellar the thing I thought was a photograph of some scene he meant to use as a background for that monster</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id      author  \\\n",
       "0  id00159  HPL          \n",
       "1  id00553  EAP          \n",
       "2  id00666  \"\" said he   \n",
       "3  id00788  MWS          \n",
       "4  id00830  HPL          \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                              text  \n",
       "0  Where if anywhere had he been on those nights of daemoniac alienage                                                                                                                                                                                                                                                                                              \n",
       "1  Yet the effect is incongruous to the timid alone                                                                                                                                                                                                                                                                                                                 \n",
       "2  You may easily believe                                                                                                                                                                                                                                                                                                                                           \n",
       "3  I am glad Woodville is not with me for perhaps he would grieve and I desire to see smiles alone during the last scene of my life when I last wrote to him I told him of my ill health but not of its mortal tendency lest he should conceive it to be his duty to come to me for I fear lest the tears of friendship should destroy the blessed calm of my mind  \n",
       "4  You know the curled up paper tacked to that frightful canvas in the cellar the thing I thought was a photograph of some scene he meant to use as a background for that monster                                                                                                                                                                                   "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataUnion = dataUnion.drop('text').withColumnRenamed('textNoPunct', 'text')\n",
    "dataUnion.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove rows that do not have valid author fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id00159</td>\n",
       "      <td>HPL</td>\n",
       "      <td>Where if anywhere had he been on those nights of daemoniac alienage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id00553</td>\n",
       "      <td>EAP</td>\n",
       "      <td>Yet the effect is incongruous to the timid alone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00788</td>\n",
       "      <td>MWS</td>\n",
       "      <td>I am glad Woodville is not with me for perhaps he would grieve and I desire to see smiles alone during the last scene of my life when I last wrote to him I told him of my ill health but not of its mortal tendency lest he should conceive it to be his duty to come to me for I fear lest the tears of friendship should destroy the blessed calm of my mind</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id00830</td>\n",
       "      <td>HPL</td>\n",
       "      <td>You know the curled up paper tacked to that frightful canvas in the cellar the thing I thought was a photograph of some scene he meant to use as a background for that monster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id01380</td>\n",
       "      <td>MWS</td>\n",
       "      <td>Sometimes she observed the war of elements thinking that they also declared against her and listened to the pattering of the rain in gloomy despair</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id author  \\\n",
       "0  id00159  HPL     \n",
       "1  id00553  EAP     \n",
       "2  id00788  MWS     \n",
       "3  id00830  HPL     \n",
       "4  id01380  MWS     \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                              text  \n",
       "0  Where if anywhere had he been on those nights of daemoniac alienage                                                                                                                                                                                                                                                                                              \n",
       "1  Yet the effect is incongruous to the timid alone                                                                                                                                                                                                                                                                                                                 \n",
       "2  I am glad Woodville is not with me for perhaps he would grieve and I desire to see smiles alone during the last scene of my life when I last wrote to him I told him of my ill health but not of its mortal tendency lest he should conceive it to be his duty to come to me for I fear lest the tears of friendship should destroy the blessed calm of my mind  \n",
       "3  You know the curled up paper tacked to that frightful canvas in the cellar the thing I thought was a photograph of some scene he meant to use as a background for that monster                                                                                                                                                                                   \n",
       "4  Sometimes she observed the war of elements thinking that they also declared against her and listened to the pattering of the rain in gloomy despair                                                                                                                                                                                                              "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import SQLTransformer\n",
    "authorTrans = SQLTransformer(\n",
    "    statement=\"SELECT * FROM __THIS__ WHERE author='EAP' OR author='HPL' OR author='MWS'\")\n",
    "dataUnion = authorTrans.transform(dataUnion)\n",
    "dataUnion.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>words</th>\n",
       "      <th>#tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Where if anywhere had he been on those nights of daemoniac alienage</td>\n",
       "      <td>[where, if, anywhere, had, he, been, on, those, nights, of, daemoniac, alienage]</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yet the effect is incongruous to the timid alone</td>\n",
       "      <td>[yet, the, effect, is, incongruous, to, the, timid, alone]</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I am glad Woodville is not with me for perhaps he would grieve and I desire to see smiles alone during the last scene of my life when I last wrote to him I told him of my ill health but not of its mortal tendency lest he should conceive it to be his duty to come to me for I fear lest the tears of friendship should destroy the blessed calm of my mind</td>\n",
       "      <td>[i, am, glad, woodville, is, not, with, me, for, perhaps, he, would, grieve, and, i, desire, to, see, smiles, alone, during, the, last, scene, of, my, life, when, i, last, wrote, to, him, i, told, him, of, my, ill, health, but, not, of, its, mortal, tendency, lest, he, should, conceive, it, to, be, his, duty, to, come, to, me, for, i, fear, lest, the, tears, of, friendship, should, destroy, the, blessed, calm, of, my, mind]</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You know the curled up paper tacked to that frightful canvas in the cellar the thing I thought was a photograph of some scene he meant to use as a background for that monster</td>\n",
       "      <td>[you, know, the, curled, up, paper, tacked, to, that, frightful, canvas, in, the, cellar, the, thing, i, thought, was, a, photograph, of, some, scene, he, meant, to, use, as, a, background, for, that, monster]</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sometimes she observed the war of elements thinking that they also declared against her and listened to the pattering of the rain in gloomy despair</td>\n",
       "      <td>[sometimes, she, observed, the, war, of, elements, thinking, that, they, also, declared, against, her, and, listened, to, the, pattering, of, the, rain, in, gloomy, despair]</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                              text  \\\n",
       "0  Where if anywhere had he been on those nights of daemoniac alienage                                                                                                                                                                                                                                                                                               \n",
       "1  Yet the effect is incongruous to the timid alone                                                                                                                                                                                                                                                                                                                  \n",
       "2  I am glad Woodville is not with me for perhaps he would grieve and I desire to see smiles alone during the last scene of my life when I last wrote to him I told him of my ill health but not of its mortal tendency lest he should conceive it to be his duty to come to me for I fear lest the tears of friendship should destroy the blessed calm of my mind   \n",
       "3  You know the curled up paper tacked to that frightful canvas in the cellar the thing I thought was a photograph of some scene he meant to use as a background for that monster                                                                                                                                                                                    \n",
       "4  Sometimes she observed the war of elements thinking that they also declared against her and listened to the pattering of the rain in gloomy despair                                                                                                                                                                                                               \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                         words  \\\n",
       "0  [where, if, anywhere, had, he, been, on, those, nights, of, daemoniac, alienage]                                                                                                                                                                                                                                                                                                                                                              \n",
       "1  [yet, the, effect, is, incongruous, to, the, timid, alone]                                                                                                                                                                                                                                                                                                                                                                                    \n",
       "2  [i, am, glad, woodville, is, not, with, me, for, perhaps, he, would, grieve, and, i, desire, to, see, smiles, alone, during, the, last, scene, of, my, life, when, i, last, wrote, to, him, i, told, him, of, my, ill, health, but, not, of, its, mortal, tendency, lest, he, should, conceive, it, to, be, his, duty, to, come, to, me, for, i, fear, lest, the, tears, of, friendship, should, destroy, the, blessed, calm, of, my, mind]   \n",
       "3  [you, know, the, curled, up, paper, tacked, to, that, frightful, canvas, in, the, cellar, the, thing, i, thought, was, a, photograph, of, some, scene, he, meant, to, use, as, a, background, for, that, monster]                                                                                                                                                                                                                             \n",
       "4  [sometimes, she, observed, the, war, of, elements, thinking, that, they, also, declared, against, her, and, listened, to, the, pattering, of, the, rain, in, gloomy, despair]                                                                                                                                                                                                                                                                 \n",
       "\n",
       "   #tokens  \n",
       "0  12       \n",
       "1  9        \n",
       "2  75       \n",
       "3  34       \n",
       "4  25       "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "\n",
    "countTokens = udf(lambda words: len(words), IntegerType())\n",
    "\n",
    "tokenized = tokenizer.transform(dataUnion)\n",
    "(tokenized.select(\"text\", \"words\")\n",
    "    .withColumn(\"#tokens\", countTokens(col(\"words\"))).toPandas().head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>words</th>\n",
       "      <th>filtered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Where if anywhere had he been on those nights of daemoniac alienage</td>\n",
       "      <td>[where, if, anywhere, had, he, been, on, those, nights, of, daemoniac, alienage]</td>\n",
       "      <td>[anywhere, nights, daemoniac, alienage]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yet the effect is incongruous to the timid alone</td>\n",
       "      <td>[yet, the, effect, is, incongruous, to, the, timid, alone]</td>\n",
       "      <td>[yet, effect, incongruous, timid, alone]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I am glad Woodville is not with me for perhaps he would grieve and I desire to see smiles alone during the last scene of my life when I last wrote to him I told him of my ill health but not of its mortal tendency lest he should conceive it to be his duty to come to me for I fear lest the tears of friendship should destroy the blessed calm of my mind</td>\n",
       "      <td>[i, am, glad, woodville, is, not, with, me, for, perhaps, he, would, grieve, and, i, desire, to, see, smiles, alone, during, the, last, scene, of, my, life, when, i, last, wrote, to, him, i, told, him, of, my, ill, health, but, not, of, its, mortal, tendency, lest, he, should, conceive, it, to, be, his, duty, to, come, to, me, for, i, fear, lest, the, tears, of, friendship, should, destroy, the, blessed, calm, of, my, mind]</td>\n",
       "      <td>[glad, woodville, perhaps, would, grieve, desire, see, smiles, alone, last, scene, life, last, wrote, told, ill, health, mortal, tendency, lest, conceive, duty, come, fear, lest, tears, friendship, destroy, blessed, calm, mind]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You know the curled up paper tacked to that frightful canvas in the cellar the thing I thought was a photograph of some scene he meant to use as a background for that monster</td>\n",
       "      <td>[you, know, the, curled, up, paper, tacked, to, that, frightful, canvas, in, the, cellar, the, thing, i, thought, was, a, photograph, of, some, scene, he, meant, to, use, as, a, background, for, that, monster]</td>\n",
       "      <td>[know, curled, paper, tacked, frightful, canvas, cellar, thing, thought, photograph, scene, meant, use, background, monster]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sometimes she observed the war of elements thinking that they also declared against her and listened to the pattering of the rain in gloomy despair</td>\n",
       "      <td>[sometimes, she, observed, the, war, of, elements, thinking, that, they, also, declared, against, her, and, listened, to, the, pattering, of, the, rain, in, gloomy, despair]</td>\n",
       "      <td>[sometimes, observed, war, elements, thinking, also, declared, listened, pattering, rain, gloomy, despair]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                              text  \\\n",
       "0  Where if anywhere had he been on those nights of daemoniac alienage                                                                                                                                                                                                                                                                                               \n",
       "1  Yet the effect is incongruous to the timid alone                                                                                                                                                                                                                                                                                                                  \n",
       "2  I am glad Woodville is not with me for perhaps he would grieve and I desire to see smiles alone during the last scene of my life when I last wrote to him I told him of my ill health but not of its mortal tendency lest he should conceive it to be his duty to come to me for I fear lest the tears of friendship should destroy the blessed calm of my mind   \n",
       "3  You know the curled up paper tacked to that frightful canvas in the cellar the thing I thought was a photograph of some scene he meant to use as a background for that monster                                                                                                                                                                                    \n",
       "4  Sometimes she observed the war of elements thinking that they also declared against her and listened to the pattering of the rain in gloomy despair                                                                                                                                                                                                               \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                         words  \\\n",
       "0  [where, if, anywhere, had, he, been, on, those, nights, of, daemoniac, alienage]                                                                                                                                                                                                                                                                                                                                                              \n",
       "1  [yet, the, effect, is, incongruous, to, the, timid, alone]                                                                                                                                                                                                                                                                                                                                                                                    \n",
       "2  [i, am, glad, woodville, is, not, with, me, for, perhaps, he, would, grieve, and, i, desire, to, see, smiles, alone, during, the, last, scene, of, my, life, when, i, last, wrote, to, him, i, told, him, of, my, ill, health, but, not, of, its, mortal, tendency, lest, he, should, conceive, it, to, be, his, duty, to, come, to, me, for, i, fear, lest, the, tears, of, friendship, should, destroy, the, blessed, calm, of, my, mind]   \n",
       "3  [you, know, the, curled, up, paper, tacked, to, that, frightful, canvas, in, the, cellar, the, thing, i, thought, was, a, photograph, of, some, scene, he, meant, to, use, as, a, background, for, that, monster]                                                                                                                                                                                                                             \n",
       "4  [sometimes, she, observed, the, war, of, elements, thinking, that, they, also, declared, against, her, and, listened, to, the, pattering, of, the, rain, in, gloomy, despair]                                                                                                                                                                                                                                                                 \n",
       "\n",
       "                                                                                                                                                                                                                              filtered  \n",
       "0  [anywhere, nights, daemoniac, alienage]                                                                                                                                                                                              \n",
       "1  [yet, effect, incongruous, timid, alone]                                                                                                                                                                                             \n",
       "2  [glad, woodville, perhaps, would, grieve, desire, see, smiles, alone, last, scene, life, last, wrote, told, ill, health, mortal, tendency, lest, conceive, duty, come, fear, lest, tears, friendship, destroy, blessed, calm, mind]  \n",
       "3  [know, curled, paper, tacked, frightful, canvas, cellar, thing, thought, photograph, scene, meant, use, background, monster]                                                                                                         \n",
       "4  [sometimes, observed, war, elements, thinking, also, declared, listened, pattering, rain, gloomy, despair]                                                                                                                           "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").setCaseSensitive(False)\n",
    "removed = remover.transform(tokenized)\n",
    "removed.select(\"text\", \"words\", \"filtered\" ).toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show list of common words removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\n",
      "me\n",
      "my\n",
      "myself\n",
      "we\n",
      "our\n",
      "ours\n",
      "ourselves\n",
      "you\n",
      "your\n",
      "yours\n",
      "yourself\n",
      "yourselves\n",
      "he\n",
      "him\n",
      "his\n",
      "himself\n",
      "she\n",
      "her\n",
      "hers\n",
      "herself\n",
      "it\n",
      "its\n",
      "itself\n",
      "they\n",
      "them\n",
      "their\n",
      "theirs\n",
      "themselves\n",
      "what\n",
      "which\n",
      "who\n",
      "whom\n",
      "this\n",
      "that\n",
      "these\n",
      "those\n",
      "am\n",
      "is\n",
      "are\n",
      "was\n",
      "were\n",
      "be\n",
      "been\n",
      "being\n",
      "have\n",
      "has\n",
      "had\n",
      "having\n",
      "do\n",
      "does\n",
      "did\n",
      "doing\n",
      "a\n",
      "an\n",
      "the\n",
      "and\n",
      "but\n",
      "if\n",
      "or\n",
      "because\n",
      "as\n",
      "until\n",
      "while\n",
      "of\n",
      "at\n",
      "by\n",
      "for\n",
      "with\n",
      "about\n",
      "against\n",
      "between\n",
      "into\n",
      "through\n",
      "during\n",
      "before\n",
      "after\n",
      "above\n",
      "below\n",
      "to\n",
      "from\n",
      "up\n",
      "down\n",
      "in\n",
      "out\n",
      "on\n",
      "off\n",
      "over\n",
      "under\n",
      "again\n",
      "further\n",
      "then\n",
      "once\n",
      "here\n",
      "there\n",
      "when\n",
      "where\n",
      "why\n",
      "how\n",
      "all\n",
      "any\n",
      "both\n",
      "each\n",
      "few\n",
      "more\n",
      "most\n",
      "other\n",
      "some\n",
      "such\n",
      "no\n",
      "nor\n",
      "not\n",
      "only\n",
      "own\n",
      "same\n",
      "so\n",
      "than\n",
      "too\n",
      "very\n",
      "s\n",
      "t\n",
      "can\n",
      "will\n",
      "just\n",
      "don\n",
      "should\n",
      "now\n",
      "d\n",
      "ll\n",
      "m\n",
      "o\n",
      "re\n",
      "ve\n",
      "y\n",
      "ain\n",
      "aren\n",
      "couldn\n",
      "didn\n",
      "doesn\n",
      "hadn\n",
      "hasn\n",
      "haven\n",
      "isn\n",
      "ma\n",
      "mightn\n",
      "mustn\n",
      "needn\n",
      "shan\n",
      "shouldn\n",
      "wasn\n",
      "weren\n",
      "won\n",
      "wouldn\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "[print(x) for x in remover.getStopWords()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hash the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>filtered</th>\n",
       "      <th>rawFeatures</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Where if anywhere had he been on those nights of daemoniac alienage</td>\n",
       "      <td>[anywhere, nights, daemoniac, alienage]</td>\n",
       "      <td>(0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yet the effect is incongruous to the timid alone</td>\n",
       "      <td>[yet, effect, incongruous, timid, alone]</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I am glad Woodville is not with me for perhaps he would grieve and I desire to see smiles alone during the last scene of my life when I last wrote to him I told him of my ill health but not of its mortal tendency lest he should conceive it to be his duty to come to me for I fear lest the tears of friendship should destroy the blessed calm of my mind</td>\n",
       "      <td>[glad, woodville, perhaps, would, grieve, desire, see, smiles, alone, last, scene, life, last, wrote, told, ill, health, mortal, tendency, lest, conceive, duty, come, fear, lest, tears, friendship, destroy, blessed, calm, mind]</td>\n",
       "      <td>(0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 2.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You know the curled up paper tacked to that frightful canvas in the cellar the thing I thought was a photograph of some scene he meant to use as a background for that monster</td>\n",
       "      <td>[know, curled, paper, tacked, frightful, canvas, cellar, thing, thought, photograph, scene, meant, use, background, monster]</td>\n",
       "      <td>(0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sometimes she observed the war of elements thinking that they also declared against her and listened to the pattering of the rain in gloomy despair</td>\n",
       "      <td>[sometimes, observed, war, elements, thinking, also, declared, listened, pattering, rain, gloomy, despair]</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 2.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                              text  \\\n",
       "0  Where if anywhere had he been on those nights of daemoniac alienage                                                                                                                                                                                                                                                                                               \n",
       "1  Yet the effect is incongruous to the timid alone                                                                                                                                                                                                                                                                                                                  \n",
       "2  I am glad Woodville is not with me for perhaps he would grieve and I desire to see smiles alone during the last scene of my life when I last wrote to him I told him of my ill health but not of its mortal tendency lest he should conceive it to be his duty to come to me for I fear lest the tears of friendship should destroy the blessed calm of my mind   \n",
       "3  You know the curled up paper tacked to that frightful canvas in the cellar the thing I thought was a photograph of some scene he meant to use as a background for that monster                                                                                                                                                                                    \n",
       "4  Sometimes she observed the war of elements thinking that they also declared against her and listened to the pattering of the rain in gloomy despair                                                                                                                                                                                                               \n",
       "\n",
       "                                                                                                                                                                                                                              filtered  \\\n",
       "0  [anywhere, nights, daemoniac, alienage]                                                                                                                                                                                               \n",
       "1  [yet, effect, incongruous, timid, alone]                                                                                                                                                                                              \n",
       "2  [glad, woodville, perhaps, would, grieve, desire, see, smiles, alone, last, scene, life, last, wrote, told, ill, health, mortal, tendency, lest, conceive, duty, come, fear, lest, tears, friendship, destroy, blessed, calm, mind]   \n",
       "3  [know, curled, paper, tacked, frightful, canvas, cellar, thing, thought, photograph, scene, meant, use, background, monster]                                                                                                          \n",
       "4  [sometimes, observed, war, elements, thinking, also, declared, listened, pattering, rain, gloomy, despair]                                                                                                                            \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            rawFeatures  \n",
       "0  (0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)  \n",
       "1  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)  \n",
       "2  (0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 2.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0)  \n",
       "3  (0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0)  \n",
       "4  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 2.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0)  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=100)\n",
    "featurizedData = hashingTF.transform(removed)\n",
    "featurizedData.select(\"text\", \"filtered\", \"rawFeatures\").toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverse weight words that occur frequently across all text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>rawFeatures</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Where if anywhere had he been on those nights of daemoniac alienage</td>\n",
       "      <td>(0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)</td>\n",
       "      <td>(0.0, 2.3578376752, 0.0, 0.0, 0.0, 0.0, 2.23381436968, 2.31083762115, 0.0, 0.0, 0.0, 2.40962528497, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yet the effect is incongruous to the timid alone</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.23019211475, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.10190430106, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.07059584048, 0.0, 2.1897007534, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.21379830497, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I am glad Woodville is not with me for perhaps he would grieve and I desire to see smiles alone during the last scene of my life when I last wrote to him I told him of my ill health but not of its mortal tendency lest he should conceive it to be his duty to come to me for I fear lest the tears of friendship should destroy the blessed calm of my mind</td>\n",
       "      <td>(0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 2.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0)</td>\n",
       "      <td>(0.0, 0.0, 0.0, 2.08881451813, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.40962528497, 0.0, 1.51129462677, 0.0, 4.20199207157, 2.26007638317, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.96695410623, 0.0, 2.23019211475, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.03470009477, 0.0, 0.0, 0.0, 0.0, 1.96022806703, 0.0, 2.55577345979, 0.0, 0.0, 0.0, 2.21787163036, 0.0, 4.31305395943, 1.96062246656, 0.0, 0.0, 2.50623420641, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.24685916724, 2.17982900492, 1.92117419623, 1.94768871778, 2.07059584048, 0.0, 2.1897007534, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.91814503829, 2.31814307651, 0.0, 2.12395388522, 0.0, 0.0, 0.0, 1.95159040045, 0.0, 0.0, 0.0, 2.1603731383, 4.12016652018, 0.0, 0.0, 0.0, 0.0, 1.94846783741, 1.95472274443, 0.0, 0.0, 0.0, 0.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You know the curled up paper tacked to that frightful canvas in the cellar the thing I thought was a photograph of some scene he meant to use as a background for that monster</td>\n",
       "      <td>(0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0)</td>\n",
       "      <td>(0.0, 2.3578376752, 0.0, 0.0, 0.0, 1.87994272924, 0.0, 0.0, 0.0, 0.0, 2.12348952487, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0091168165, 0.0, 0.0, 0.0, 0.0, 2.29197960252, 0.0, 0.0, 0.0, 2.14507647293, 2.55577345979, 0.0, 2.42960098039, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.07059584048, 0.0, 2.1897007534, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.16956749293, 0.0, 0.0, 0.0, 2.09150872722, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.06008326009, 0.0, 0.0, 0.0, 0.0, 0.0, 1.95472274443, 2.24633409503, 0.0, 0.0, 0.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sometimes she observed the war of elements thinking that they also declared against her and listened to the pattering of the rain in gloomy despair</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 2.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0)</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.23381436968, 2.31083762115, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.26862801198, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.04795664982, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.17982900492, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.16956749293, 0.0, 0.0, 2.31814307651, 0.0, 4.24790777044, 0.0, 0.0, 0.0, 1.95159040045, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.34965528004, 0.0, 0.0, 0.0, 2.24633409503, 0.0, 0.0, 0.0)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                              text  \\\n",
       "0  Where if anywhere had he been on those nights of daemoniac alienage                                                                                                                                                                                                                                                                                               \n",
       "1  Yet the effect is incongruous to the timid alone                                                                                                                                                                                                                                                                                                                  \n",
       "2  I am glad Woodville is not with me for perhaps he would grieve and I desire to see smiles alone during the last scene of my life when I last wrote to him I told him of my ill health but not of its mortal tendency lest he should conceive it to be his duty to come to me for I fear lest the tears of friendship should destroy the blessed calm of my mind   \n",
       "3  You know the curled up paper tacked to that frightful canvas in the cellar the thing I thought was a photograph of some scene he meant to use as a background for that monster                                                                                                                                                                                    \n",
       "4  Sometimes she observed the war of elements thinking that they also declared against her and listened to the pattering of the rain in gloomy despair                                                                                                                                                                                                               \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            rawFeatures  \\\n",
       "0  (0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)   \n",
       "1  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)   \n",
       "2  (0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 2.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0)   \n",
       "3  (0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0)   \n",
       "4  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 2.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0)   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     features  \n",
       "0  (0.0, 2.3578376752, 0.0, 0.0, 0.0, 0.0, 2.23381436968, 2.31083762115, 0.0, 0.0, 0.0, 2.40962528497, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)                                                                                                                                                                                                                                                 \n",
       "1  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.23019211475, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.10190430106, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.07059584048, 0.0, 2.1897007534, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.21379830497, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)                                                                                                                                                                                                                                       \n",
       "2  (0.0, 0.0, 0.0, 2.08881451813, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.40962528497, 0.0, 1.51129462677, 0.0, 4.20199207157, 2.26007638317, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.96695410623, 0.0, 2.23019211475, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.03470009477, 0.0, 0.0, 0.0, 0.0, 1.96022806703, 0.0, 2.55577345979, 0.0, 0.0, 0.0, 2.21787163036, 0.0, 4.31305395943, 1.96062246656, 0.0, 0.0, 2.50623420641, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.24685916724, 2.17982900492, 1.92117419623, 1.94768871778, 2.07059584048, 0.0, 2.1897007534, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.91814503829, 2.31814307651, 0.0, 2.12395388522, 0.0, 0.0, 0.0, 1.95159040045, 0.0, 0.0, 0.0, 2.1603731383, 4.12016652018, 0.0, 0.0, 0.0, 0.0, 1.94846783741, 1.95472274443, 0.0, 0.0, 0.0, 0.0)  \n",
       "3  (0.0, 2.3578376752, 0.0, 0.0, 0.0, 1.87994272924, 0.0, 0.0, 0.0, 0.0, 2.12348952487, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0091168165, 0.0, 0.0, 0.0, 0.0, 2.29197960252, 0.0, 0.0, 0.0, 2.14507647293, 2.55577345979, 0.0, 2.42960098039, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.07059584048, 0.0, 2.1897007534, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.16956749293, 0.0, 0.0, 0.0, 2.09150872722, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.06008326009, 0.0, 0.0, 0.0, 0.0, 0.0, 1.95472274443, 2.24633409503, 0.0, 0.0, 0.0)                                                                                                                                     \n",
       "4  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.23381436968, 2.31083762115, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.26862801198, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.04795664982, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.17982900492, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.16956749293, 0.0, 0.0, 2.31814307651, 0.0, 4.24790777044, 0.0, 0.0, 0.0, 1.95159040045, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.34965528004, 0.0, 0.0, 0.0, 2.24633409503, 0.0, 0.0, 0.0)                                                                                                                                                                          "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import IDF\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "rescaledData.select(\"text\", \"rawFeatures\", \"features\").toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode the label column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "labelIndexer = StringIndexer(inputCol='author', outputCol='label').fit(dataUnion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Logistic Regression Algorithm to predict author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(labelCol = \"label\", maxIter=10, regParam=0.3, threshold=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert indexed labels back to original labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import IndexToString\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", labels=labelIndexer.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the machine learning pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages = [removePunctuationTrans, authorTrans, tokenizer, remover, hashingTF, idf, labelIndexer, lr, labelConverter]\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages = stages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the parameter setting of the pipeline stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove Punctuation SQL Transformer:\n",
      "statement: SQL statement (current: SELECT *, TRANSLATE(text,',.;?''\"+','') AS textNoPunct FROM __THIS__)\n",
      "*************************\n",
      "Remove Invalid Author Labels SQL Transformer:\n",
      "statement: SQL statement (current: SELECT * FROM __THIS__ WHERE author='EAP' OR author='HPL' OR author='MWS')\n",
      "*************************\n",
      "inputCol: input column name. (current: text)\n",
      "outputCol: output column name. (default: Tokenizer_42c4bc9876fac23c27bd__output, current: words)\n",
      "*************************\n",
      "Tokenizer:\n",
      "inputCol: input column name. (current: text)\n",
      "outputCol: output column name. (default: Tokenizer_42c4bc9876fac23c27bd__output, current: words)\n",
      "*************************\n",
      "Remover:\n",
      "caseSensitive: whether to do a case sensitive comparison over the stop words (default: False, current: False)\n",
      "inputCol: input column name. (current: words)\n",
      "outputCol: output column name. (default: StopWordsRemover_448f9ce0d82672eff3f6__output, current: filtered)\n",
      "stopWords: The words to be filtered out (default: [u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'you', u'your', u'yours', u'yourself', u'yourselves', u'he', u'him', u'his', u'himself', u'she', u'her', u'hers', u'herself', u'it', u'its', u'itself', u'they', u'them', u'their', u'theirs', u'themselves', u'what', u'which', u'who', u'whom', u'this', u'that', u'these', u'those', u'am', u'is', u'are', u'was', u'were', u'be', u'been', u'being', u'have', u'has', u'had', u'having', u'do', u'does', u'did', u'doing', u'a', u'an', u'the', u'and', u'but', u'if', u'or', u'because', u'as', u'until', u'while', u'of', u'at', u'by', u'for', u'with', u'about', u'against', u'between', u'into', u'through', u'during', u'before', u'after', u'above', u'below', u'to', u'from', u'up', u'down', u'in', u'out', u'on', u'off', u'over', u'under', u'again', u'further', u'then', u'once', u'here', u'there', u'when', u'where', u'why', u'how', u'all', u'any', u'both', u'each', u'few', u'more', u'most', u'other', u'some', u'such', u'no', u'nor', u'not', u'only', u'own', u'same', u'so', u'than', u'too', u'very', u's', u't', u'can', u'will', u'just', u'don', u'should', u'now', u'd', u'll', u'm', u'o', u're', u've', u'y', u'ain', u'aren', u'couldn', u'didn', u'doesn', u'hadn', u'hasn', u'haven', u'isn', u'ma', u'mightn', u'mustn', u'needn', u'shan', u'shouldn', u'wasn', u'weren', u'won', u'wouldn'])\n",
      "*************************\n",
      "HashingTF:\n",
      "binary: If True, all non zero counts are set to 1. This is useful for discrete probabilistic models that model binary events rather than integer counts. Default False. (default: False)\n",
      "inputCol: input column name. (current: filtered)\n",
      "numFeatures: number of features. (default: 262144, current: 100)\n",
      "outputCol: output column name. (default: HashingTF_47e6ba0d71802a85a3dd__output, current: rawFeatures)\n",
      "*************************\n",
      "IDF:\n",
      "inputCol: input column name. (current: rawFeatures)\n",
      "minDocFreq: minimum number of documents in which a term should appear for filtering (default: 0)\n",
      "outputCol: output column name. (default: IDF_468bb4aef74ca52de156__output, current: features)\n",
      "*************************\n",
      "LogisticRegression:\n",
      "aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\n",
      "family: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial (default: auto)\n",
      "featuresCol: features column name. (default: features)\n",
      "fitIntercept: whether to fit an intercept term. (default: True)\n",
      "labelCol: label column name. (default: label, current: label)\n",
      "maxIter: max number of iterations (>= 0). (default: 100, current: 10)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
      "rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\n",
      "regParam: regularization parameter (>= 0). (default: 0.0, current: 0.3)\n",
      "standardization: whether to standardize the training features before fitting the model. (default: True)\n",
      "threshold: Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p]. (default: 0.5, current: 0.7)\n",
      "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n",
      "*************************\n",
      "Pipeline:\n",
      "stages: a list of pipeline stages (current: [SQLTransformer_45bfb199f4d770c2d009, SQLTransformer_401b9806f5a73682630e, Tokenizer_42c4bc9876fac23c27bd, StopWordsRemover_448f9ce0d82672eff3f6, HashingTF_47e6ba0d71802a85a3dd, IDF_468bb4aef74ca52de156, StringIndexer_45898c748bf06572c5e3, LogisticRegression_4a848e72e00826fecb9f, IndexToString_43cda45d9dd8980d7f11])\n"
     ]
    }
   ],
   "source": [
    "print(\"Remove Punctuation SQL Transformer:\")\n",
    "print(removePunctuationTrans.explainParams())\n",
    "print(\"*************************\")\n",
    "print(\"Remove Invalid Author Labels SQL Transformer:\")\n",
    "print(authorTrans.explainParams())\n",
    "print(\"*************************\")\n",
    "print(tokenizer.explainParams())\n",
    "print(\"*************************\")\n",
    "print(\"Tokenizer:\")\n",
    "print(tokenizer.explainParams())\n",
    "print(\"*************************\")\n",
    "print(\"Remover:\")\n",
    "print(remover.explainParams())\n",
    "print(\"*************************\")\n",
    "print(\"HashingTF:\")\n",
    "print(hashingTF.explainParams())\n",
    "print(\"*************************\")\n",
    "print(\"IDF:\")\n",
    "print(idf.explainParams())\n",
    "print(\"*************************\")\n",
    "print(\"LogisticRegression:\")\n",
    "print(lr.explainParams())\n",
    "print(\"*************************\")\n",
    "print(\"Pipeline:\")\n",
    "print(pipeline.explainParams())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the dataset into training and test data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of records in the training data set is 12547.\n",
      "The number of rows labeled EAP in the training data set is 4933.\n",
      "The number of rows labeled HPL in the training data set is 3801.\n",
      "The number of rows labeled MWS in the training data set is 3813.\n",
      "\n",
      "The number of records in the test data set is 5460.\n",
      "The number of rows labeled EAP in the test data set is 2092.\n",
      "The number of rows labeled HPL in the test data set is 1644.\n",
      "The number of rows labeled MWS in the test data set is 1724.\n"
     ]
    }
   ],
   "source": [
    "train, test = dataUnion.randomSplit([70.0,30.0], seed=1)\n",
    "print('The number of records in the training data set is {}.'.format(train.count()))\n",
    "print('The number of rows labeled EAP in the training data set is {}.'.format(train.filter(train['author'] == 'EAP').count()))\n",
    "print('The number of rows labeled HPL in the training data set is {}.'.format(train.filter(train['author'] == 'HPL').count()))\n",
    "print('The number of rows labeled MWS in the training data set is {}.'.format(train.filter(train['author'] == 'MWS').count()))\n",
    "print(\"\")\n",
    "print('The number of records in the test data set is {}.'.format(test.count()))\n",
    "print('The number of rows labeled EAP in the test data set is {}.'.format(test.filter(test['author'] == 'EAP').count()))\n",
    "print('The number of rows labeled HPL in the test data set is {}.'.format(test.filter(test['author'] == 'HPL').count()))\n",
    "print('The number of rows labeled MWS in the test data set is {}.'.format(test.filter(test['author'] == 'MWS').count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model using the training data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions using the test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>label</th>\n",
       "      <th>prediction</th>\n",
       "      <th>predictedLabel</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MWS</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[0.374734316168, 0.310769999674, 0.314495684158]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EAP</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[0.436005102966, 0.276185090862, 0.287809806172]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MWS</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[0.390615230322, 0.291331800252, 0.318052969426]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MWS</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>HPL</td>\n",
       "      <td>[0.229277580922, 0.271608026015, 0.499114393063]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HPL</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>HPL</td>\n",
       "      <td>[0.356927998973, 0.285659993041, 0.357412007986]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  author  label  prediction predictedLabel  \\\n",
       "0  MWS    1      0           EAP             \n",
       "1  EAP    0      0           EAP             \n",
       "2  MWS    1      0           EAP             \n",
       "3  MWS    1      2           HPL             \n",
       "4  HPL    2      2           HPL             \n",
       "\n",
       "                                        probability  \n",
       "0  [0.374734316168, 0.310769999674, 0.314495684158]  \n",
       "1  [0.436005102966, 0.276185090862, 0.287809806172]  \n",
       "2  [0.390615230322, 0.291331800252, 0.318052969426]  \n",
       "3  [0.229277580922, 0.271608026015, 0.499114393063]  \n",
       "4  [0.356927998973, 0.285659993041, 0.357412007986]  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.select(\"author\", \"label\", \"prediction\", 'predictedLabel', \"probability\").toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model performance by calculating the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 47.05%.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol = \"label\", predictionCol=\"prediction\").setMetricName(\"accuracy\")\n",
    "print('Accuracy = {:0.2f}%.'.format(evaluator.evaluate(predictions)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate the prediction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted EAP correctly 1697 times.\n",
      "Failed to predict EAP 395 times.\n",
      "Predicted EAP incorrectly 2107 times.\n"
     ]
    }
   ],
   "source": [
    "EAPandEAP = predictions.filter(predictions['author']=='EAP').filter(predictions['predictedLabel']=='EAP').count()\n",
    "EAPnotEAP = predictions.filter(predictions['author']=='EAP').filter(predictions['predictedLabel']!='EAP').count()\n",
    "notEAPbutEAP = predictions.filter(predictions['author']!='EAP').filter(predictions['predictedLabel']=='EAP').count()\n",
    "print(\"Predicted EAP correctly {} times.\".format(EAPandEAP))\n",
    "print(\"Failed to predict EAP {} times.\".format(EAPnotEAP))\n",
    "print(\"Predicted EAP incorrectly {} times.\".format(notEAPbutEAP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted HPL correctly 511 times.\n",
      "Failed to predict HPL 1133 times.\n",
      "Predicted HPL incorrectly 434 times.\n"
     ]
    }
   ],
   "source": [
    "HPLandHPL = predictions.filter(predictions['author']=='HPL').filter(predictions['predictedLabel']=='HPL').count()\n",
    "HPLnotHPL = predictions.filter(predictions['author']=='HPL').filter(predictions['predictedLabel']!='HPL').count()\n",
    "notHPLbutHPL = predictions.filter(predictions['author']!='HPL').filter(predictions['predictedLabel']=='HPL').count()\n",
    "print(\"Predicted HPL correctly {} times.\".format(HPLandHPL))\n",
    "print(\"Failed to predict HPL {} times.\".format(HPLnotHPL))\n",
    "print(\"Predicted HPL incorrectly {} times.\".format(notHPLbutHPL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted MWS correctly 361 times.\n",
      "Failed to predict MWS 1363 times.\n",
      "Predicted MWS incorrectly 350 times.\n"
     ]
    }
   ],
   "source": [
    "MWSandMWS = predictions.filter(predictions['author']=='MWS').filter(predictions['predictedLabel']=='MWS').count()\n",
    "MWSnotMWS = predictions.filter(predictions['author']=='MWS').filter(predictions['predictedLabel']!='MWS').count()\n",
    "notMWSbutMWS = predictions.filter(predictions['author']!='MWS').filter(predictions['predictedLabel']=='MWS').count()\n",
    "print(\"Predicted MWS correctly {} times.\".format(MWSandMWS))\n",
    "print(\"Failed to predict MWS {} times.\".format(MWSnotMWS))\n",
    "print(\"Predicted MWS incorrectly {} times.\".format(notMWSbutMWS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Natural Language Watson Understanding to create rich features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup configuration for the Watson Natural Language Understanding (NLU) service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import watson_developer_cloud\n",
    "from watson_developer_cloud import NaturalLanguageUnderstandingV1\n",
    "import watson_developer_cloud.natural_language_understanding.features.v1 as Features\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLU_USERNAME = 'de841bd0-8142-4715-b7f6-0885510fde6a'\n",
    "NLU_PASSWORD = 'D8K4U5duW72f'\n",
    "natural_language_understanding = NaturalLanguageUnderstandingV1(\n",
    "  username=NLU_USERNAME,\n",
    "  password=NLU_PASSWORD,\n",
    "  version=\"2017-02-27\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show example of employing NLU API on single row of the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataNLUtest = dataUnion.select(dataUnion[\"text\"]).toJSON().collect()[0][8:-1]\n",
    "print(dataNLUtest)\n",
    "import json\n",
    "features=[\n",
    "    Features.Emotion(),\n",
    "    Features.Sentiment()\n",
    "  ]\n",
    "nlu = natural_language_understanding.analyze(text=dataNLUtest, features=features, language='en', clean='true')\n",
    "anger = nlu['emotion']['document']['emotion']['anger']\n",
    "joy = nlu['emotion']['document']['emotion']['joy']\n",
    "sadness = nlu['emotion']['document']['emotion']['sadness']\n",
    "fear = nlu['emotion']['document']['emotion']['fear']\n",
    "disgust = nlu['emotion']['document']['emotion']['disgust']\n",
    "sentiment = nlu['sentiment']['document']['score']\n",
    "\n",
    "print(\"\")\n",
    "print(\"Anger = {}\".format(anger))\n",
    "print(\"Joy = {}\".format(joy))\n",
    "print(\"Sadness = {}\".format(sadness))\n",
    "print(\"Fear = {}\".format(fear))\n",
    "print(\"Disgust = {}\".format(disgust))\n",
    "print(\"Sentiment = {}\".format(sentiment))\n",
    "print(\"\")\n",
    "print(json.dumps(nlu, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define UDF to create NLU derived features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "#import json\n",
    "udfNLU = (udf(lambda text: json.dumps(NaturalLanguageUnderstandingV1(\n",
    "    username=NLU_USERNAME, password=NLU_PASSWORD, version=\"2017-02-27\")\n",
    "    .analyze(text=text, features=features, language='en', clean='true'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoke UDF to create new column with NLU output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataSmall = data.limit(10)\n",
    "#dataSmall = dataSmall.withColumn('nlu', udfNLU(dataSmall['text']))\n",
    "#dataSmall.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data00NLU = data00.withColumn('nlu', udfNLU(data00['text']))\n",
    "data01NLU = data01.withColumn('nlu', udfNLU(data01['text']))\n",
    "data02NLU = data02.withColumn('nlu', udfNLU(data02['text']))\n",
    "data03NLU = data03.withColumn('nlu', udfNLU(data03['text']))\n",
    "data04NLU = data04.withColumn('nlu', udfNLU(data04['text']))\n",
    "data05NLU = data05.withColumn('nlu', udfNLU(data05['text']))\n",
    "data06NLU = data06.withColumn('nlu', udfNLU(data06['text']))\n",
    "data07NLU = data07.withColumn('nlu', udfNLU(data07['text']))\n",
    "data08NLU = data08.withColumn('nlu', udfNLU(data08['text']))\n",
    "data09NLU = data09.withColumn('nlu', udfNLU(data09['text']))\n",
    "data10NLU = data10.withColumn('nlu', udfNLU(data10['text']))\n",
    "data11NLU = data11.withColumn('nlu', udfNLU(data11['text']))\n",
    "data12NLU = data12.withColumn('nlu', udfNLU(data12['text']))\n",
    "data13NLU = data13.withColumn('nlu', udfNLU(data13['text']))\n",
    "data14NLU = data14.withColumn('nlu', udfNLU(data14['text']))\n",
    "data15NLU = data15.withColumn('nlu', udfNLU(data15['text']))\n",
    "data16NLU = data16.withColumn('nlu', udfNLU(data16['text']))\n",
    "data17NLU = data17.withColumn('nlu', udfNLU(data17['text']))\n",
    "data18NLU = data18.withColumn('nlu', udfNLU(data18['text']))\n",
    "data19NLU = data19.withColumn('nlu', udfNLU(data19['text']))\n",
    "data20NLU = data20.withColumn('nlu', udfNLU(data20['text']))\n",
    "data21NLU = data21.withColumn('nlu', udfNLU(data21['text']))\n",
    "data22NLU = data22.withColumn('nlu', udfNLU(data22['text']))\n",
    "data23NLU = data23.withColumn('nlu', udfNLU(data23['text']))\n",
    "data24NLU = data24.withColumn('nlu', udfNLU(data24['text']))\n",
    "data25NLU = data25.withColumn('nlu', udfNLU(data25['text']))\n",
    "data26NLU = data26.withColumn('nlu', udfNLU(data26['text']))\n",
    "data27NLU = data27.withColumn('nlu', udfNLU(data27['text']))\n",
    "data28NLU = data28.withColumn('nlu', udfNLU(data28['text']))\n",
    "data29NLU = data29.withColumn('nlu', udfNLU(data29['text']))\n",
    "data30NLU = data30.withColumn('nlu', udfNLU(data30['text']))\n",
    "data31NLU = data31.withColumn('nlu', udfNLU(data31['text']))\n",
    "data32NLU = data32.withColumn('nlu', udfNLU(data32['text']))\n",
    "data33NLU = data33.withColumn('nlu', udfNLU(data33['text']))\n",
    "data34NLU = data34.withColumn('nlu', udfNLU(data34['text']))\n",
    "data35NLU = data35.withColumn('nlu', udfNLU(data35['text']))\n",
    "data36NLU = data36.withColumn('nlu', udfNLU(data36['text']))\n",
    "data37NLU = data37.withColumn('nlu', udfNLU(data37['text']))\n",
    "data38NLU = data38.withColumn('nlu', udfNLU(data38['text']))\n",
    "data39NLU = data39.withColumn('nlu', udfNLU(data39['text']))\n",
    "data40NLU = data40.withColumn('nlu', udfNLU(data40['text']))\n",
    "data41NLU = data41.withColumn('nlu', udfNLU(data41['text']))\n",
    "data42NLU = data42.withColumn('nlu', udfNLU(data42['text']))\n",
    "data43NLU = data43.withColumn('nlu', udfNLU(data43['text']))\n",
    "data44NLU = data44.withColumn('nlu', udfNLU(data44['text']))\n",
    "data45NLU = data45.withColumn('nlu', udfNLU(data45['text']))\n",
    "data46NLU = data46.withColumn('nlu', udfNLU(data46['text']))\n",
    "data47NLU = data47.withColumn('nlu', udfNLU(data47['text']))\n",
    "data48NLU = data48.withColumn('nlu', udfNLU(data48['text']))\n",
    "data49NLU = data49.withColumn('nlu', udfNLU(data49['text']))\n",
    "data50NLU = data50.withColumn('nlu', udfNLU(data50['text']))\n",
    "data51NLU = data51.withColumn('nlu', udfNLU(data51['text']))\n",
    "data52NLU = data52.withColumn('nlu', udfNLU(data52['text']))\n",
    "data53NLU = data53.withColumn('nlu', udfNLU(data53['text']))\n",
    "data54NLU = data54.withColumn('nlu', udfNLU(data54['text']))\n",
    "data55NLU = data55.withColumn('nlu', udfNLU(data55['text']))\n",
    "data56NLU = data56.withColumn('nlu', udfNLU(data56['text']))\n",
    "data57NLU = data57.withColumn('nlu', udfNLU(data57['text']))\n",
    "data58NLU = data58.withColumn('nlu', udfNLU(data58['text']))\n",
    "data59NLU = data59.withColumn('nlu', udfNLU(data59['text']))\n",
    "data60NLU = data60.withColumn('nlu', udfNLU(data60['text']))\n",
    "data61NLU = data61.withColumn('nlu', udfNLU(data61['text']))\n",
    "data62NLU = data62.withColumn('nlu', udfNLU(data62['text']))\n",
    "data63NLU = data63.withColumn('nlu', udfNLU(data63['text']))\n",
    "data64NLU = data64.withColumn('nlu', udfNLU(data64['text']))\n",
    "data65NLU = data65.withColumn('nlu', udfNLU(data65['text']))\n",
    "data66NLU = data66.withColumn('nlu', udfNLU(data66['text']))\n",
    "data67NLU = data67.withColumn('nlu', udfNLU(data67['text']))\n",
    "data68NLU = data68.withColumn('nlu', udfNLU(data68['text']))\n",
    "data69NLU = data69.withColumn('nlu', udfNLU(data69['text']))\n",
    "data70NLU = data70.withColumn('nlu', udfNLU(data70['text']))\n",
    "data71NLU = data71.withColumn('nlu', udfNLU(data71['text']))\n",
    "data72NLU = data72.withColumn('nlu', udfNLU(data72['text']))\n",
    "data73NLU = data73.withColumn('nlu', udfNLU(data73['text']))\n",
    "data74NLU = data74.withColumn('nlu', udfNLU(data74['text']))\n",
    "data75NLU = data75.withColumn('nlu', udfNLU(data75['text']))\n",
    "data76NLU = data76.withColumn('nlu', udfNLU(data76['text']))\n",
    "data77NLU = data77.withColumn('nlu', udfNLU(data77['text']))\n",
    "data78NLU = data78.withColumn('nlu', udfNLU(data78['text']))\n",
    "data79NLU = data79.withColumn('nlu', udfNLU(data79['text']))\n",
    "data80NLU = data80.withColumn('nlu', udfNLU(data80['text']))\n",
    "data81NLU = data81.withColumn('nlu', udfNLU(data81['text']))\n",
    "data82NLU = data82.withColumn('nlu', udfNLU(data82['text']))\n",
    "data83NLU = data83.withColumn('nlu', udfNLU(data83['text']))\n",
    "data84NLU = data84.withColumn('nlu', udfNLU(data84['text']))\n",
    "data85NLU = data85.withColumn('nlu', udfNLU(data85['text']))\n",
    "data86NLU = data86.withColumn('nlu', udfNLU(data86['text']))\n",
    "data87NLU = data87.withColumn('nlu', udfNLU(data87['text']))\n",
    "data88NLU = data88.withColumn('nlu', udfNLU(data88['text']))\n",
    "data89NLU = data89.withColumn('nlu', udfNLU(data89['text']))\n",
    "data90NLU = data90.withColumn('nlu', udfNLU(data90['text']))\n",
    "data91NLU = data91.withColumn('nlu', udfNLU(data91['text']))\n",
    "data92NLU = data92.withColumn('nlu', udfNLU(data92['text']))\n",
    "data93NLU = data93.withColumn('nlu', udfNLU(data93['text']))\n",
    "data94NLU = data94.withColumn('nlu', udfNLU(data94['text']))\n",
    "data95NLU = data95.withColumn('nlu', udfNLU(data95['text']))\n",
    "data96NLU = data96.withColumn('nlu', udfNLU(data96['text']))\n",
    "data97NLU = data97.withColumn('nlu', udfNLU(data97['text']))\n",
    "data98NLU = data98.withColumn('nlu', udfNLU(data98['text']))\n",
    "data99NLU = data99.withColumn('nlu', udfNLU(data99['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The combined dataset contains 18047 rows.\n"
     ]
    }
   ],
   "source": [
    "dataNLU= (data00NLU.union(data01NLU).union(data02NLU).union(data03NLU).union(data04NLU).union(data05NLU).union(data06NLU).union(data07NLU).union(data08NLU).union(data09NLU)\n",
    "            .union(data10NLU).union(data11NLU).union(data12NLU).union(data13NLU).union(data14NLU).union(data15NLU).union(data16NLU).union(data17NLU).union(data18NLU).union(data19NLU)\n",
    "            .union(data20NLU).union(data21NLU).union(data22NLU).union(data23NLU).union(data24NLU).union(data25NLU).union(data26NLU).union(data27NLU).union(data28NLU).union(data29NLU)\n",
    "            .union(data30NLU).union(data31NLU).union(data32NLU).union(data33NLU).union(data34NLU).union(data35NLU).union(data36NLU).union(data37NLU).union(data38NLU).union(data39NLU)\n",
    "            .union(data40NLU).union(data41NLU).union(data42NLU).union(data43NLU).union(data44NLU).union(data45NLU).union(data46NLU).union(data47NLU).union(data48NLU).union(data49NLU)\n",
    "            .union(data50NLU).union(data51NLU).union(data52NLU).union(data53NLU).union(data54NLU).union(data55NLU).union(data56NLU).union(data57NLU).union(data58NLU).union(data59NLU)\n",
    "            .union(data60NLU).union(data61NLU).union(data62NLU).union(data63NLU).union(data64NLU).union(data65NLU).union(data66NLU).union(data67NLU).union(data68NLU).union(data69NLU)\n",
    "            .union(data70NLU).union(data71NLU).union(data72NLU).union(data73NLU).union(data74NLU).union(data75NLU).union(data76NLU).union(data77NLU).union(data78NLU).union(data79NLU)\n",
    "            .union(data80NLU).union(data81NLU).union(data82NLU).union(data83NLU).union(data84NLU).union(data85NLU).union(data86NLU).union(data87NLU).union(data88NLU).union(data89NLU)\n",
    "            .union(data90NLU).union(data91NLU).union(data92NLU).union(data93NLU).union(data94NLU).union(data95NLU).union(data96NLU).union(data97NLU).union(data98NLU).union(data99NLU)\n",
    "            .cache())\n",
    "\n",
    "print(\"The combined dataset contains {} rows.\".format(dataNLU.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define UDFs to identify bad rows retuned by NLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "udfAngerTest = udf(lambda nlu: json.loads(nlu))\n",
    "udfJoyTest = udf(lambda nlu: json.loads(nlu))\n",
    "udfSadnessTest = udf(lambda nlu: json.loads(nlu))\n",
    "udfFearTest = udf(lambda nlu: json.loads(nlu))\n",
    "udfDisgustTest = udf(lambda nlu: json.loads(nlu))\n",
    "udfSentimentTest = udf(lambda nlu: json.loads(nlu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataNLUtest = (dataNLU.withColumn('AngerTest', udfAngerTest(dataNLU['nlu']))\n",
    "        .withColumn('JoyTest', udfJoyTest(dataNLU['nlu']))\n",
    "        .withColumn('SadnessTest', udfSadnessTest(dataNLU['nlu']))\n",
    "        .withColumn('FearTest', udfFearTest(dataNLU['nlu']))\n",
    "        .withColumn('DisgustTest', udfDisgustTest(dataNLU['nlu']))\n",
    "        .withColumn('SentimentTest', udfSentimentTest(dataNLU['nlu'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of bad rows found = 0\n",
      "Number of bad rows found = 0\n",
      "Number of bad rows found = 0\n",
      "Number of bad rows found = 0\n",
      "Number of bad rows found = 0\n",
      "Number of bad rows found = 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of bad rows found = {}\".format(dataNLUtest.filter(~ col('AngerTest').like('%anger%')).count()))\n",
    "print(\"Number of bad rows found = {}\".format(dataNLUtest.filter(~ col('JoyTest').like('%joy%')).count()))\n",
    "print(\"Number of bad rows found = {}\".format(dataNLUtest.filter(~ col('SadnessTest').like('%sadness%')).count()))\n",
    "print(\"Number of bad rows found = {}\".format(dataNLUtest.filter(~ col('FearTest').like('%fear%')).count()))\n",
    "print(\"Number of bad rows found = {}\".format(dataNLUtest.filter(~ col('DisgustTest').like('%disgust%')).count()))\n",
    "print(\"Number of bad rows found = {}\".format(dataNLUtest.filter(~ col('SentimentTest').like('%sentiment%')).count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove bad rows returned from NLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataNLU = (dataNLUtest.filter(col('AngerTest').like('%anger%'))\n",
    "            .filter(col('JoyTest').like('%joy%'))\n",
    "            .filter(col('SadnessTest').like('%sadness%'))\n",
    "            .filter(col('FearTest').like('%fear%'))\n",
    "            .filter(col('DisgustTest').like('%disgust%'))\n",
    "            .filter(col('SentimentTest').like('%sentiment%')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of bad rows found = 0\n",
      "Number of bad rows found = 0\n",
      "Number of bad rows found = 0\n",
      "Number of bad rows found = 0\n",
      "Number of bad rows found = 0\n",
      "Number of bad rows found = 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of bad rows found = {}\".format(dataNLU.filter(~ col('AngerTest').like('%anger%')).count()))\n",
    "print(\"Number of bad rows found = {}\".format(dataNLU.filter(~ col('JoyTest').like('%joy%')).count()))\n",
    "print(\"Number of bad rows found = {}\".format(dataNLU.filter(~ col('SadnessTest').like('%sadness%')).count()))\n",
    "print(\"Number of bad rows found = {}\".format(dataNLU.filter(~ col('FearTest').like('%fear%')).count()))\n",
    "print(\"Number of bad rows found = {}\".format(dataNLU.filter(~ col('DisgustTest').like('%disgust%')).count()))\n",
    "print(\"Number of bad rows found = {}\".format(dataNLU.filter(~ col('SentimentTest').like('%sentiment%')).count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop NLU test columnts\n",
    "dataNLU = dataNLU.drop('AngerTest','JoyTest', 'SadnessTest', 'FearTest', 'DisgustTest', 'SentimentTest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define UDFs to extract NLU derived features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "udfAnger = udf(lambda nlu: json.loads(nlu)[\"emotion\"][\"document\"][\"emotion\"][\"anger\"], DoubleType())\n",
    "udfJoy = udf(lambda nlu: json.loads(nlu)[\"emotion\"][\"document\"][\"emotion\"][\"joy\"], DoubleType())\n",
    "udfSadness = udf(lambda nlu: json.loads(nlu)[\"emotion\"][\"document\"][\"emotion\"][\"sadness\"], DoubleType())\n",
    "udfFear = udf(lambda nlu: json.loads(nlu)[\"emotion\"][\"document\"][\"emotion\"][\"fear\"], DoubleType())\n",
    "udfDisgust = udf(lambda nlu: json.loads(nlu)[\"emotion\"][\"document\"][\"emotion\"][\"disgust\"], DoubleType())\n",
    "udfSentiment = udf(lambda nlu: json.loads(nlu)['sentiment']['document']['score'], DoubleType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoke UDFs to create new columns for the enhanced emotion and sentiment features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataNLU2 = (dataSmall.withColumn('Anger', udfAnger(dataSmall['nlu']))\n",
    "#        .withColumn('Joy', udfJoy(dataSmall['nlu']))\n",
    "#        .withColumn('Sadness', udfSadness(dataSmall['nlu']))\n",
    "#        .withColumn('Fear', udfFear(dataSmall['nlu']))\n",
    "#        .withColumn('Disgust', udfDisgust(dataSmall['nlu']))\n",
    "#        .withColumn('Sentiment', udfSentiment(dataSmall['nlu'])))\n",
    "#dataNLU2.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataNLU = (dataNLU.withColumn('Anger', udfAnger(dataNLU['nlu']))\n",
    "        .withColumn('Joy', udfJoy(dataNLU['nlu']))\n",
    "        .withColumn('Sadness', udfSadness(dataNLU['nlu']))\n",
    "        .withColumn('Fear', udfFear(dataNLU['nlu']))\n",
    "        .withColumn('Disgust', udfDisgust(dataNLU['nlu']))\n",
    "        .withColumn('Sentiment', udfSentiment(dataNLU['nlu'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Anger</th>\n",
       "      <th>Joy</th>\n",
       "      <th>Sadness</th>\n",
       "      <th>Fear</th>\n",
       "      <th>Disgust</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I walked rapidly, softly, and close to the ruined houses.</td>\n",
       "      <td>0.417696</td>\n",
       "      <td>0.021155</td>\n",
       "      <td>0.482235</td>\n",
       "      <td>0.298235</td>\n",
       "      <td>0.134835</td>\n",
       "      <td>-0.651749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The youth's febrile mind, apparently, was dwelling on strange things; and the doctor shuddered now and then as he spoke of them.</td>\n",
       "      <td>0.055444</td>\n",
       "      <td>0.116561</td>\n",
       "      <td>0.365396</td>\n",
       "      <td>0.351243</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>-0.765087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"But since the murderer has been discovered \"\" \"\"The murderer discovered Good God how can that be? who could attempt to pursue him?\"</td>\n",
       "      <td>0.218197</td>\n",
       "      <td>0.137784</td>\n",
       "      <td>0.393301</td>\n",
       "      <td>0.066075</td>\n",
       "      <td>0.429190</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>So thick were the vapours that the way was hard, and though Atal followed on at last, he could scarce see the grey shape of Barzai on the dim slope above in the clouded moonlight.</td>\n",
       "      <td>0.032738</td>\n",
       "      <td>0.150773</td>\n",
       "      <td>0.694929</td>\n",
       "      <td>0.203870</td>\n",
       "      <td>0.029030</td>\n",
       "      <td>-0.506536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>By authority of the king such districts were placed under ban, and all persons forbidden, under pain of death, to intrude upon their dismal solitude.</td>\n",
       "      <td>0.247888</td>\n",
       "      <td>0.026101</td>\n",
       "      <td>0.771124</td>\n",
       "      <td>0.095373</td>\n",
       "      <td>0.122879</td>\n",
       "      <td>-0.858506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Thus, while Perdita was entertaining her guests, and anxiously awaiting the arrival of her lord, his ring was brought her; and she was told that a poor woman had a note to deliver to her from its wearer.</td>\n",
       "      <td>0.098105</td>\n",
       "      <td>0.104288</td>\n",
       "      <td>0.470236</td>\n",
       "      <td>0.089047</td>\n",
       "      <td>0.107151</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>As the evening wore away he became more and more absorbed in reverie, from which no sallies of mine could arouse him.</td>\n",
       "      <td>0.110423</td>\n",
       "      <td>0.141169</td>\n",
       "      <td>0.527687</td>\n",
       "      <td>0.172764</td>\n",
       "      <td>0.117253</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>He stopped in his tracks then, flailing his arms wildly in the air, began to stagger backward.</td>\n",
       "      <td>0.234947</td>\n",
       "      <td>0.170794</td>\n",
       "      <td>0.115705</td>\n",
       "      <td>0.310494</td>\n",
       "      <td>0.091302</td>\n",
       "      <td>-0.868316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>In feeling my way I had found many angles, and thus deduced an idea of great irregularity; so potent is the effect of total darkness upon one arousing from lethargy or sleep The angles were simply those of a few slight depressions, or niches, at odd intervals.</td>\n",
       "      <td>0.021526</td>\n",
       "      <td>0.102365</td>\n",
       "      <td>0.736927</td>\n",
       "      <td>0.158123</td>\n",
       "      <td>0.025682</td>\n",
       "      <td>-0.891721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>He seemed insensible to the presence of any one else, but if, as a trial to awaken his sensibility, my aunt brought me into the room he would instantly rush out with every symptom of fury and distraction.</td>\n",
       "      <td>0.437653</td>\n",
       "      <td>0.155442</td>\n",
       "      <td>0.238697</td>\n",
       "      <td>0.203852</td>\n",
       "      <td>0.013623</td>\n",
       "      <td>-0.915947</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                   text  \\\n",
       "0  I walked rapidly, softly, and close to the ruined houses.                                                                                                                                                                                                              \n",
       "1  The youth's febrile mind, apparently, was dwelling on strange things; and the doctor shuddered now and then as he spoke of them.                                                                                                                                       \n",
       "2  \"But since the murderer has been discovered \"\" \"\"The murderer discovered Good God how can that be? who could attempt to pursue him?\"                                                                                                                                   \n",
       "3  So thick were the vapours that the way was hard, and though Atal followed on at last, he could scarce see the grey shape of Barzai on the dim slope above in the clouded moonlight.                                                                                    \n",
       "4  By authority of the king such districts were placed under ban, and all persons forbidden, under pain of death, to intrude upon their dismal solitude.                                                                                                                  \n",
       "5  Thus, while Perdita was entertaining her guests, and anxiously awaiting the arrival of her lord, his ring was brought her; and she was told that a poor woman had a note to deliver to her from its wearer.                                                            \n",
       "6  As the evening wore away he became more and more absorbed in reverie, from which no sallies of mine could arouse him.                                                                                                                                                  \n",
       "7  He stopped in his tracks then, flailing his arms wildly in the air, began to stagger backward.                                                                                                                                                                         \n",
       "8  In feeling my way I had found many angles, and thus deduced an idea of great irregularity; so potent is the effect of total darkness upon one arousing from lethargy or sleep The angles were simply those of a few slight depressions, or niches, at odd intervals.   \n",
       "9  He seemed insensible to the presence of any one else, but if, as a trial to awaken his sensibility, my aunt brought me into the room he would instantly rush out with every symptom of fury and distraction.                                                           \n",
       "\n",
       "      Anger       Joy   Sadness      Fear   Disgust  Sentiment  \n",
       "0  0.417696  0.021155  0.482235  0.298235  0.134835 -0.651749   \n",
       "1  0.055444  0.116561  0.365396  0.351243  0.088965 -0.765087   \n",
       "2  0.218197  0.137784  0.393301  0.066075  0.429190  0.000000   \n",
       "3  0.032738  0.150773  0.694929  0.203870  0.029030 -0.506536   \n",
       "4  0.247888  0.026101  0.771124  0.095373  0.122879 -0.858506   \n",
       "5  0.098105  0.104288  0.470236  0.089047  0.107151  0.000000   \n",
       "6  0.110423  0.141169  0.527687  0.172764  0.117253  0.000000   \n",
       "7  0.234947  0.170794  0.115705  0.310494  0.091302 -0.868316   \n",
       "8  0.021526  0.102365  0.736927  0.158123  0.025682 -0.891721   \n",
       "9  0.437653  0.155442  0.238697  0.203852  0.013623 -0.915947   "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataNLU.select(dataNLU['text'], dataNLU['Anger'], dataNLU['Joy'], dataNLU['Sadness'], dataNLU['Fear'], dataNLU['Disgust'], dataNLU['Sentiment']).toPandas().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrain model with NLU features added"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the dataset into training and test data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainNLU, testNLU = dataNLU.randomSplit([70.0,30.0], seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bucketize the NLU features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Bucketizer\n",
    "AngerBucketSplits = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "AngerBucket = Bucketizer(splits=AngerBucketSplits, inputCol=\"Anger\", outputCol=\"AngerBucket\")\n",
    "JoyBucketSplits = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "JoyBucket = Bucketizer(splits=JoyBucketSplits, inputCol=\"Joy\", outputCol=\"JoyBucket\")\n",
    "SadnessBucketSplits = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "SadnessBucket = Bucketizer(splits=SadnessBucketSplits, inputCol=\"Sadness\", outputCol=\"SadnessBucket\")\n",
    "FearBucketSplits = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "FearBucket = Bucketizer(splits=FearBucketSplits, inputCol=\"Fear\", outputCol=\"FearBucket\")\n",
    "DisgustBucketSplits = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "DisgustBucket = Bucketizer(splits=DisgustBucketSplits, inputCol=\"Disgust\", outputCol=\"DisgustBucket\")\n",
    "SentimentBucketSplits = [-1.0, -0.9, -0.8, -0.7, -0.6, -0.5, -0.4, -0.3, -0.2, -0.1, 0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "SentimentBucket = Bucketizer(splits=SentimentBucketSplits, inputCol=\"Sentiment\", outputCol=\"SentimentBucket\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = (VectorAssembler(inputCols=[\"features\", \"AngerBucket\", \"JoyBucket\", \"SadnessBucket\", \"FearBucket\", \"DisgustBucket\",\"SentimentBucket\"], \n",
    "             outputCol=\"featuresNLU\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a revised machine learning pipeline utilizing the new bucketed NLU feaures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrNLU = LogisticRegression(labelCol = \"label\", featuresCol= \"featuresNLU\", maxIter=10, regParam=0.3, threshold=0.5)\n",
    "stagesNLU = ([tokenizer, remover, hashingTF, idf, AngerBucket, JoyBucket, SadnessBucket, FearBucket, DisgustBucket, SentimentBucket,\n",
    "            assembler, labelIndexer, lrNLU, labelConverter])\n",
    "pipelineNLU = Pipeline(stages = stagesNLU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the new model using the training data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelNLU = pipelineNLU.fit(trainNLU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make updated predictions (with NLU features) using the test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionsNLU = modelNLU.transform(testNLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>label</th>\n",
       "      <th>prediction</th>\n",
       "      <th>predictedLabel</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EAP</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[0.406589386585, 0.259198863094, 0.334211750321]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EAP</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[0.445357856625, 0.310846362146, 0.24379578123]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MWS</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[0.341663611648, 0.316793807219, 0.341542581133]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MWS</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[0.43338070479, 0.341784103287, 0.224835191923]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MWS</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[0.404221211604, 0.286485576834, 0.309293211562]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  author  label  prediction predictedLabel  \\\n",
       "0  EAP    0      0           EAP             \n",
       "1  EAP    0      0           EAP             \n",
       "2  MWS    1      0           EAP             \n",
       "3  MWS    1      0           EAP             \n",
       "4  MWS    1      0           EAP             \n",
       "\n",
       "                                        probability  \n",
       "0  [0.406589386585, 0.259198863094, 0.334211750321]  \n",
       "1  [0.445357856625, 0.310846362146, 0.24379578123]   \n",
       "2  [0.341663611648, 0.316793807219, 0.341542581133]  \n",
       "3  [0.43338070479, 0.341784103287, 0.224835191923]   \n",
       "4  [0.404221211604, 0.286485576834, 0.309293211562]  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictionsNLU.select(\"author\", \"label\", \"prediction\", 'predictedLabel', \"probability\").toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the updated model performance by calculating the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with NLU = 47.94%.\n"
     ]
    }
   ],
   "source": [
    "evaluatorNLU = MulticlassClassificationEvaluator(labelCol = \"label\", predictionCol=\"prediction\").setMetricName(\"accuracy\")\n",
    "print('Accuracy with NLU = {:0.2f}%.'.format(evaluatorNLU.evaluate(predictionsNLU)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate Improved Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted EAP correctly 1597 times vs. 1703 previously.\n",
      "Failed to predict EAP 532 times vs. 424 previously.\n",
      "Predicted EAP incorrectly 1805 times vs. 2137 previously.\n"
     ]
    }
   ],
   "source": [
    "EAPandEAPnlu = predictionsNLU.filter(predictionsNLU['author']=='EAP').filter(predictionsNLU['predictedLabel']=='EAP').count()\n",
    "EAPnotEAPnlu = predictionsNLU.filter(predictionsNLU['author']=='EAP').filter(predictionsNLU['predictedLabel']!='EAP').count()\n",
    "notEAPbutEAPnlu = predictionsNLU.filter(predictionsNLU['author']!='EAP').filter(predictionsNLU['predictedLabel']=='EAP').count()\n",
    "print(\"Predicted EAP correctly {} times vs. {} previously.\".format(EAPandEAPnlu, EAPandEAP))\n",
    "print(\"Failed to predict EAP {} times vs. {} previously.\".format(EAPnotEAPnlu, EAPnotEAP))\n",
    "print(\"Predicted EAP incorrectly {} times vs. {} previously.\".format(notEAPbutEAPnlu, notEAPbutEAP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted HPL correctly 582 times vs. 466 previously.\n",
      "Failed to predict HPL 1050 times vs. 1162 previously.\n",
      "Predicted HPL incorrectly 581 times vs. 483 previously.\n"
     ]
    }
   ],
   "source": [
    "HPLandHPLnlu = predictionsNLU.filter(predictionsNLU['author']=='HPL').filter(predictionsNLU['predictedLabel']=='HPL').count()\n",
    "HPLnotHPLnlu = predictionsNLU.filter(predictionsNLU['author']=='HPL').filter(predictionsNLU['predictedLabel']!='HPL').count()\n",
    "notHPLbutHPLnlu = predictionsNLU.filter(predictionsNLU['author']!='HPL').filter(predictionsNLU['predictedLabel']=='HPL').count()\n",
    "print(\"Predicted HPL correctly {} times vs. {} previously.\".format(HPLandHPLnlu, HPLandHPL))\n",
    "print(\"Failed to predict HPL {} times vs. {} previously.\".format(HPLnotHPLnlu, HPLnotHPL))\n",
    "print(\"Predicted HPL incorrectly {} times vs. {} previously.\".format(notHPLbutHPLnlu, notHPLbutHPL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted MWS correctly 444 times vs. 318 previously.\n",
      "Failed to predict MWS 1266 times vs. 1387 previously.\n",
      "Predicted MWS incorrectly 462 times vs. 353 previously.\n"
     ]
    }
   ],
   "source": [
    "MWSandMWSnlu = predictionsNLU.filter(predictionsNLU['author']=='MWS').filter(predictionsNLU['predictedLabel']=='MWS').count()\n",
    "MWSnotMWSnlu = predictionsNLU.filter(predictionsNLU['author']=='MWS').filter(predictionsNLU['predictedLabel']!='MWS').count()\n",
    "notMWSbutMWSnlu = predictionsNLU.filter(predictionsNLU['author']!='MWS').filter(predictionsNLU['predictedLabel']=='MWS').count()\n",
    "print(\"Predicted MWS correctly {} times vs. {} previously.\".format(MWSandMWSnlu, MWSandMWS))\n",
    "print(\"Failed to predict MWS {} times vs. {} previously.\".format(MWSnotMWSnlu, MWSnotMWS))\n",
    "print(\"Predicted MWS incorrectly {} times vs. {} previously.\".format(notMWSbutMWSnlu, notMWSbutMWS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![IBM Logo](http://www-03.ibm.com/press/img/Large_IBM_Logo_TN.jpg)\n",
    "\n",
    "Rich Tarro  \n",
    "Solutions Architect, IBM Corporation  \n",
    "rtarro@us.ibm.com\n",
    "\n",
    "December 8, 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 with Spark 2.1",
   "language": "python",
   "name": "python2-spark21"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
